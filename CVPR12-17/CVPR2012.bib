@INPROCEEDINGS{6247648,
author={},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Author index},
year={2012},
volume={},
number={},
pages={1-103},
abstract={Presents an index of the authors whose papers are published in the conference.},
keywords={},
doi={10.1109/CVPR.2012.6247648},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247650,
author={},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Third floor - exhibition halls},
year={2012},
volume={},
number={},
pages={1-1},
abstract={Shows the floor plane for the third floor exhibition halls.},
keywords={},
doi={10.1109/CVPR.2012.6247650},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247649,
author={},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Table of contents},
year={2012},
volume={},
number={},
pages={1-41},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/CVPR.2012.6247649},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247651,
author={Lin, Wen-Yan and Liu, Linlin and Matsushita, Yasuyuki and Low, Kok-Lim and Liu, Siying},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Aligning images in the wild},
year={2012},
volume={},
number={},
pages={1-8},
abstract={Aligning image pairs with significant appearance change is a long standing computer vision challenge. Much of this problem stems from the local patch descriptors' instability to appearance variation. In this paper we suggest this instability is due less to descriptor corruption and more the difficulty in utilizing local information to canonically define the orientation (scale and rotation) at which a patch's descriptor should be computed. We address this issue by jointly estimating correspondence and relative patch orientation, within a hierarchical algorithm that utilizes a smoothly varying parameterization of geometric transformations. By collectively estimating the correspondence and orientation of all the features, we can align and orient features that cannot be stably matched with only local information. At the price of smoothing over motion discontinuities (due to independent motion or parallax), this approach can align image pairs that display significant inter-image appearance variations.},
keywords={Imaging;Frequency modulation;Equations;Vectors;Lighting;Image color analysis;Robustness},
doi={10.1109/CVPR.2012.6247651},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247652,
author={Kong, Naejin and Tai, Yu-Wing and Shin, Sung Yong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A physically-based approach to reflection separation},
year={2012},
volume={},
number={},
pages={9-16},
abstract={We propose a physically-based approach to separate reflection using multiple polarized images with a background scene captured behind glass. The input consists of three polarized images, each captured from the same view point but with a different polarizer angle separated by 45 degrees. The output is the high-quality separation of the reflection and background layers from each of the input images. A main technical challenge for this problem is that the mixing coefficient for the reflection and background layers depends on the angle of incidence and the orientation of the plane of incidence, which are spatially-varying over the pixels of an image. Exploiting physical properties of polarization for a double-surfaced glass medium, we propose an algorithm which automatically finds the optimal separation of the reflection and background layers. Thorough experiments, we demonstrate that our approach can generate superior results to those of previous methods.},
keywords={Glass;Image edge detection;Ash;Mutual information;Equations;Cost function;Cameras},
doi={10.1109/CVPR.2012.6247652},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247653,
author={Tai, Yu-Wing and Lin, Stephen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Motion-aware noise filtering for deblurring of noisy and blurry images},
year={2012},
volume={},
number={},
pages={17-24},
abstract={Image noise can present a serious problem in motion deblurring. While most state-of-the-art motion deblurring algorithms can deal with small levels of noise, in many cases such as low-light imaging, the noise is large enough in the blurred image that it cannot be handled effectively by these algorithms. In this paper, we propose a technique for jointly denoising and deblurring such images that elevates the performance of existing motion deblurring algorithms. Our method takes advantage of estimated motion blur kernels to improve denoising, by constraining the denoised image to be consistent with the estimated camera motion (i.e., no high frequency noise features that do not match the motion blur). This improved denoising then leads to higher quality blur kernel estimation and deblurring performance. The two operations are iterated in this manner to obtain results superior to suppressing noise effects through regularization in deblurring or by applying denoising as a preprocess. This is demonstrated in experiments both quantitatively and qualitatively using various image examples.},
keywords={Noise;Kernel;Noise reduction;Equations;Estimation;Noise measurement;Deconvolution},
doi={10.1109/CVPR.2012.6247653},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247654,
author={Kim, Sunyeong and Tai, Yu-Wing and Kim, Seon Joo and Brown, Michael S. and Matsushita, Yasuyuki},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Nonlinear camera response functions and image deblurring},
year={2012},
volume={},
number={},
pages={25-32},
abstract={This paper investigates the role that nonlinear camera response functions (CRFs) have on image deblurring. In particular, we show how nonlinear CRFs can cause a spatially invariant blur to behave as a spatially varying blur. This can result in noticeable ringing artifacts when deconvolution is applied even with a known point spread function (PSF). In addition, we show how CRFs can adversely affect PSF estimation algorithms in the case of blind deconvolution. To help counter these effects, we introduce two methods to estimate the CRF directly from one or more blurred images when the PSF is known or unknown. While not as accurate as conventional CRF estimation algorithms based on multiple exposures or calibration patterns, our approach is still quite effective in improving deblurring results in situations where the CRF is unknown.},
keywords={Image edge detection;Estimation;Shape;Cameras;Image restoration;Kernel;Equations},
doi={10.1109/CVPR.2012.6247654},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247655,
author={Shih, Yi Chang and Davis, Abe and Hasinoff, Samuel W. and Durand, Frédo and Freeman, William T.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Laser speckle photography for surface tampering detection},
year={2012},
volume={},
number={},
pages={33-40},
abstract={It is often desirable to detect whether a surface has been touched, even when the changes made to that surface are too subtle to see in a pair of before and after images. To address this challenge, we introduce a new imaging technique that combines computational photography and laser speckle imaging. Without requiring controlled laboratory conditions, our method is able to detect surface changes that would be indistinguishable in regular photographs. It is also mobile and does not need to be present at the time of contact with the surface, making it well suited for applications where the surface of interest cannot be constantly monitored. Our approach takes advantage of the fact that tiny surface deformations cause phase changes in reflected coherent light which alter the speckle pattern visible under laser illumination. We take before and after images of the surface under laser light and can detect subtle contact by correlating the speckle patterns in these images. A key challenge we address is that speckle imaging is very sensitive to the location of the camera, so removing and reintroducing the camera requires high-accuracy viewpoint alignment. To this end, we use a combination of computational rephotography and correlation analysis of the speckle pattern as a function of camera translation. Our technique provides a reliable way of detecting subtle surface contact at a level that was previously only possible under laboratory conditions. With our system, the detection of these subtle surface changes can now be brought into the wild.},
keywords={Speckle;Cameras;Apertures;Surface emitting lasers;Correlation},
doi={10.1109/CVPR.2012.6247655},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247656,
author={Wanner, Sven and Goldluecke, Bastian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Globally consistent depth labeling of 4D light fields},
year={2012},
volume={},
number={},
pages={41-48},
abstract={We present a novel paradigm to deal with depth reconstruction from 4D light fields in a variational framework. Taking into account the special structure of light field data, we reformulate the problem of stereo matching to a constrained labeling problem on epipolar plane images, which can be thought of as vertical and horizontal 2D cuts through the field. This alternative formulation allows to estimate accurate depth values even for specular surfaces, while simultaneously taking into account global visibility constraints in order to obtain consistent depth maps for all views. The resulting optimization problems are solved with state-of-the-art convex relaxation techniques. We test our algorithm on a number of synthetic and real-world examples captured with a light field gantry and a plenoptic camera, and compare to ground truth where available. All data sets as well as source code are provided online for additional evaluation.},
keywords={Labeling;Cameras;Optimization;Tensile stress;Robustness;Minimization},
doi={10.1109/CVPR.2012.6247656},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247657,
author={Lee, Ken-Yi and Chung, Cheng-Da and Chuang, Yung-Yu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Scene warping: Layer-based stereoscopic image resizing},
year={2012},
volume={},
number={},
pages={49-56},
abstract={This paper proposes scene warping, a layer-based stereoscopic image resizing method using image warping. The proposed method decomposes the input stereoscopic image pair into layers according to the depth and color information. A quad mesh is placed onto each layer to guide the image warping for resizing. The warped layers are composited by their depth orders to synthesize the resized stereoscopic image. We formulate an energy function to guide the warping for each layer so that the composited image avoids distortions and holes, maintains good stereoscopic properties and contains as many important pixels as possible in the reduced image space. The proposed method offers the advantages of less discontinuous artifacts, less-distorted objects, correct depth ordering and enhanced stereoscopic quality. Experiments show that our method compares favorably with existing methods.},
keywords={Stereo image processing;Object segmentation;Optimization;Image quality;Energy measurement;Image color analysis;Image edge detection},
doi={10.1109/CVPR.2012.6247657},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247658,
author={Li, Kai and Xu, Feng and Wang, Jue and Dai, Qionghai and Liu, Yebin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A data-driven approach for facial expression synthesis in video},
year={2012},
volume={},
number={},
pages={57-64},
abstract={This paper presents a method to synthesize a realistic facial animation of a target person, driven by a facial performance video of another person. Different from traditional facial animation approaches, our system takes advantage of an existing facial performance database of the target person, and generates the final video by retrieving frames from the database that have similar expressions to the input ones. To achieve this we develop an expression similarity metric for accurately measuring the expression difference between two video frames. To enforce temporal coherence, our system employs a shortest path algorithm to choose the optimal image for each frame from a set of candidate frames determined by the similarity metric. Finally, our system adopts an expression mapping method to further minimize the expression difference between the input and retrieved frames. Experimental results show that our system can generate high quality facial animation using the proposed data-driven approach.},
keywords={Face;Databases;Measurement;Equations;Coherence;Solid modeling;Facial animation},
doi={10.1109/CVPR.2012.6247658},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247659,
author={Bianco, Simone and Schettini, Raimondo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Color constancy using faces},
year={2012},
volume={},
number={},
pages={65-72},
abstract={In this work, we investigate how illuminant estimation can be performed exploiting the color statistics extracted from the faces automatically detected in the image. The proposed method is based on two observations: first, skin colors tend to form a cluster in the color space, making it a cue to estimate the illuminant in the scene; second, many photographic images are portraits or contain people. The proposed method has been tested on a public dataset of images in RAW format, using both a manual and a real face detector. Experimental results demonstrate the effectiveness of our approach. The proposed method can be directly used in many digital still camera processing pipelines with an embedded face detector working on gray level images.},
keywords={Image color analysis;Skin;Detectors;Cameras;Estimation;Histograms;Standards},
doi={10.1109/CVPR.2012.6247659},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247660,
author={Ji, Hui and Wang, Kang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A two-stage approach to blind spatially-varying motion deblurring},
year={2012},
volume={},
number={},
pages={73-80},
abstract={Many blind motion deblur methods model the motion blur as a spatially invariant convolution process. However, motion blur caused by the camera movement in 3D space during shutter time often leads to spatially varying blurring effect over the image. In this paper, we proposed an efficient two-stage approach to remove spatially-varying motion blurring from a single photo. There are three main components in our approach: (i) a minimization method of estimating region-wise blur kernels by using both image information and correlations among neighboring kernels, (ii) an interpolation scheme of constructing pixel-wise blur matrix from region-wise blur kernels, and (iii) a non-blind deblurring method robust to kernel errors. The experiments showed that the proposed method outperformed the existing software based approaches on tested real images.},
keywords={Kernel;Cameras;Interpolation;Image restoration;Image edge detection;Transforms;Estimation},
doi={10.1109/CVPR.2012.6247660},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247661,
author={Ancuti, Cosmin and Ancuti, Codruta Orniana and Haber, Tom and Bekaert, Philippe},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Enhancing underwater images and videos by fusion},
year={2012},
volume={},
number={},
pages={81-88},
abstract={This paper describes a novel strategy to enhance underwater videos and images. Built on the fusion principles, our strategy derives the inputs and the weight measures only from the degraded version of the image. In order to overcome the limitations of the underwater medium we define two inputs that represent color corrected and contrast enhanced versions of the original underwater image/frame, but also four weight maps that aim to increase the visibility of the distant objects degraded due to the medium scattering and absorption. Our strategy is a single image approach that does not require specialized hardware or knowledge about the underwater conditions or scene structure. Our fusion framework also supports temporal coherence between adjacent frames by performing an effective edge preserving noise reduction strategy. The enhanced images and videos are characterized by reduced noise level, better exposed-ness of the dark regions, improved global contrast while the finest details and edges are enhanced significantly. In addition, the utility of our enhancing technique is proved for several challenging applications.},
keywords={Image color analysis;Videos;Image edge detection;Image restoration;Noise;Laplace equations;Lighting},
doi={10.1109/CVPR.2012.6247661},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247662,
author={Liu, Shuaicheng and Wang, Yinting and Yuan, Lu and Bu, Jiajun and Tan, Ping and Sun, Jian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Video stabilization with a depth camera},
year={2012},
volume={},
number={},
pages={89-95},
abstract={Previous video stabilization methods often employ homographies to model transitions between consecutive frames, or require robust long feature tracks. However, the homography model is invalid for scenes with significant depth variations, and feature point tracking is fragile in videos with textureless objects, severe occlusion or camera rotation. To address these challenging cases, we propose to solve video stabilization with an additional depth sensor such as the Kinect camera. Though the depth image is noisy, incomplete and low resolution, it facilitates both camera motion estimation and frame warping, which make the video stabilization a much well posed problem. The experiments demonstrate the effectiveness of our algorithm.},
keywords={Cameras;Trajectory;Streaming media;Tracking;Vectors;Robustness;Sensors},
doi={10.1109/CVPR.2012.6247662},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247663,
author={Colaço, Andrea and Kirmani, Ahmed and Howland, Gregory A. and Howell, John C. and Goyal, Vivek K},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Compressive depth map acquisition using a single photon-counting detector: Parametric signal processing meets sparsity},
year={2012},
volume={},
number={},
pages={96-102},
abstract={Active range acquisition systems such as light detection and ranging (LIDAR) and time-of-flight (TOF) cameras achieve high depth resolution but suffer from poor spatial resolution. In this paper we introduce a new range acquisition architecture that does not rely on scene raster scanning as in LIDAR or on a two-dimensional array of sensors as used in TOF cameras. Instead, we achieve spatial resolution through patterned sensing of the scene using a digital micromirror device (DMD) array. Our depth map reconstruction uses parametric signal modeling to recover the set of distinct depth ranges present in the scene. Then, using a convex program that exploits the sparsity of the Laplacian of the depth map, we recover the spatial content at the estimated depth ranges. In our experiments we acquired 64×64-pixel depth maps of fronto-parallel scenes at ranges up to 2.1 M using a pulsed laser, a DMD array and a single photon-counting detector. We also demonstrated imaging in the presence of unknown partially-transmissive occluders. The prototype and results provide promising directions for non-scanning, low-complexity range acquisition devices for various computer vision applications.},
keywords={Photonics;Imaging;Detectors;Spatial resolution;Histograms;Image reconstruction},
doi={10.1109/CVPR.2012.6247663},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247664,
author={Taylor, Jonathan and Shotton, Jamie and Sharp, Toby and Fitzgibbon, Andrew},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The Vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation},
year={2012},
volume={},
number={},
pages={103-110},
abstract={Fitting an articulated model to image data is often approached as an optimization over both model pose and model-to-image correspondence. For complex models such as humans, previous work has required a good initialization, or an alternating minimization between correspondence and pose. In this paper we investigate one-shot pose estimation: can we directly infer correspondences using a regression function trained to be invariant to body size and shape, and then optimize the model pose just once? We evaluate on several challenging single-frame data sets containing a wide variety of body poses, shapes, torso rotations, and image cropping. Our experiments demonstrate that one-shot pose estimation achieves state of the art results and runs in real-time.},
keywords={Optimization;Shape;Joints;Vegetation;Manifolds;Estimation;Training},
doi={10.1109/CVPR.2012.6247664},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247665,
author={He, Kaiming and Sun, Jian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Computing nearest-neighbor fields via Propagation-Assisted KD-Trees},
year={2012},
volume={},
number={},
pages={111-118},
abstract={Matching patches between two images, also known as computing nearest-neighbor fields, has been proven a useful technique in various computer vision/graphics algorithms. But this is a computationally challenging nearest-neighbor search task, because both the query set and the candidate set are of image size. In this paper, we propose Propagation-Assisted KD-Trees to quickly compute an approximate solution. We develop a novel propagation search method for kd-trees. In this method the tree nodes checked by each query are propagated from the nearby queries. This method not only avoids the time-consuming backtracking in traditional tree methods, but is more accurate. Experiments on public data show that our method is 10-20 times faster than the PatchMatch method [4] at the same accuracy, or reduces its error by 70% at the same running time. Our method is also 2-5 times faster and is more accurate than Coherency Sensitive Hashing [22], a latest state-of-the-art method.},
keywords={Accuracy;Artificial neural networks;Buildings;Search methods;Principal component analysis;Transforms;Standards},
doi={10.1109/CVPR.2012.6247665},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247666,
author={Lian, Zhouhui and Godil, Afzal and Rosin, Paul L. and Sun, Xianfang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A new convexity measurement for 3D meshes},
year={2012},
volume={},
number={},
pages={119-126},
abstract={This paper presents a novel convexity measurement for 3D meshes. The new convexity measure is calculated by minimizing the ratio of the summed area of valid regions in a mesh's six views, which are projected on faces of the bounding box whose edges are parallel to the coordinate axes, to the sum of three orthogonal projected areas of the mesh. The complete definition, theoretical analysis, and a computing algorithm of our convexity measure are explicitly described. This paper also proposes a new 3D shape descriptor CD (i.e., Convexity Distribution) based on the distribution of above-mentioned ratios, which are computed by randomly rotating the mesh around its center, to better describe the object's convexity-related properties compared to existing convexity measurements. Our experiments not only show that the proposed convexity measure corresponds well with human intuition, but also demonstrate the effectiveness of the new convexity measure and the new shape descriptor by significantly improving the performance of other methods in the application of 3D shape retrieval.},
keywords={Shape;Shape measurement;Solid modeling;Area measurement;Manganese;Computational modeling;Coordinate measuring machines},
doi={10.1109/CVPR.2012.6247666},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247667,
author={Zhou, Feng and De la Torre, Fernando},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Factorized graph matching},
year={2012},
volume={},
number={},
pages={127-134},
abstract={Graph matching plays a central role in solving correspondence problems in computer vision. Graph matching problems that incorporate pair-wise constraints can be cast as a quadratic assignment problem (QAP). Unfortunately, QAP is NP-hard and many algorithms have been proposed to solve different relaxations. This paper presents factorized graph matching (FGM), a novel framework for interpreting and optimizing graph matching problems. In this work we show that the affinity matrix can be factorized as a Kronecker product of smaller matrices. There are three main benefits of using this factorization in graph matching: (1) There is no need to compute the costly (in space and time) pair-wise affinity matrix; (2) The factorization provides a taxonomy for graph matching and reveals the connection among several methods; (3) Using the factorization we derive a new approximation of the original problem that improves state-of-the-art algorithms in graph matching. Experimental results in synthetic and real databases illustrate the benefits of FGM. The code is available at http://humansensing.cs.cmu.edu/fgm.},
keywords={Linear approximation;Optimization;Approximation algorithms;Computer vision;Vectors;Linear programming},
doi={10.1109/CVPR.2012.6247667},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247668,
author={Lin, Liang and Wang, Xiaolong and Yang, Wei and Lai, Jianhuang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning contour-fragment-based shape model with And-Or tree representation},
year={2012},
volume={},
number={},
pages={135-142},
abstract={This paper proposes a simple yet effective method to learn the hierarchical object shape model consisting of local contour fragments, which represents a category of shapes in the form of an And-Or tree. This model extends the traditional hierarchical tree structures by introducing the “switch” variables (i.e. the or-nodes) that explicitly specify production rules to capture shape variations. We thus define the model with three layers: the leaf-nodes for detecting local contour fragments, the or-nodes specifying selection of leaf-nodes, and the root-node encoding the holistic distortion. In the training stage, for optimization of the And-Or tree learning, we extend the concave-convex procedure (CCCP) by embedding the structural clustering during the iterative learning steps. The inference of shape detection is consistent with the model optimization, which integrates the local testings via the leaf-nodes and or-nodes with the global verification via the root-node. The advantages of our approach are validated on the challenging shape databases (i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method is able to accurately localize shape contours against unreliable edge detection and edge tracing. (2) The And-Or tree model enables us to well capture the intraclass variance.},
keywords={Shape;Vectors;Optimization;Context;Testing;Image edge detection;Switches},
doi={10.1109/CVPR.2012.6247668},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247669,
author={Jiang, Hao and Tian, Tai-Peng and He, Kun and Sclaroff, Stan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Scale resilient, rotation invariant articulated object matching},
year={2012},
volume={},
number={},
pages={143-150},
abstract={A novel method is proposed for matching articulated objects in cluttered videos. The method needs only a single exemplar image of the target object. Instead of using a small set of large parts to represent an articulated object, the proposed model uses hundreds of small units to represent walks along paths of pixels between key points on an articulated object. Matching directly on dense pixels is key to achieving reliable matching when motion blur occurs. The proposed method fits the model to local image properties, conforms to structure constraints, and remembers the steps taken along a pixel path. The model formulation handles variations in object scaling, rotation and articulation. Recovery of the optimal pixel walks is posed as a special shortest path problem, which can be solved efficiently via dynamic programming. Further speedup is achieved via factorization of the path costs. An efficient method is proposed to find multiple walks and simultaneously match multiple key points. Experiments show that the proposed method is efficient and reliable and can be used to match articulated objects in fast motion videos with strong clutter and blurry imagery.},
keywords={Kernel;Vectors;Image edge detection;Optimization;Indexes;Videos;Reliability},
doi={10.1109/CVPR.2012.6247669},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247670,
author={Wang, Xinggang and Bai, Xiang and Ma, Tianyang and Liu, Wenyu and Latecki, Longin Jan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fan Shape Model for object detection},
year={2012},
volume={},
number={},
pages={151-158},
abstract={We propose a novel shape model for object detection called Fan Shape Model (FSM). We model contour sample points as rays of final length emanating for a reference point. As in folding fan, its slats, which we call rays, are very flexible. This flexibility allows FSM to tolerate large shape variance. However, the order and the adjacency relation of the slats stay invariant during fan deformation, since the slats are connected with a thin fabric. In analogy, we enforce the order and adjacency relation of the rays to stay invariant during the deformation. Therefore, FSM preserves discriminative power while allowing for a substantial shape deformation. FSM allows also for precise scale estimation during object detection. Thus, there is not need to scale the shape model or image in order to perform object detection. Another advantage of FSM is the fact that it can be applied directly to edge images, since it does not require any linking of edge pixels to edge fragments (contours).},
keywords={Shape;Image edge detection;Object detection;Training;Computational modeling;Estimation;Joining processes},
doi={10.1109/CVPR.2012.6247670},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247671,
author={Kokkinos, Iasonas and Bronstein, Michael M. and Litman, Roee and Bronstein, Alex M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Intrinsic shape context descriptors for deformable shapes},
year={2012},
volume={},
number={},
pages={159-166},
abstract={In this work, we present intrinsic shape context (ISC) descriptors for 3D shapes. We generalize to surfaces the polar sampling of the image domain used in shape contexts: for this purpose, we chart the surface by shooting geodesic outwards from the point being analyzed; `angle' is treated as tantamount to geodesic shooting direction, and radius as geodesic distance. To deal with orientation ambiguity, we exploit properties of the Fourier transform. Our charting method is intrinsic, i.e., invariant to isometric shape transformations. The resulting descriptor is a meta-descriptor that can be applied to any photometric or geometric property field defined on the shape, in particular, we can leverage recent developments in intrinsic shape analysis and construct ISC based on state-of-the-art dense shape descriptors such as heat kernel signatures. Our experiments demonstrate a notable improvement in shape matching on standard benchmarks.},
keywords={Shape;Context;Surface treatment;Geometry;Heating;Kernel;Standards},
doi={10.1109/CVPR.2012.6247671},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247672,
author={Gu, Steve and Zheng, Ying and Tomasi, Carlo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Twisted window search for efficient shape localization},
year={2012},
volume={},
number={},
pages={167-173},
abstract={Many computer vision systems approximate targets' shape with rectangular bounding boxes. This choice trades localization accuracy for efficient computation. We propose twisted window search, a strict generalization over rectangular window search, for the globally optimal localization of a target's shape. Despite its generality, we show that the new algorithm runs in O(n3), an asymptotic time complexity that is no greater than that of rectangular window search on an image of resolution n × n. We demonstrate improved results of twisted window search for localizing and tracking non-rigid objects with significant orientation, scale and shape change. Twisted window search runs at nearly 10 frames per second in our MATLAB/C++ implementation on images of resolution 240 × 320 on a quad-core laptop.},
keywords={Shape;Complexity theory;Optimization;Search problems;Transforms;Equations;Image segmentation},
doi={10.1109/CVPR.2012.6247672},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247673,
author={Hontani, Hidekata and Matsuno, Takamiti and Sawada, Yoshihide},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust nonrigid ICP using outlier-sparsity regularization},
year={2012},
volume={},
number={},
pages={174-181},
abstract={We show how to incorporate a statistical shape model into the nonrigid ICP framework, and propose a robust nonrigid ICP algorithm. In the nonrigid ICP framework, a template surface is represented by a set of points, and the shape of the template is parametrized by a transformation matrix per one template point. In the proposed method, the statistics of the matrices are estimated based on a set of training surfaces, and the statistical shape model is incorporated into the nonrigid ICP framework by modifying the representation of the stiffness of the template. The statistical shape model and a noise model make it possible to discriminate outliers from inliers in given targets. Our proposed method detects the outliers, which are not represented by the models appropriately, based on their sparseness. The detected outliers are automatically excluded from the target to be registered, and the template is deformed to fit the inliers only. As the result, the accuracy of the registration is improved. The performance of the proposed method is evaluated qualitatively and quantitatively using synthetic data and clinical CT images.},
keywords={Robustness;Iterative closest point algorithm;Shape;Training;Cost function;Computational modeling;Noise},
doi={10.1109/CVPR.2012.6247673},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247674,
author={Rodolà, Emanuele and Bronstein, Alex M. and Albarelli, Andrea and Bergamasco, Filippo and Torsello, Andrea},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A game-theoretic approach to deformable shape matching},
year={2012},
volume={},
number={},
pages={182-189},
abstract={We consider the problem of minimum distortion intrinsic correspondence between deformable shapes, many useful formulations of which give rise to the NP-hard quadratic assignment problem (QAP). Previous attempts to use the spectral relaxation have had limited success due to the lack of sparsity of the obtained “fuzzy” solution. In this paper, we adopt the recently introduced alternative L1 relaxation of the QAP based on the principles of game theory. We relate it to the Gromov and Lipschitz metrics between metric spaces and demonstrate on state-of-the-art benchmarks that the proposed approach is capable of finding very accurate sparse correspondences between deformable shapes.},
keywords={Shape;Heating;Geometry;Distortion measurement;Robustness;Optimization},
doi={10.1109/CVPR.2012.6247674},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247675,
author={Letouzey, Antoine and Boyer, Edmond},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Progressive shape models},
year={2012},
volume={},
number={},
pages={190-197},
abstract={In this paper we address the problem of recovering both the topology and the geometry of a deformable shape using temporal mesh sequences. The interest arises in multi-camera applications when unknown natural dynamic scenes are captured. While several approaches allow recovery of shape models from static scenes, few consider dynamic scenes with evolving topology and without prior knowledge. In this nonetheless generic situation, a single time observation is not necessarily sufficient to infer the correct topology of the observed shape and evidences must be accumulated over time in order to learn the topology and to enable temporally consistent modelling. This appears to be a new problem for which no formal solution exists. We propose a principled approach based on the assumption that the observed objects have a fixed topology. Under this assumption, we can progressively learn the topology meanwhile capturing the deformation of the dynamic scene. The approach has been successfully experimented on several standard 4D datasets.},
keywords={Topology;Shape;Deformable models;Geometry;Solid modeling;Computational modeling;Estimation},
doi={10.1109/CVPR.2012.6247675},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247676,
author={Sethi, Manu and Rangarajan, Anand and Gurumoorthy, Karthik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The Schrödinger distance transform (SDT) for point-sets and curves},
year={2012},
volume={},
number={},
pages={198-205},
abstract={Despite the ubiquitous use of distance transforms in the shape analysis literature and the popularity of fast marching and fast sweeping methods - essentially Hamilton-Jacobi solvers, there is very little recent work leveraging the Hamilton-Jacobi to Schrödinger connection for representational and computational purposes. In this work, we exploit the linearity of the Schrödinger equation to (i) design fast discrete convolution methods using the FFT to compute the distance transform, (ii) derive the histogram of oriented gradients (HOG) via the squared magnitude of the Fourier transform of the wave function, (iii) extend the Schrödinger formalism to cover the case of curves parametrized as line segments as opposed to point-sets, (iv) demonstrate that the Schrödinger formalism permits the addition of wave functions - an operation that is not allowed for distance transforms, and finally (v) construct a fundamentally new Schrödinger equation and show that it can represent both the distance transform and its gradient density - not possible in earlier efforts.},
keywords={Transforms;Equations;Shape;Wave functions;Approximation methods;Image segmentation;Linearity},
doi={10.1109/CVPR.2012.6247676},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247677,
author={Hauagge, Daniel Cabrini and Snavely, Noah},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image matching using local symmetry features},
year={2012},
volume={},
number={},
pages={206-213},
abstract={We present a new technique for extracting local features from images of architectural scenes, based on detecting and representing local symmetries. These new features are motivated by the fact that local symmetries, at different scales, are a fundamental characteristic of many urban images, and are potentially more invariant to large appearance changes than lower-level features such as SIFT. Hence, we apply these features to the problem of matching challenging pairs of photos of urban scenes. Our features are based on simple measures of local bilateral and rotational symmetries computed using local image operations. These measures are used both for feature detection and for computing descriptors. We demonstrate our method on a challenging new dataset containing image pairs exhibiting a range of dramatic variations in lighting, age, and rendering style, and show that our features can improve matching performance for this difficult task.},
keywords={Feature extraction;Detectors;Histograms;Robustness;Lighting;Transforms;Image edge detection},
doi={10.1109/CVPR.2012.6247677},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247678,
author={Liang, Jian and Lai, Rongjie and Wong, Tsz Wai and Zhao, Hongkai},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Geometric understanding of point clouds using Laplace-Beltrami operator},
year={2012},
volume={},
number={},
pages={214-221},
abstract={In this paper, we propose a general framework for approximating differential operator directly on point clouds and use it for geometric understanding on them. The discrete approximation of differential operator on the underlying manifold represented by point clouds is based only on local approximation using nearest neighbors, which is simple, efficient and accurate. This allows us to extract the complete local geometry, solve partial differential equations and perform intrinsic calculations on surfaces. Since no mesh or parametrization is needed, our method can work with point clouds in any dimensions or co-dimensions or even with variable dimensions. The computation complexity scaled well with the number of points and the intrinsic dimensions (rather than the embedded dimensions). We use this method to define the Laplace-Beltrami (LB) operator on point clouds, which links local and global information together. With this operator, we propose a few key applications essential to geometric understanding for point clouds, including the computation of LB eigenvalues and eigenfunctions, the extraction of skeletons from point clouds, and the extraction of conformal structures from point clouds.},
keywords={Eigenvalues and eigenfunctions;Least squares approximation;Skeleton;Manifolds;Geometry;Harmonic analysis},
doi={10.1109/CVPR.2012.6247678},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247679,
author={Zhao, Youdong and Song, Xi and Jia, Yunde},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On the dimensionality of video bricks under varying illumination},
year={2012},
volume={},
number={},
pages={222-229},
abstract={Illumination models of the image set of an object (e.g., human face) under varying lighting conditions have been either empirically or analytically explored. However, the theoretical dimensionality of video bricks of an object under varying illumination is still unknown. In this paper, we focus on this question concretely and give both analytical and empirical results. We derive the theoretical upper bound of the dimensionality of video bricks by investigating the analytical formula of appearance changes due to motion variables of light sources. Theoretical results show in real-world scenes video bricks of an object under varying illumination could be expressed well by a low-dimensional linear subspace. Empirical results of the principal component analysis on the YaleB Face database and our video database are consistent with the theoretical results completely. The application of the low-dimensional linear models of video bricks is demonstrated by the foreground detection task in visual surveillance with drastic illumination changes.},
keywords={Lighting;Light sources;Upper bound;Face;Video sequences;Eigenvalues and eigenfunctions;Databases},
doi={10.1109/CVPR.2012.6247679},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247680,
author={Shi, Boxin and Tan, Ping and Matsushita, Yasuyuki and Ikeuchi, Katsushi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A biquadratic reflectance model for radiometric image analysis},
year={2012},
volume={},
number={},
pages={230-237},
abstract={Radiometric image analysis methods heavily rely on reflectance models. Due to the complexity of real materials, methods based on simple models such as the Lambertian model often suffer from inaccuracy. On the other hand, more advanced models such as the Cook-Torrance model severely complicate the analysis problem. We tackle this dilemma by focusing on the low-frequency component of the reflectance. We propose a compact biquadratic reflectance model to represent the reflectance of a broad class of materials precisely in the low-frequency domain. We validate our model by fitting to both existing parametric models and non-parametric measured data, and show that our model outperforms existing parametric diffuse models. We show applications of reflectometry using general diffuse surfaces and photometric stereo for general isotropic materials. Experimental results show the effectiveness of our biquadratic model and its usefulness in radiometric image analysis.},
keywords={Mathematical model;Computational modeling;Analytical models;Materials;Integrated circuit modeling;Radiometry;Brain modeling},
doi={10.1109/CVPR.2012.6247680},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247681,
author={Lombardi, Stephen and Nishino, Ko},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Single image multimaterial estimation},
year={2012},
volume={},
number={},
pages={238-245},
abstract={Estimating the reflectance and illumination from a single image becomes particularly challenging when the object surface consists of multiple materials. The key difficulty lies in recovering the reflectance from sparse angular samples while correctly assigning them to different materials. We tackle this problem by extracting and fully leveraging reflectance priors. The idea is to strongly constrain the possible solutions so that the recovered reflectance conform with those of real-world materials. We achieve this by modeling the parameter space of a directional statistics BRDF model and by extracting an analytical distribution of the subspace that real-world materials span. This is used, with other priors, in a layered MRF-based formulation that models material regions and their spatially varying reflectance with continuous latent layers. The material regions and their reflectance, and the direction and strength of a single point source are jointly estimated. We demonstrate the effectiveness of the method on real and synthetic images.},
keywords={Materials;Estimation;Light sources;Image color analysis;Lighting;Reliability;Brain modeling},
doi={10.1109/CVPR.2012.6247681},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247682,
author={Tian, Yuandong and Narasimhan, Srinivasa G. and Vannevel, Alan J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Depth from optical turbulence},
year={2012},
volume={},
number={},
pages={246-253},
abstract={Turbulence near hot surfaces such as desert terrains and roads during the summer, causes shimmering, distortion and blurring in images. While recent works have focused on image restoration, this paper explores what information about the scene can be extracted from the distortion caused by turbulence. Based on the physical model of wave propagation, we first study the relationship between the scene depth and the amount of distortion caused by homogenous turbulence. We then extend this relationship to more practical scenarios such as finite extent and height-varying turbulence, and present simple algorithms to estimate depth ordering, depth discontinuity and relative depth, from a sequence of short exposure images. In the case of general non-homogenous turbulence, we show that a statistical property of turbulence can be used to improve long-range structure-from-motion (or stereo). We demonstrate the accuracy of our methods in both laboratory and outdoor settings and conclude that turbulence (when present) can be a strong and useful depth cue.},
keywords={Cameras;Optical distortion;Computational modeling;Surface waves;Roads;Refractive index},
doi={10.1109/CVPR.2012.6247682},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247683,
author={Okatani, Takayuki and Deguchi, Koichiro},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Optimal integration of photometric and geometric surface measurements using inaccurate reflectance/illumination knowledge},
year={2012},
volume={},
number={},
pages={254-261},
abstract={In this paper, we present a method for accurately estimating the shape of an object by integrating the surface orientation measured by photometric stereo and the position measured by some range-measuring method. We first show that even if the knowledge of the reflectance/illumination is inaccurate, the first derivatives of the photometrically measured orientation can be accurately estimated at the surface points where they have small values. We propose a probabilistic framework to quantitate the (in)accuracy of the knowledge and connect it to the estimation accuracy of these derivatives. Based on this framework, we consider optimally integrating the surface orientation and position to obtain the object shape with higher accuracy. The integration reduces to an optimization problem, and it is efficiently solved by belief propagation. We present several experimental results showing the effectiveness of the proposed approach.},
keywords={Shape;Lighting;Accuracy;Position measurement;Shape measurement;Probabilistic logic;Estimation},
doi={10.1109/CVPR.2012.6247683},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247684,
author={Ackermann, Jens and Langguth, Fabian and Fuhrmann, Simon and Goesele, Michael},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Photometric stereo for outdoor webcams},
year={2012},
volume={},
number={},
pages={262-269},
abstract={We present a photometric stereo technique that operates on time-lapse sequences captured by static outdoor webcams over the course of several months. Outdoor webcams produce a large set of uncontrolled images subject to varying lighting and weather conditions. We first automatically select a suitable subset of the captured frames for further processing, reducing the dataset size by several orders of magnitude. A camera calibration step is applied to recover the camera response function, the absolute camera orientation, and to compute the light directions for each image. Finally, we describe a new photometric stereo technique for non-Lambertian scenes and unknown light source intensities to recover normal maps and spatially varying materials of the scene.},
keywords={Cameras;Materials;Sun;Lighting;Calibration;Light sources;Image reconstruction},
doi={10.1109/CVPR.2012.6247684},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247685,
author={Sato, Imari and Okabe, Takahiro and Sato, Yoichi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Bispectral photometric stereo based on fluorescence},
year={2012},
volume={},
number={},
pages={270-277},
abstract={We propose a novel technique called bispectral photometric stereo that makes effective use of fluorescence for shape reconstruction. Fluorescence is a common phenomenon occurring in many objects from natural gems and corals, to fluorescent dyes used in clothing. One of the important characteristics of fluorescence is its wavelength-shifting behavior: fluorescent materials absorb light at a certain wavelength and then reemit it at longer wavelengths. Due to the complexity of its emission process, fluorescence tends to be excluded from most algorithms in computer vision and image processing. In this paper, we show that there is a strong similarity between fluorescence and ideal diffuse reflection and that fluorescence can provide distinct clues on how to estimate an object's shape. Moreover, fluorescence's wavelength-shifting property enables us to estimate the shape of an object by applying photometric stereo to emission-only images without suffering from specular reflection. This is the significant advantage of the fluorescence-based method over previous methods based on reflection.},
keywords={Materials;Shape;Surface waves;Image color analysis;Cameras;Lighting;Light sources},
doi={10.1109/CVPR.2012.6247685},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247686,
author={Serra, Marc and Penacchio, Olivier and Benavente, Robert and Vanrell, Maria},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Names and shades of color for intrinsic image estimation},
year={2012},
volume={},
number={},
pages={278-285},
abstract={In the last years, intrinsic image decomposition has gained attention. Most of the state-of-the-art methods are based on the assumption that reflectance changes come along with strong image edges. Recently, user intervention in the recovery problem has proved to be a remarkable source of improvement. In this paper, we propose a novel approach that aims to overcome the shortcomings of pure edge-based methods by introducing strong surface descriptors, such as the color-name descriptor which introduces high-level considerations resembling top-down intervention. We also use a second surface descriptor, termed color-shade, which allows us to include physical considerations derived from the image formation model capturing gradual color surface variations. Both color cues are combined by means of a Markov Random Field. The method is quantitatively tested on the MIT ground truth dataset using different error metrics, achieving state-of-the-art performance.},
keywords={Image color analysis;Image edge detection;Vectors;Coherence;Lighting;Labeling;Geometry},
doi={10.1109/CVPR.2012.6247686},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247687,
author={Shan, Qi and Agarwal, Sameer and Curless, Brian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Refractive height fields from single and multiple images},
year={2012},
volume={},
number={},
pages={286-293},
abstract={We propose a novel framework for reconstructing homogenous, transparent, refractive height-fields from a single viewpoint. The height-field is imaged against a known planar background, or sequence of backgrounds. Unlike existing approaches that do a point-by-point reconstruction - which is known to have intractable ambiguities - our method estimates and optimizes for the entire height-field at the same time. The formulation supports shape recovery from measured distortions (deflections) or directly from the images themselves, including from a single image. We report results for a variety of refractive height-fields showing significant improvement over prior art.},
keywords={Surface reconstruction;Image reconstruction;Optimization;Shape;Indexes;Glass;Cameras},
doi={10.1109/CVPR.2012.6247687},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247688,
author={Jia, Zhaoyin and Gallagher, Andrew and Chang, Yao-Jen and Chen, Tsuhan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A learning-based framework for depth ordering},
year={2012},
volume={},
number={},
pages={294-301},
abstract={Depth ordering is instrumental for understanding the 3D geometry of an image. Humans are surprisingly good at depth ordering even with abstract 2D line drawings. In this paper we propose a learning-based framework for depth ordering inference. Boundary and junction characteristics are important clues for this task, and we have developed new features based on these attributes. Although each feature individually can produce reasonable depth ordering results, each still has limitations, and we can achieve better performance by combining them. In practice, local depth ordering inferences can be contradictory. Therefore, we propose a Markov Random Field model with terms that are more global than previous work, and use graph optimization to encourage a globally consistent ordering. In addition, to produce better object segmentation for the task of depth ordering, we propose to explicitly enforce closed loops and long edges for the occlusion boundary detection. We collect a new depth-order dataset for this problem, including more than a thousand human-labeled images with various daily objects and configurations. The proposed algorithm shows promising performance over conventional methods on both synthetic and real scenes.},
keywords={Junctions;Image segmentation;Image edge detection;Vectors;Histograms;Humans;Abstracts},
doi={10.1109/CVPR.2012.6247688},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247689,
author={Xue, Tianfan and Liu, Jianzhuang and Tang, Xiaoou},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Example-based 3D object reconstruction from line drawings},
year={2012},
volume={},
number={},
pages={302-309},
abstract={Recovering 3D geometry from a single 2D line drawing is an important and challenging problem in computer vision. It has wide applications in interactive 3D modeling from images, computer-aided design, and 3D object retrieval. Previous methods of 3D reconstruction from line drawings are mainly based on a set of heuristic rules. They are not robust to sketch errors and often fail for objects that do not satisfy the rules. In this paper, we propose a novel approach, called example-based 3D object reconstruction from line drawings, which is based on the observation that a natural or man-made complex 3D object normally consists of a set of basic 3D objects. Given a line drawing, a graphical model is built where each node denotes a basic object whose candidates are from a 3D model (example) database. The 3D reconstruction is solved using a maximum-a-posteriori (MAP) estimation such that the reconstructed result best fits the line drawing. Our experiments show that this approach achieves much better reconstruction accuracy and are more robust to imperfect line drawings than previous methods.},
keywords={Solid modeling;Databases;Shape;Three dimensional displays;Graphical models;Vectors;Image reconstruction},
doi={10.1109/CVPR.2012.6247689},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247690,
author={Ye, Jinwei and Ji, Yu and Li, Feng and Yu, Jingyi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Angular domain reconstruction of dynamic 3D fluid surfaces},
year={2012},
volume={},
number={},
pages={310-317},
abstract={We present a novel and simple computational imaging solution to robustly and accurately recover 3D dynamic fluid surfaces. Traditional specular surface reconstruction schemes place special patterns (checkerboard or color patterns) beneath the fluid surface to establish point-pixel correspondences. However, point-pixel correspondences alone are insufficient to recover surface normal or height and they rely on additional constraints to resolve the ambiguity. In this paper, we exploit using Bokode - a computational optical device that emulates a pinhole projector - for capturing ray-ray correspondences which can then be used to directly recover the surface normals. We further develop a robust feature matching algorithm based on the Active-Appearance Model to robustly establishing ray-ray correspondences. Our solution results in an angularly sampled normal field and we derive a new angular-domain surface integration scheme to recover the surface from the normal fields. Specifically, we reformulate the problem as an over-constrained linear system under spherical coordinate and solve it using Singular Value Decomposition. Experiments results on real and synthetic surfaces demonstrate that our approach is robust and accurate, and is easier to implement than state-of-the-art multi-camera based approaches.},
keywords={Surface reconstruction;Cameras;Optical surface waves;Shape;Sea surface;Image reconstruction;Active appearance model},
doi={10.1109/CVPR.2012.6247690},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247691,
author={Ikehata, Satoshi and Wipf, David and Matsushita, Yasuyuki and Aizawa, Kiyoharu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust photometric stereo using sparse regression},
year={2012},
volume={},
number={},
pages={318-325},
abstract={This paper presents a robust photometric stereo method that effectively compensates for various non-Lambertian corruptions such as specularities, shadows, and image noise. We construct a constrained sparse regression problem that enforces both Lambertian, rank-3 structure and sparse, additive corruptions. A solution method is derived using a hierarchical Bayesian approximation to accurately estimate the surface normals while simultaneously separating the non-Lambertian corruptions. Extensive evaluations are performed that show state-of-the-art performance using both synthetic and real-world images.},
keywords={Lighting;Vectors;Bayesian methods;Robustness;Estimation;Sparse matrices;Minimization},
doi={10.1109/CVPR.2012.6247691},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247692,
author={Zhou, Qian-Yi and Neumann, Ulrich},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={2.5D building modeling by discovering global regularities},
year={2012},
volume={},
number={},
pages={326-333},
abstract={We introduce global regularities in the 2.5D building modeling problem, to reflect the orientation and placement similarities between planar elements in building structures. Given a 2.5D point cloud scan, we present an automatic approach that simultaneously detects locally fitted plane primitives and global regularities. While global regularities are extracted by analyzing the plane primitives, they adjust the planes in return and effectively correct local fitting errors. We explore a broad variety of global regularities between 2.5D planar elements including both planer roof patches and planar facade patches. By aligning planar elements to global regularities, our method significantly improves the model quality in terms of both geometry and human judgement.},
keywords={Buildings;Humans;Shape;Computational modeling;Geometry;Noise measurement;Solid modeling},
doi={10.1109/CVPR.2012.6247692},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247693,
author={Barron, Jonathan T. and Malik, Jitendra},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Shape, albedo, and illumination from a single image of an unknown object},
year={2012},
volume={},
number={},
pages={334-341},
abstract={We address the problem of recovering shape, albedo, and illumination from a single grayscale image of an object, using shading as our primary cue. Because this problem is fundamentally underconstrained, we construct statistical models of albedo and shape, and define an optimization problem that searches for the most likely explanation of a single image. We present two priors on albedo which encourage local smoothness and global sparsity, and three priors on shape which encourage flatness, outward-facing orientation at the occluding contour, and local smoothness. We present an optimization technique for using these priors to recover shape, albedo, and a spherical harmonic model of illumination. Our model, which we call SAIFS (shape, albedo, and illumination from shading) produces reasonable results on arbitrary grayscale images taken in the real world, and outperforms all previous grayscale “intrinsic image” - style algorithms on the MIT Intrinsic Images dataset.},
keywords={Shape;Lighting;Entropy;Gray-scale;Materials;Optimization;Training},
doi={10.1109/CVPR.2012.6247693},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247694,
author={Zhou, Changyin and Troccoli, Alejandro and Pulli, Kari},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust stereo with flash and no-flash image pairs},
year={2012},
volume={},
number={},
pages={342-349},
abstract={We propose a new stereo technique using a pair of flash and no-flash stereo images that is both efficient and robust in handling occlusion boundaries. Our work is motivated by the observation that the brightness variations introduced by the flash can provide a robust cue for establishing stereo matches at occlusion boundaries. This photometric cue is computed per pixel, and though on its own is not robust to reliably resolve depth, it can provide a new discriminant to support patch-based stereo matching algorithms. Our experiments using a hand-held Fujifilm W3 3D camera show satisfying stereo performance over a variety of scenes, including several outdoor scenes.},
keywords={Ash;Cameras;Memory management;Robustness;Shape;Lighting;Dynamic programming},
doi={10.1109/CVPR.2012.6247694},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247695,
author={Wang, Xiaoyu and Hua, Gang and Han, Tony X.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Detection by detections: Non-parametric detector adaptation for a video},
year={2012},
volume={},
number={},
pages={350-357},
abstract={We propose an approach to improving the detection results of a generic offline trained detector on a specific video. Our method does not leverage visual tracking as most detection by tracking methods do. Instead, the proposed detection by detections approach can serve as a more confident initialization for detection by tracking methods. Different from other supervised detector adaptation methods, we constrain the task to videos and no supervised labels for the target video are required for the adaptation; we intend to fill the gap between detection by tracking and pure detection by frames. As a non-parametric detector adaptation method, confident detections are collected to re-rank and to group other detections. We focus on methods with high precision detection results since it is necessitated in real application. Extensive experiments with two state-of-the-art detectors demonstrate the efficacy of our approach.},
keywords={Detectors;Visualization;Feature extraction;Vectors;Vocabulary;Target tracking;Encoding},
doi={10.1109/CVPR.2012.6247695},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247696,
author={Xiong, Ying and Saenko, Kate and Darrell, Trevor and Zickler, Todd},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={From pixels to physics: Probabilistic color de-rendering},
year={2012},
volume={},
number={},
pages={358-365},
abstract={Consumer digital cameras use tone-mapping to produce compact, narrow-gamut images that are nonetheless visually pleasing. In doing so, they discard or distort substantial radiometric signal that could otherwise be used for computer vision. Existing methods attempt to undo these effects through deterministic maps that de-render the reported narrow-gamut colors back to their original wide-gamut sensor measurements. Deterministic approaches are unreliable, however, because the reverse narrow-to-wide mapping is one-to-many and has inherent uncertainty. Our solution is to use probabilistic maps, providing uncertainty estimates useful to many applications. We use a non-parametric Bayesian regression technique - local Gaussian process regression - to learn for each pixel's narrow-gamut color a probability distribution over the scene colors that could have created it. Using a variety of consumer cameras we show that these distributions, once learned from training data, are effective in simple probabilistic adaptations of two popular applications: multi-exposure imaging and photometric stereo. Our results on these applications are better than those of corresponding deterministic approaches, especially for saturated and out-of-gamut colors.},
keywords={Image color analysis;Transform coding;Colored noise;Cameras;Color;Probabilistic logic;Radiometry},
doi={10.1109/CVPR.2012.6247696},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247697,
author={Wu, Di and O'Toole, Matthew and Velten, Andreas and Agrawal, Amit and Raskar, Ramesh},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Decomposing global light transport using time of flight imaging},
year={2012},
volume={},
number={},
pages={366-373},
abstract={Global light transport is composed of direct and indirect components. In this paper, we take the first steps toward analyzing light transport using high temporal resolution information via time of flight (ToF) images. The time profile at each pixel encodes complex interactions between the incident light and the scene geometry with spatially-varying material properties. We exploit the time profile to decompose light transport into its constituent direct, subsurface scattering, and interreflection components. We show that the time profile is well modelled using a Gaussian function for the direct and interreflection components, and a decaying exponential function for the subsurface scattering component. We use our direct, subsurface scattering, and interreflection separation algorithm for four computer vision applications: recovering projective depth maps, identifying subsurface scattering objects, measuring parameters of analytical subsurface scattering models, and performing edge detection using ToF images.},
keywords={Scattering;Lighting;Image edge detection;Geometry;Light sources;Cameras},
doi={10.1109/CVPR.2012.6247697},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247698,
author={Torralba, Antonio and Freeman, William T.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Accidental pinhole and pinspeck cameras: Revealing the scene outside the picture},
year={2012},
volume={},
number={},
pages={374-381},
abstract={We identify and study two types of “accidental” images that can be formed in scenes. The first is an accidental pinhole camera image. These images are often mistaken for shadows, but can reveal structures outside a room, or the unseen shape of the light aperture into the room. The second class of accidental images are “inverse” pinhole camera images, formed by subtracting an image with a small occluder present from a reference image without the occluder. The reference image can be an earlier frame of a video sequence. Both types of accidental images happen in a variety of different situations (an indoor scene illuminated by natural light, a street with a person walking under the shadow of a building, etc.). Accidental cameras can reveal information about the scene outside the image, the lighting conditions, or the aperture by which light enters the scene.},
keywords={Cameras;Apertures;Shape;Signal to noise ratio;Lighting;Sensors},
doi={10.1109/CVPR.2012.6247698},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247699,
author={Gallagher, Andrew C.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Jigsaw puzzles with pieces of unknown orientation},
year={2012},
volume={},
number={},
pages={382-389},
abstract={This paper introduces new types of square-piece jigsaw puzzles: those for which the orientation of each jigsaw piece is unknown. We propose a tree-based reassembly that greedily merges components while respecting the geometric constraints of the puzzle problem. The algorithm has state-of-the-art performance for puzzle assembly, whether or not the orientation of the pieces is known. Our algorithm makes fewer assumptions than past work, and success is shown even when pieces from multiple puzzles are mixed together. For solving puzzles where jigsaw piece location is known but orientation is unknown, we propose a pairwise MRF where each node represents a jigsaw piece's orientation. Other contributions of the paper include an improved measure (MGC) for quantifying the compatibility of potential jigsaw piece matches based on expecting smoothness in gradient distributions across boundaries.},
keywords={Assembly;Image color analysis;Vegetation;Shape;Image edge detection;Merging;Color},
doi={10.1109/CVPR.2012.6247699},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247700,
author={Bryner, Darshan and Srivastava, Anuj and Klassen, Eric},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Affine-invariant, elastic shape analysis of planar contours},
year={2012},
volume={},
number={},
pages={390-397},
abstract={We present a Riemannian framework for analyzing shapes of planar contours in which metrics and other analyses are invariant to affine transformations and re-parameterizations of contours. Current methods that are affine invariant are restricted to point sets and do not handle full curves, while methods that analyze parameterized curves are restricted to equivalence under similarity transformation (rigid motion and scale). We construct a pre-shape manifold of standardized curves - curves whose centroid is at the origin, are of unit length, and their x and y coordinates are uncorrelated - and develop a path-straightening technique for computing geodesics on this nonlinear manifold under the elastic Riemannian metric. The removal of the rotation and the re-parameterization groups results in a quotient space, termed affine elastic shape space, and the resulting geodesic paths exhibit an improved matching of features across curves. These geodesics are used for shape comparison, retrieval, and statistical modeling of given curves. Experimental results using both simulated and real data, and an application involving pose-invariant activity recognition, demonstrate the success of this framework.},
keywords={Shape;Space vehicles;Measurement;Vectors;Manifolds;Standardization;Orbits},
doi={10.1109/CVPR.2012.6247700},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247701,
author={Cho, Minsu and Lee, Kyoung Mu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Progressive graph matching: Making a move of graphs via probabilistic voting},
year={2012},
volume={},
number={},
pages={398-405},
abstract={Graph matching is widely used in a variety of scientific fields, including computer vision, due to its powerful performance, robustness, and generality. Its computational complexity, however, limits the permissible size of input graphs in practice. Therefore, in real-world applications, the initial construction of graphs to match becomes a critical factor for the matching performance, and often leads to unsatisfactory results. In this paper, to resolve the issue, we propose a novel progressive framework which combines probabilistic progression of graphs with matching of graphs. The algorithm efficiently re-estimates in a Bayesian manner the most plausible target graphs based on the current matching result, and guarantees to boost the matching objective at the subsequent graph matching. Experimental evaluation demonstrates that our approach effectively handles the limits of conventional graph matching and achieves significant improvement in challenging image matching problems.},
keywords={Robustness;Bayesian methods;Image matching;Space exploration;Probabilistic logic;Approximation algorithms;Impedance matching},
doi={10.1109/CVPR.2012.6247701},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247702,
author={Eslami, S. M. Ali and Heess, Nicolas and Winn, John},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The Shape Boltzmann Machine: A strong model of object shape},
year={2012},
volume={},
number={},
pages={406-413},
abstract={A good model of object shape is essential in applications such as segmentation, object detection, inpainting and graphics. For example, when performing segmentation, local constraints on the shape can help where the object boundary is noisy or unclear, and global constraints can resolve ambiguities where background clutter looks similar to part of the object. In general, the stronger the model of shape, the more performance is improved. In this paper, we use a type of Deep Boltzmann Machine [22] that we call a Shape Boltzmann Machine (ShapeBM) for the task of modeling binary shape images. We show that the ShapeBM characterizes a strong model of shape, in that samples from the model look realistic and it can generalize to generate samples that differ from training examples. We find that the ShapeBM learns distributions that are qualitatively and quantitatively better than existing models for this task.},
keywords={Shape;Training;Mathematical model;Legged locomotion;Analytical models;Educational institutions;Image segmentation},
doi={10.1109/CVPR.2012.6247702},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247703,
author={Leifman, George and Shtrom, Elizabeth and Tal, Ayellet},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Surface regions of interest for viewpoint selection},
year={2012},
volume={},
number={},
pages={414-421},
abstract={While the detection of the interesting regions in images has been extensively studied, relatively few papers have addressed surfaces. This paper proposes an algorithm for detecting the regions of interest of surfaces. It looks for regions that are distinct both locally and globally and accounts for the distance to the foci of attention. Many applications can utilize these regions. In this paper we explore one such application - viewpoint selection. The most informative views are those that collectively provide the most descriptive presentation of the surface. We show that our results compete favorably with the state-of-the-art results.},
keywords={Extremities;Shape;Facial features;Humans;Geometry;Histograms},
doi={10.1109/CVPR.2012.6247703},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247704,
author={Ernst, Jan and Singh, Maneesh K. and Ramesh, Visvanathan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discrete texture traces: Topological representation of geometric context},
year={2012},
volume={},
number={},
pages={422-429},
abstract={Modeling representations of image patches that are quasi-invariant to spatial deformations is an important problem in computer vision. In this paper, we propose a novel concept, the texture trace, that allows sparse patch representations which are quasi-invariant to smooth deformations and robust against occlusions. We first propose a continuous domain model, the profile trace, which is a function only of the topological properties of an image and is by construction invariant to any homeomorphic transformation of the domain. We analyze its theoretical properties and then derive a discrete-domain approximation, the Discrete Texture Trace (DTT). DTTs are designed to be computationally practical and shown by a set of controlled experiments to be quasi-invariant to smooth spatial deformations as well as common image perturbations. We then show how DTTs can be naturally adapted to the incremental tracking problem, yielding highly precise results on par with the state of the art on challenging real data without using heavy machine learning tools. Indeed, we show that with even just using one image at the start of a sequence (i.e. no incremental updating), our method already outperforms four of six state of the art methods of the recent literature on challenging sequences.},
keywords={Computational modeling;Approximation methods;Noise;Mathematical model;Image edge detection;Tracking;Deformable models},
doi={10.1109/CVPR.2012.6247704},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247705,
author={Lu, Jiangbo and Shi, Keyang and Min, Dongbo and Lin, Liang and Do, Minh N.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Cross-based local multipoint filtering},
year={2012},
volume={},
number={},
pages={430-437},
abstract={This paper presents a cross-based framework of performing local multipoint filtering efficiently. We formulate the filtering process as a local multipoint regression problem, consisting of two main steps: 1) multipoint estimation, calculating the estimates for a set of points within a shape-adaptive local support, and 2) aggregation, fusing a number of multipoint estimates available for each point. Compared with the guided filter that applies the linear regression to all pixels covered by a fixed-sized square window non-adaptively, the proposed filtering framework is a more generalized form. Two specific filtering methods are instantiated from this framework, based on piecewise constant and piecewise linear modeling, respectively. Leveraging a cross-based local support representation and integration technique, the proposed filtering methods achieve theoretically strong results in an efficient manner, with the two main steps' complexity independent of the filtering kernel size. We demonstrate the strength of the proposed filters in various applications including stereo matching, depth map enhancement, edge-preserving smoothing, color image denoising, detail enhancement, and flash/no-flash denoising.},
keywords={Smoothing methods;Image edge detection;Color;Linear regression;Noise;Computational modeling;Polynomials},
doi={10.1109/CVPR.2012.6247705},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247706,
author={Borji, Ali},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Boosting bottom-up and top-down visual features for saliency estimation},
year={2012},
volume={},
number={},
pages={438-445},
abstract={Despite significant recent progress, the best available visual saliency models still lag behind human performance in predicting eye fixations in free-viewing of natural scenes. Majority of models are based on low-level visual features and the importance of top-down factors has not yet been fully explored or modeled. Here, we combine low-level features such as orientation, color, intensity, saliency maps of previous best bottom-up models with top-down cognitive visual features (e.g., faces, humans, cars, etc.) and learn a direct mapping from those features to eye fixations using Regression, SVM, and AdaBoost classifiers. By extensive experimenting over three benchmark eye-tracking datasets using three popular evaluation scores, we show that our boosting model outperforms 27 state-of-the-art models and is so far the closest model to the accuracy of human model for fixation prediction. Furthermore, our model successfully detects the most salient object in a scene without sophisticated image processings such as region segmentation.},
keywords={Humans;Predictive models;Image color analysis;Visualization;Feature extraction;Computational modeling;Support vector machines},
doi={10.1109/CVPR.2012.6247706},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247707,
author={Park, Jaesik and Tai, Yu-Wing and Kweon, In So},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Identigram/watermark removal using cross-channel correlation},
year={2012},
volume={},
number={},
pages={446-453},
abstract={We introduce a method to repair an image which has been stamped by an identigram or a watermark. Our method is based on the cross-channel correlation which assures the co-occurrence of image discontinuities and correlation of color distributions across different color channels of an image. Using blind source separation, we find the transformation of color space which separates the structures of identigram and that of the original image into two different individual color channels. To repair the image contents in the corrupted channel, we formulate the problem using Bayes' rule where the prior and the likelihood probabilities are defined based on the cross-channel correlation assumption. We compare our results with results from inpainting and texture synthesis-based hole filling techniques. Our results are pleasable for real-world examples and have the maximum PSNR for synthetic examples.},
keywords={Image color analysis;Watermarking;Maintenance engineering;Correlation;Equations;Covariance matrix;Algorithm design and analysis},
doi={10.1109/CVPR.2012.6247707},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247708,
author={Niu, Yuzhen and Geng, Yujie and Li, Xueqing and Liu, Feng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Leveraging stereopsis for saliency analysis},
year={2012},
volume={},
number={},
pages={454-461},
abstract={Stereopsis provides an additional depth cue and plays an important role in the human vision system. This paper explores stereopsis for saliency analysis and presents two approaches to stereo saliency detection from stereoscopic images. The first approach computes stereo saliency based on the global disparity contrast in the input image. The second approach leverages domain knowledge in stereoscopic photography. A good stereoscopic image takes care of its disparity distribution to avoid 3D fatigue. Particularly, salient content tends to be positioned in the stereoscopic comfort zone to alleviate the vergence-accommodation conflict. Accordingly, our method computes stereo saliency of an image region based on the distance between its perceived location and the comfort zone. Moreover, we consider objects popping out from the screen salient as these objects tend to catch a viewer's attention. We build a stereo saliency analysis benchmark dataset that contains 1000 stereoscopic images with salient object masks. Our experiments on this dataset show that stereo saliency provides a useful complement to existing visual saliency analysis and our method can successfully detect salient content from images that are difficult for monocular saliency analysis methods.},
keywords={Stereo image processing;Visualization;Cameras;Photography;Benchmark testing;Image color analysis},
doi={10.1109/CVPR.2012.6247708},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247709,
author={Liu, Xian-Ming and Wang, Changhu and Yao, Hongxun and Zhang, Lei},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The scale of edges},
year={2012},
volume={},
number={},
pages={462-469},
abstract={Although the scale of isotropic visual elements such as blobs and interest points, e.g. SIFT[12], has been well studied and adopted in various applications, how to determine the scale of anisotropic elements such as edges is still an open problem. In this paper, we study the scale of edges, and try to answer two questions: 1) what is the scale of edges, and 2) how to calculate it. From the points of human cognition and physical interpretation, we illustrate the existence of the scale of edges and provide a quantitative definition. Then, an automatic edge scale selection approach is proposed. Finally, a cognitive experiment is conducted to validate the rationality of the detected scales. Moreover, the importance of identifying the scale of edges is also shown in applications such as boundary detection and hierarchical edge parsing.},
keywords={Image edge detection;Image resolution;Humans;Detectors;Heating;Anisotropic magnetoresistance;Visualization},
doi={10.1109/CVPR.2012.6247709},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247710,
author={Borji, Ali and Sihite, Dicky N. and Itti, Laurent},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Probabilistic learning of task-specific visual attention},
year={2012},
volume={},
number={},
pages={470-477},
abstract={Despite a considerable amount of previous work on bottom-up saliency modeling for predicting human fixations over static and dynamic stimuli, few studies have thus far attempted to model top-down and task-driven influences of visual attention. Here, taking advantage of the sequential nature of real-world tasks, we propose a unified Bayesian approach for modeling task-driven visual attention. Several sources of information, including global context of a scene, previous attended locations, and previous motor actions, are integrated over time to predict the next attended location. Recording eye movements while subjects engage in 5 contemporary 2D and 3D video games, as modest counterparts of everyday tasks, we show that our approach is able to predict human attention and gaze better than the state-of-the-art, with a large margin (about 15% increase in prediction accuracy). The advantage of our approach is that it is automatic and applicable to arbitrary visual tasks.},
keywords={Games;Visualization;Computational modeling;Predictive models;Bayesian methods;Vectors;Context},
doi={10.1109/CVPR.2012.6247710},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247711,
author={Borji, Ali and Itti, Laurent},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Exploiting local and global patch rarities for saliency detection},
year={2012},
volume={},
number={},
pages={478-485},
abstract={We introduce a saliency model based on two key ideas. The first one is considering local and global image patch rarities as two complementary processes. The second one is based on our observation that for different images, one of the RGB and Lab color spaces outperforms the other in saliency detection. We propose a framework that measures patch rarities in each color space and combines them in a final map. For each color channel, first, the input image is partitioned into non-overlapping patches and then each patch is represented by a vector of coefficients that linearly reconstruct it from a learned dictionary of patches from natural scenes. Next, two measures of saliency (Local and Global) are calculated and fused to indicate saliency of each patch. Local saliency is distinctiveness of a patch from its surrounding patches. Global saliency is the inverse of a patch's probability of happening over the entire image. The final saliency map is built by normalizing and fusing local and global saliency maps of all channels from both color systems. Extensive evaluation over four benchmark eye-tracking datasets shows the significant advantage of our approach over 10 state-of-the-art saliency models.},
keywords={Image color analysis;Computational modeling;Humans;Visualization;Dictionaries;Adaptation models;Vectors},
doi={10.1109/CVPR.2012.6247711},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247712,
author={Segev, Dana and Schechner, Yoav Y. and Elad, Michael},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Example-based cross-modal denoising},
year={2012},
volume={},
number={},
pages={486-493},
abstract={Widespread current cameras are part of multisensory systems with an integrated computer (smartphones). Computer vision thus starts evolving to cross-modal sensing, where vision and other sensors cooperate. This exists in humans and animals, reflecting nature, where visual events are often accompanied with sounds. Can vision assist in denoising another modality? As a case study, we demonstrate this principle by using video to denoise audio. Unimodal (audio-only) denoising is very difficult when the noise source is non-stationary, complex (e.g., another speaker or music in the background), strong and not individually accessible in any modality (unseen). Cross-modal association can help: a clear video can direct the audio estimator. We show this using an example-based approach. A training movie having clear audio provides cross-modal examples. In testing, cross-modal input segments having noisy audio rely on the examples for denoising. The video channel drives the search for relevant training examples. We demonstrate this in speech and music experiments.},
keywords={Visualization;Vectors;Noise;Noise reduction;Training;Feature extraction;Speech},
doi={10.1109/CVPR.2012.6247712},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247713,
author={Viola, Fabio and Fitzgibbon, Andrew and Cipolla, Roberto},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A unifying resolution-independent formulation for early vision},
year={2012},
volume={},
number={},
pages={494-501},
abstract={We present a model for early vision tasks such as denoising, super-resolution, deblurring, and demosaicing. The model provides a resolution-independent representation of discrete images which admits a truly rotationally invariant prior. The model generalizes several existing approaches: variational methods, finite element methods, and discrete random fields. The primary contribution is a novel energy functional which has not previously been written down, which combines the discrete measurements from pixels with a continuous-domain world viewed through continous-domain point-spread functions. The value of the functional is that simple priors (such as total variation and generalizations) on the continous-domain world become realistic priors on the sampled images. We show that despite its apparent complexity, optimization of this model depends on just a few computational primitives, which although tedious to derive, can now be reused in many domains. We define a set of optimization algorithms which greatly overcome the apparent complexity of this model, and make possible its practical application. New experimental results include infinite-resolution upsampling, and a method for obtaining “subpixel superpixels”.},
keywords={Kernel;Image resolution;Image edge detection;Optimization;Image reconstruction;Integral equations;Computational modeling},
doi={10.1109/CVPR.2012.6247713},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247714,
author={Nishigaki, Morimichi and Fermüller, Cornelia and DeMenthon, Daniel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The image torque operator: A new tool for mid-level vision},
year={2012},
volume={},
number={},
pages={502-509},
abstract={Contours are a powerful cue for semantic image understanding. Objects and parts of objects in the image are delineated from their surrounding by closed contours which make up their boundary. In this paper we introduce a new bottom-up visual operator to capture the concept of closed contours, which we call the `Torque' operator. Its computation is inspired by the mechanical definition of torque or moment of force, and applied to image edges. The torque operator takes as input edges and computes over regions of different size a measure of how well the edges are aligned to form a closed, convex contour. We explore fundamental properties of this measure and demonstrate that it can be made a useful tool for visual attention, segmentation, and boundary edge detection by verifying its benefits on these applications.},
keywords={Torque;Image edge detection;Vectors;Visualization;Image segmentation;Torque measurement;Force},
doi={10.1109/CVPR.2012.6247714},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247715,
author={Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={FREAK: Fast Retina Keypoint},
year={2012},
volume={},
number={},
pages={510-517},
abstract={A large number of vision applications rely on matching keypoints across images. The last decade featured an arms-race towards faster and more robust keypoints and association algorithms: Scale Invariant Feature Transform (SIFT)[17], Speed-up Robust Feature (SURF)[4], and more recently Binary Robust Invariant Scalable Keypoints (BRISK)[I6] to name a few. These days, the deployment of vision algorithms on smart phones and embedded devices with low memory and computation complexity has even upped the ante: the goal is to make descriptors faster to compute, more compact while remaining robust to scale, rotation and noise. To best address the current requirements, we propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Keypoint (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. Our experiments show that FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are thus competitive alternatives to existing keypoints in particular for embedded applications.},
keywords={Retina;Robustness;Humans;Detectors;Vectors;Kernel;Noise},
doi={10.1109/CVPR.2012.6247715},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247716,
author={Simon, Loic and Teboul, Olivier and Koutsourakis, Panagiotis and Van Gool, Luc and Paragios, Nikos},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Parameter-free/Pareto-driven procedural 3D reconstruction of buildings from ground-level sequences},
year={2012},
volume={},
number={},
pages={518-525},
abstract={In this paper we address multi-view reconstruction of urban environments using 3D shape grammars. Our formulation expresses the solution to the problem as a shape grammar parse tree where both the tree and the corresponding derivation parameters are unknown. Besides the grammar constraint, the solution is guided by an image support that is twofold. First, we seek for a derivation that induces optimal semantic partitions in the different views. Second, using structure-from-motion, noisy depth maps can be determined towards minimizing their distance from to the ones predicted by any potential solution. We show how the underlying data structure can be efficiently optimized using evolutionary algorithms with automatic parameter selection. To the best of our knowledge, it is the first time that the multi-view 3D procedural modeling problem is tackled. Promising results demonstrate the potentials of the method towards producing a compact representation of urban environments.},
keywords={Grammar;Shape;Layout;Buildings;Semantics;Evolutionary computation;Solid modeling},
doi={10.1109/CVPR.2012.6247716},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247717,
author={Zhao, Peng and Yang, Lei and Zhang, Honghui and Quan, Long},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Per-pixel translational symmetry detection, optimization, and segmentation},
year={2012},
volume={},
number={},
pages={526-533},
abstract={We present a novel method for translational symmetry detection, optimization, and symmetry object segmentation in façade images. Unlike most previous methods, our detection algorithm accumulates pixel-level correspondence in translation space. Thus it does not rely on feature point detection and handles patterns with low repetition counts. To improve the robustness with multiple interfering symmetries, we introduce an image-space global optimization, which resolves multiple per-pixel symmetry lattices. We then propose a learning-based method that generates refined segmentation of foreground symmetry objects of arbitrary shapes, with the aid of the per-pixel symmetry information. Our proposed method is accurate, robust and efficient as demonstrated by an extensive evaluation using a large façade image database.},
keywords={Lattices;Vectors;Feature extraction;Optimization;Image segmentation;Transforms;Robustness},
doi={10.1109/CVPR.2012.6247717},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247718,
author={Oswald, Martin R. and Töppe, Eno and Cremers, Daniel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast and globally optimal single view reconstruction of curved objects},
year={2012},
volume={},
number={},
pages={534-541},
abstract={We propose a novel algorithmic solution for estimating a three-dimensional model of an object observed in a single image. Based on a minimal user input, the algorithm interactively determines the objects' silhouette and subsequently computes a silhouette-consistent 3D model which is precisely the globally minimal surface with user-specified volume. In contrast to a recently published approach to single view reconstruction, the proposed algorithm does not constrain the resolution in the depth-direction, it assures the global optimum and is faster by about an order of magnitude. Experiments demonstrate that plausible high-resolution 3D models can be generated in fractions of a second and compare favorably with other methods.},
keywords={Image reconstruction;Surface reconstruction;Optimization;Shape;Runtime;Solid modeling;Minimization},
doi={10.1109/CVPR.2012.6247718},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247719,
author={Joulin, Armand and Bach, Francis and Ponce, Jean},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-class cosegmentation},
year={2012},
volume={},
number={},
pages={542-549},
abstract={Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision. In the cosegmentation context, on the other hand, the availability of multiple images assumed to contain instances of the same object classes provides a weak form of supervision that can be exploited by discriminative approaches. Unfortunately, most existing algorithms are limited to a very small number of images and/or object classes (typically two of each). This paper proposes a novel energy-minimization approach to cosegmentation that can handle multiple classes and a significantly larger number of images. The proposed cost function combines spectral- and discriminative-clustering terms, and it admits a probabilistic interpretation. It is optimized using an efficient EM method, initialized using a convex quadratic approximation of the energy. Comparative experiments show that the proposed approach matches or improves the state of the art on several standard datasets.},
keywords={Cost function;Image segmentation;Approximation methods;Standards;Nickel;Image color analysis;Probabilistic logic},
doi={10.1109/CVPR.2012.6247719},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247720,
author={Pham, Duc-Son and Budhaditya, Saha and Phung, Dinh and Venkatesh, Svetha},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Improved subspace clustering via exploitation of spatial constraints},
year={2012},
volume={},
number={},
pages={550-557},
abstract={We present a novel approach to improving subspace clustering by exploiting the spatial constraints. The new method encourages the sparse solution to be consistent with the spatial geometry of the tracked points, by embedding weights into the sparse formulation. By doing so, we are able to correct sparse representations in a principled manner without introducing much additional computational cost. We discuss alternative ways to treat the missing and corrupted data using the latest theory in robust lasso regression and suggest numerical algorithms so solve the proposed formulation. The experiments on the benchmark Johns Hopkins 155 dataset demonstrate that exploiting spatial constraints significantly improves motion segmentation.},
keywords={Computer vision;Motion segmentation;Sparse matrices;Clustering algorithms;Robustness;Kernel;Noise},
doi={10.1109/CVPR.2012.6247720},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247721,
author={Kuettel, Daniel and Ferrari, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Figure-ground segmentation by transferring window masks},
year={2012},
volume={},
number={},
pages={558-565},
abstract={We present a novel technique for figure-ground segmentation, where the goal is to separate all foreground objects in a test image from the background. We decompose the test image and all images in a supervised training set into overlapping windows likely to cover foreground objects. The key idea is to transfer segmentation masks from training windows that are visually similar to windows in the test image. These transferred masks are then used to derive the unary potentials of a binary, pairwise energy function defined over the pixels of the test image, which is minimized with standard graph-cuts. This results in a fully automatic segmentation scheme, as opposed to interactive techniques based on similar energy functions. Using windows as support regions for transfer efficiently exploits the training data, as the test image does not need to be globally similar to a training image for the method to work. This enables to compose novel scenes using local parts of training images. Our approach obtains very competitive results on three datasets (PASCAL VOC 2010 segmentation challenge, Weizmann horses, Graz-02).},
keywords={Image segmentation;Training;Computational modeling;Labeling;Pipelines;Training data;Minimization},
doi={10.1109/CVPR.2012.6247721},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247722,
author={Türetken, Engin and Benmansour, Fethallah and Fua, Pascal},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automated reconstruction of tree structures using path classifiers and Mixed Integer Programming},
year={2012},
volume={},
number={},
pages={566-573},
abstract={Although tracing linear structures in 2D images and 3D image stacks has received much attention over the years, full automation remains elusive. In this paper, we formulate the delineation problem as one of solving a Quadratic Mixed Integer Program (Q-MIP) in a graph of potential paths, which can be done optimally up to a very small tolerance. We further propose a novel approach to weighting these paths, which results in a Q-MIP solution that accurately matches the ground truth. We demonstrate that our approach outperforms a state-of-the-art technique based on the k-Minimum Spanning Tree formulation on a 2D dataset of aerial images and a 3D dataset of confocal microscopy stacks.},
keywords={Histograms;Joining processes;Image edge detection;Image reconstruction;Robustness;Vectors;Noise},
doi={10.1109/CVPR.2012.6247722},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247723,
author={Liu, Hairong and Yan, Shuicheng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient structure detection via random consensus graph},
year={2012},
volume={},
number={},
pages={574-581},
abstract={In this paper, we propose an efficient method to detect the underlying structures in data. The same as RANSAC, we randomly sample MSSs (minimal size samples) and generate hypotheses. Instead of analyzing each hypothesis separately, the consensus information in all hypotheses is naturally fused into a hypergraph, called random consensus graph, with real structures corresponding to its dense subgraphs. The sampling process is essentially a progressive refinement procedure of the random consensus graph. Due to the huge number of hyperedges, it is generally inefficient to detect dense subgraphs on random consensus graphs. To overcome this issue, we construct a pairwise graph which approximately retains the dense subgraphs of the random consensus graph. The underlying structures are then revealed by detecting the dense subgraphs of the pair-wise graph. Since our method fuses information from all hypotheses, it can robustly detect structures even under a small number of MSSs. The graph framework enables our method to simultaneously discover multiple structures. Besides, our method is very efficient, and scales well for large scale problems. Extensive experiments illustrate the superiority of our proposed method over previous approaches, achieving several orders of magnitude speedup along with satisfactory accuracy and robustness.},
keywords={Vectors;Robustness;Periodic structures;Complexity theory;Noise;Image edge detection;Approximation methods},
doi={10.1109/CVPR.2012.6247723},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247724,
author={Zhang, Yimeng and Chen, Tsuhan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient inference for fully-connected CRFs with stationarity},
year={2012},
volume={},
number={},
pages={582-589},
abstract={The Conditional Random Field (CRF) is a popular tool for object-based image segmentation. CRFs used in practice typically have edges only between adjacent image pixels. To represent object relationship statistics beyond adjacent pixels, prior work either represents only weak spatial information using the segmented regions, or encodes only global object co-occurrences. In this paper, we propose a unified model that augments the pixel-wise CRFs to capture object spatial relationships. To this end, we use a fully connected CRF, which has an edge for each pair of pixels. The edge potentials are defined to capture the spatial information and preserve the object boundaries at the same time. Traditional inference methods, such as belief propagation and graph cuts, are impractical in such a case where billions of edges are defined. Under only one assumption that the spatial relationships among different objects only depend on their relative positions (spatially stationary), we develop an efficient inference algorithm that converges in a few seconds on a standard resolution image, where belief propagation takes more than one hour for a single iteration.},
keywords={Image color analysis;Image edge detection;Image segmentation;Inference algorithms;Face;Context;Object segmentation},
doi={10.1109/CVPR.2012.6247724},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247725,
author={Kuang, Zhanghui and Schnieders, Dirk and Zhou, Hao and Wong, Kwan-Yee K. and Yu, Yizhou and Peng, Bo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning image-specific parameters for interactive segmentation},
year={2012},
volume={},
number={},
pages={590-597},
abstract={In this paper, we present a novel interactive image segmentation technique that automatically learns segmentation parameters tailored for each and every image. Unlike existing work, our method does not require any offline parameter tuning or training stage, and is capable of determining image-specific parameters according to some simple user interactions with the target image. We formulate the segmentation problem as an inference of a conditional random field (CRF) over a segmentation mask and the target image, and parametrize this CRF by different weights (e.g., color, texture and smoothing). The weight parameters are learned via an energy margin maximization, which is solved using a constraint approximation scheme and the cutting plane method. Experimental results show that our method, by learning image-specific parameters automatically, outperforms other state-of-the-art interactive image segmentation techniques.},
keywords={Image segmentation;Image color analysis;Smoothing methods;Approximation methods;Training;Learning systems;Indexes},
doi={10.1109/CVPR.2012.6247725},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247726,
author={Liu, Risheng and Lin, Zhouchen and De la Torre, Fernando and Su, Zhixun},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fixed-rank representation for unsupervised visual learning},
year={2012},
volume={},
number={},
pages={598-605},
abstract={Subspace clustering and feature extraction are two of the most commonly used unsupervised learning techniques in computer vision and pattern recognition. State-of-the-art techniques for subspace clustering make use of recent advances in sparsity and rank minimization. However, existing techniques are computationally expensive and may result in degenerate solutions that degrade clustering performance in the case of insufficient data sampling. To partially solve these problems, and inspired by existing work on matrix factorization, this paper proposes fixed-rank representation (FRR) as a unified framework for unsupervised visual learning. FRR is able to reveal the structure of multiple subspaces in closed-form when the data is noiseless. Furthermore, we prove that under some suitable conditions, even with insufficient observations, FRR can still reveal the true subspace memberships. To achieve robustness to outliers and noise, a sparse regularizer is introduced into the FRR framework. Beyond subspace clustering, FRR can be used for unsupervised feature extraction. As a non-trivial byproduct, a fast numerical solver is developed for FRR. Experimental results on both synthetic data and real applications validate our theoretical analysis and demonstrate the benefits of FRR for unsupervised visual learning.},
keywords={Feature extraction;Principal component analysis;Visualization;Minimization;Noise;Vectors;Clustering algorithms},
doi={10.1109/CVPR.2012.6247726},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247727,
author={Cho, Minsu and Lee, Kyoung Mu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Mode-seeking on graphs via random walks},
year={2012},
volume={},
number={},
pages={606-613},
abstract={Mode-seeking has been widely used as a powerful data analysis technique for clustering and filtering in a metric feature space. We introduce a versatile and efficient mode-seeking method for “graph” representation where general embedding of relational data is possible beyond metric spaces. Exploiting the global structure of the graph by random walks, our method intrinsically combines mode-seeking with ranking on the graph, and performs robust analysis by seeking high-ranked authoritative data and suppressing low-ranked noise and outliers. This enables mode-seeking to be applied to a large class of challenging real-world problems involving graph representation which frequently arises in computer vision. We demonstrate our method on various synthetic experiments and real applications dealing with noisy and complex data such as scene summarization and object-based image matching.},
keywords={Extraterrestrial measurements;Noise measurement;Clustering algorithms;Noise;Robustness;Complexity theory},
doi={10.1109/CVPR.2012.6247727},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247728,
author={Ochs, Peter and Brox, Thomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Higher order motion models and spectral clustering},
year={2012},
volume={},
number={},
pages={614-621},
abstract={Motion segmentation based on point trajectories can integrate information of a whole video shot to detect and separate moving objects. Commonly, similarities are defined between pairs of trajectories. However, pairwise similarities restrict the motion model to translations. Non-translational motion, such as rotation or scaling, is penalized in such an approach. We propose to define similarities on higher order tuples rather than pairs, which leads to hypergraphs. To apply spectral clustering, the hypergraph is transferred to an ordinary graph, an operation that can be interpreted as a projection. We propose a specific nonlinear projection via a regularized maximum operator, and show that it yields significant improvements both compared to pairwise similarities and alternative hypergraph projections.},
keywords={Trajectory;Computational modeling;Motion segmentation;Computer vision;Tensile stress;Laplace equations;Complexity theory},
doi={10.1109/CVPR.2012.6247728},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247729,
author={Zitnick, C. Lawrence and Parikh, Devi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The role of image understanding in contour detection},
year={2012},
volume={},
number={},
pages={622-629},
abstract={Many cues have been proposed for contour detection or image segmentation. These include low-level image gradients to high-level information such as the identity of the objects in the scene or 3D depth understanding. While state-of-the-art approaches have been incorporating more cues, the relative importance of the cues is unclear. In this paper, we examine the relative importance of low-, mid- and high-level cues to gain a better understanding of their role in detecting object contours in an image. To accomplish this task, we conduct numerous human studies and compare their performance to several popular segmentation and contour detection machine approaches. Our findings suggest that the current state-of-the-art contour detection algorithms perform as well as humans using low-level cues. We also find evidence that the recognition of objects, but not occlusion information, leads to improved human performance. Moreover, when objects are recognized by humans, their contour detection performance increases over current machine algorithms. Finally, mid-level cues appear to offer a larger performance boost than high-level cues such as recognition.},
keywords={Image segmentation;Humans;Accuracy;Image color analysis;Image edge detection;Labeling;Sun},
doi={10.1109/CVPR.2012.6247729},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247730,
author={Sun, Mengtian and Fang, Yi and Ramani, Karthik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Center-Shift: An approach towards automatic robust mesh segmentation (ARMS)},
year={2012},
volume={},
number={},
pages={630-637},
abstract={In the area of 3D shape analysis, research in mesh segmentation has always been an important topic, as it is a fundamental low-level task which can be utilized in many applications including computer-aided design, computer animation, biomedical applications and many other fields. We define the automatic robust mesh segmentation (ARMS) method in this paper, which 1) is invariant to isometric transformation, 2) is insensitive to noise and deformation, 3) performs closely to human perception, 4) is efficient in computation, and 5) is minimally dependent on prior knowledge. In this work, we develop a new framework, namely the Center-Shift, which discovers meaningful segments of a 3D object by exploring the intrinsic geometric structure encoded in the biharmonic kernel. Our Center-Shift framework has three main steps: First, we construct a feature space where every vertex on the mesh surface is associated with the corresponding biharmonic kernel density function value. Second, we apply the Center-Shift algorithm for initial segmentation. Third, the initial segmentation result is refined through an efficient iterative process which leads to visually salient segmentation of the shape. The performance of this segmentation method is demonstrated through extensive experiments on various sets of 3D shapes and different types of noise and deformation. The experimental results of 3D shape segmentation have shown better performance of Center-Shift, compared to state-of-the-art segmentation methods.},
keywords={Kernel;Shape;Noise;Robustness;Density functional theory;Surface treatment;Heating},
doi={10.1109/CVPR.2012.6247730},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247731,
author={Bazin, Jean-Charles and Seo, Yongduek and Demonceaux, Cédric and Vasseur, Pascal and Ikeuchi, Katsushi and Kweon, Inso and Pollefeys, Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Globally optimal line clustering and vanishing point estimation in Manhattan world},
year={2012},
volume={},
number={},
pages={638-645},
abstract={The projections of world parallel lines in an image intersect at a single point called the vanishing point (VP). VPs are a key ingredient for various vision tasks including rotation estimation and 3D reconstruction. Urban environments generally exhibit some dominant orthogonal VPs. Given a set of lines extracted from a calibrated image, this paper aims to (1) determine the line clustering, i.e. find which line belongs to which VP, and (2) estimate the associated orthogonal VPs. None of the existing methods is fully satisfactory because of the inherent difficulties of the problem, such as the local minima and the chicken-and-egg aspect. In this paper, we present a new algorithm that solves the problem in a mathematically guaranteed globally optimal manner and can inherently enforce the VP orthogonality. Specifically, we formulate the task as a consensus set maximization problem over the rotation search space, and further solve it efficiently by a branch-and-bound procedure based on the Interval Analysis theory. Our algorithm has been validated successfully on sets of challenging real images as well as synthetic data sets.},
keywords={Clustering algorithms;Estimation;Algorithm design and analysis;Educational institutions;Search problems;Calibration;Optimization},
doi={10.1109/CVPR.2012.6247731},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247732,
author={Georgiadis, Georgios and Ayvaci, Alper and Soatto, Stefano},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Actionable saliency detection: Independent motion detection without independent motion estimation},
year={2012},
volume={},
number={},
pages={646-653},
abstract={We present a model and an algorithm to detect salient regions in video taken from a moving camera. In particular, we are interested in capturing small objects that move independently in the scene, such as vehicles and people as seen from aerial or ground vehicles. Many of the scenarios of interest challenge existing schemes based on background subtraction (background motion too complex), multi-body motion estimation (insufficient parallax), and occlusion detection (uniformly textured background regions). We adopt a robust statistical inference approach to simultaneously estimate a maximally reduced regressor, and select regions that violate the null hypothesis (co-visibility under an epipolar domain deformation) as “salient”. We show that our algorithm can perform even in the absence of camera calibration information: while the resulting motion estimates would be incorrect, the partition of the domain into salient vs. non-salient is unaffected. We demonstrate our algorithm on video footage from helicopters, airplanes, and ground vehicles.},
keywords={Cameras;Trajectory;Motion estimation;Robustness;Mathematical model;Visualization;Estimation},
doi={10.1109/CVPR.2012.6247732},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247733,
author={Chen, Yisong and Chan, Antoni B. and Wang, Guoping},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Adaptive figure-ground classification},
year={2012},
volume={},
number={},
pages={654-661},
abstract={We propose an adaptive figure-ground classification algorithm to automatically extract a foreground region using a user-provided bounding-box. The image is first over-segmented with an adaptive mean-shift algorithm, from which background and foreground priors are estimated. The remaining patches are iteratively assigned based on their distances to the priors, with the foreground prior being updated online. A large set of candidate segmentations are obtained by changing the initial foreground prior. The best candidate is determined by a score function that evaluates the segmentation quality. Rather than using a single distance function or score function, we generate multiple hypothesis segmentations from different combinations of distance measures and score functions. The final segmentation is then automatically obtained with a voting or weighted combination scheme from the multiple hypotheses. Experiments indicate that our method performs at or above the current state-of-the-art on several datasets, with particular success on challenging scenes that contain irregular or multiple-connected foregrounds. In addition, this improvement in accuracy is achieved with low computational cost.},
keywords={Delta modulation;Bandwidth;Image segmentation;Classification algorithms;Color;Gaussian distribution;Covariance matrix},
doi={10.1109/CVPR.2012.6247733},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247734,
author={Wang, Nan and Ai, Haizhou and Tang, Feng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={What are good parts for hair shape modeling?},
year={2012},
volume={},
number={},
pages={662-669},
abstract={Hair plays an important role in human appearance. However, hair segmentation is still a challenging problem partially due to the lack of an effective model to handle its arbitrary shape variations. In this paper, we present a part-based model robust to hair shape and environment variations. The key idea of our method is to identify local parts by promoting the effectiveness of the part-based model. To this end, we propose a measurable statistic, called Subspace Clustering Dependency (SC-Dependency), to estimate the co-occurrence probabilities between local shapes. SC-Dependency guarantees output reasonability and allows us to evaluate the effectiveness of part-wise constraints in an information-theoretic way. Then we formulate the part identification problem as an MRF that aims to optimize the effectiveness of the potential functions. Experiments are performed on a set of consumer images and show our algorithm's capability and robustness to handle hair shape variations and extreme environment conditions.},
keywords={Shape;Hair;Vocabulary;Vegetation;Accuracy;Computational modeling;Adaptation models},
doi={10.1109/CVPR.2012.6247734},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247735,
author={Ma, Tianyang and Latecki, Longin Jan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Maximum weight cliques with mutex constraints for video object segmentation},
year={2012},
volume={},
number={},
pages={670-677},
abstract={In this paper, we address the problem of video object segmentation, which is to automatically identify the primary object and segment the object out in every frame. We propose a novel formulation of selecting object region candidates simultaneously in all frames as finding a maximum weight clique in a weighted region graph. The selected regions are expected to have high objectness score (unary potential) as well as share similar appearance (binary potential). Since both unary and binary potentials are unreliable, we introduce two types of mutex (mutual exclusion) constraints on regions in the same clique: intra-frame and inter-frame constraints. Both types of constraints are expressed in a single quadratic form. We propose a novel algorithm to compute the maximal weight cliques that satisfy the constraints. We apply our method to challenging benchmark videos and obtain very competitive results that outperform state-of-the-art methods.},
keywords={Image color analysis;Image segmentation;Object segmentation;Histograms;Computational modeling;Proposals;Shape},
doi={10.1109/CVPR.2012.6247735},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247736,
author={Jiang, Hao},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Linear solution to scale invariant global figure ground separation},
year={2012},
volume={},
number={},
pages={678-685},
abstract={We propose a novel linear method for scale invariant figure ground separation in images and videos. Figure ground separation is treated as a superpixel labeling problem. We optimize superpixel foreground and background labeling so that the object foreground estimation matches model color histogram, its area and perimeter are consistent with object shape prior, and the foreground superpixels form a connected region. This optimization problem is challenging due to high-order soft and hard global constraints among large number of superpixels. We devise a scale invariant linear method that gives an integer solution with a guaranteed error bound via a branch and cut procedure. The proposed method does not rely on motion continuity and works on static images and videos with abrupt motion. Our experimental results on both synthetic ground truth data and real images show that the proposed method is efficient and robust over object appearance changes, large deformation and strong background clutter.},
keywords={Estimation;Histograms;Image color analysis;Labeling;Shape;Optimization;Videos},
doi={10.1109/CVPR.2012.6247736},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247737,
author={Kim, Edward and Li, Hongsheng and Huang, Xiaolei},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A hierarchical image clustering cosegmentation framework},
year={2012},
volume={},
number={},
pages={686-693},
abstract={Given the knowledge that the same or similar objects appear in a set of images, our goal is to simultaneously segment that object from the set of images. To solve this problem, known as the cosegmentation problem, we present a method based upon hierarchical clustering. Our framework first eliminates intra-class heterogeneity in a dataset by clustering similar images together into smaller groups. Then, from each image, our method extracts multiple levels of segmentation and creates connections between regions (e.g. superpixel) across levels to establish intra-image multi-scale constraints. Next we take advantage of the information available from other images in our group. We design and present an efficient method to create inter-image relationships, e.g. connections between image regions from one image to all other images in an image cluster. Given the intra & inter-image connections, we perform a segmentation of the group of images into foreground and background regions. Finally, we compare our segmentation accuracy to several other state-of-the-art segmentation methods on standard datasets, and also demonstrate the robustness of our method on real world data.},
keywords={Image segmentation;Histograms;Image color analysis;Feature extraction;Image edge detection;Vectors;Laplace equations},
doi={10.1109/CVPR.2012.6247737},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247738,
author={Cai, Yunliang and Baciu, George},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Higher level segmentation: Detecting and grouping of invariant repetitive patterns},
year={2012},
volume={},
number={},
pages={694-701},
abstract={The efficient and robust extraction of invariant patterns from an image is a long-standing problem in computer vision. Invariant structures are often related to repetitive or near-repetitive patterns. The perception of repetitive patterns in an image is strongly linked to the visual interpretation and composition of textures. Repetitive patterns are products of both repetitive structures as well as repetitive reflections or color patterns. In other words, patterns that exhibit near-stationary behavior provide a rich information about objects, their shapes, and their texture in an image. In this paper, we propose a new algorithm for repetitive pattern detection and grouping. The algorithm follows the classical region growing image segmentation scheme. It utilizes a mean-shift-like dynamics to group local image patches into clusters. It exploits a continuous joint alignment to (a) match similar patches and (b) refine the subspace grouping. The result of higher-level grouping for image patterns can be used to infer the geometry of object surfaces and estimate the general layout of a crowded scene.},
keywords={Shape;Image segmentation;Transforms;Computational modeling;Optimization;Clustering algorithms;Vectors},
doi={10.1109/CVPR.2012.6247738},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247739,
author={Yao, Jian and Fidler, Sanja and Urtasun, Raquel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation},
year={2012},
volume={},
number={},
pages={702-709},
abstract={In this paper we propose an approach to holistic scene understanding that reasons jointly about regions, location, class and spatial extent of objects, presence of a class in the image, as well as the scene type. Learning and inference in our model are efficient as we reason at the segment level, and introduce auxiliary variables that allow us to decompose the inherent high-order potentials into pairwise potentials between a few variables with small number of states (at most the number of classes). Inference is done via a convergent message-passing algorithm, which, unlike graph-cuts inference, has no submodularity restrictions and does not require potential specific moves. We believe this is very important, as it allows us to encode our ideas and prior knowledge about the problem without the need to change the inference engine every time we introduce a new potential. Our approach outperforms the state-of-the-art on the MSRC-21 benchmark, while being much faster. Importantly, our holistic model is able to improve performance in all tasks.},
keywords={Image segmentation;Shape;Detectors;Object detection;Joints;Random variables;Boats},
doi={10.1109/CVPR.2012.6247739},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247740,
author={Pham, Trung Thanh and Chin, Tat-Jun and Yu, Jin and Suter, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The Random Cluster Model for robust geometric fitting},
year={2012},
volume={},
number={},
pages={710-717},
abstract={Random hypothesis generation is central to robust geometric model fitting in computer vision. The predominant technique is to randomly sample minimal or elemental subsets of the data, and hypothesize the geometric model from the selected subsets. While taking minimal subsets increases the chance of simultaneously “hitting” inliers in a sample, it amplifies the noise of the underlying model, and hypotheses fitted on minimal subsets may be severely biased even if they contain purely inliers. In this paper we propose to use Random Cluster Models, a technique used to simulate coupled spin systems, to conduct hypothesis generation using subsets larger than minimal. We show how large clusters of data from genuine instances of the geometric model can be efficiently harvested to produce more accurate hypotheses. To take advantage of our hypothesis generator, we construct a simple annealing method based on graph cuts to fit multiple instances of the geometric model in the data. Experimental results show clear improvements in efficiency over other methods based on minimal subset samplers.},
keywords={Labeling;Computational modeling;Data models;Generators;Simulated annealing;Computer vision},
doi={10.1109/CVPR.2012.6247740},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247741,
author={Shahrian, Ehsan and Rajan, Deepu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Weighted color and texture sample selection for image matting},
year={2012},
volume={},
number={},
pages={718-725},
abstract={Color information is leveraged by color sampling-based matting methods to find the best known samples for foreground and background color of unknown pixels. Such methods do not perform well if there is an overlap in the color distribution of foreground and background regions because color cannot distinguish between these regions and hence, the selected samples cannot reliably estimate the matte. Similarly, alpha propagation based matting methods may fail when the affinity among neighboring pixels is reduced by strong edges. In this paper, we overcome these two problems by considering texture as a feature that can complement color to improve matting. The contribution of texture and color is automatically estimated by analyzing the content of the image. An objective function containing color and texture components is optimized to choose the best foreground and background pair among a set of candidate pairs. Experiments are carried out on a benchmark data set and an independent evaluation of the results show that the proposed method is ranked first among all other image matting methods.},
keywords={Image color analysis;Feature extraction;Robustness;Bayesian methods;Histograms;Mathematical model},
doi={10.1109/CVPR.2012.6247741},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247742,
author={Hernandez-Vela, Antonio and Zlateva, Nadezhda and Marinov, Alexander and Reyes, Miguel and Radeva, Petia and Dimov, Dimo and Escalera, Sergio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Graph cuts optimization for multi-limb human segmentation in depth maps},
year={2012},
volume={},
number={},
pages={726-732},
abstract={We present a generic framework for object segmentation using depth maps based on Random Forest and Graph-cuts theory, and apply it to the segmentation of human limbs in depth maps. First, from a set of random depth features, Random Forest is used to infer a set of label probabilities for each data sample. This vector of probabilities is used as unary term in α-β swap Graph-cuts algorithm. Moreover, depth of spatio-temporal neighboring data points are used as boundary potentials. Results on a new multi-label human depth data set show high performance in terms of segmentation overlapping of the novel methodology compared to classical approaches.},
keywords={Humans;Radio frequency;Image segmentation;Vegetation;Training;Vectors;Joints},
doi={10.1109/CVPR.2012.6247742},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247743,
author={Perazzi, Federico and Krähenbühl, Philipp and Pritch, Yael and Hornung, Alexander},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Saliency filters: Contrast based filtering for salient region detection},
year={2012},
volume={},
number={},
pages={733-740},
abstract={Saliency estimation has become a valuable tool in image processing. Yet, existing approaches exhibit considerable variation in methodology, and it is often difficult to attribute improvements in result quality to specific algorithm properties. In this paper we reconsider some of the design choices of previous methods and propose a conceptually clear and intuitive algorithm for contrast-based saliency estimation. Our algorithm consists of four basic steps. First, our method decomposes a given image into compact, perceptually homogeneous elements that abstract unnecessary detail. Based on this abstraction we compute two measures of contrast that rate the uniqueness and the spatial distribution of these elements. From the element contrast we then derive a saliency measure that produces a pixel-accurate saliency map which uniformly covers the objects of interest and consistently separates fore- and background. We show that the complete contrast and saliency estimation can be formulated in a unified way using high-dimensional Gaussian filters. This contributes to the conceptual simplicity of our method and lends itself to a highly efficient implementation with linear complexity. In a detailed experimental evaluation we analyze the contribution of each individual feature and show that our method outperforms all state-of-the-art approaches.},
keywords={Image color analysis;Image segmentation;Abstracts;Estimation;Image edge detection;Histograms;Approximation methods},
doi={10.1109/CVPR.2012.6247743},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247744,
author={Cheng, Hsien-Ting and Ahuja, Narendra},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Exploiting nonlocal spatiotemporal structure for video segmentation},
year={2012},
volume={},
number={},
pages={741-748},
abstract={Unsupervised video segmentation is a challenging problem because it involves a large amount of data, and image segments undergo noisy variations in color, texture and motion with time. However, there are significant redundancies that can help disambiguate the effects of noise. To exploit these redundancies and obtain the most spatio-temporally consistent video segmentation, we formulate the problem as a consistent labeling problem by exploiting higher order image structure. A label stands for a specific moving segment. Each segment (or region) is treated as a random variable which is to be assigned a label. Regions assigned the same label comprise a 3D space-time segment, or a region tube. The labels can also be automatically created or terminated at any frame in the video sequence, to allow objects entering or leaving the scene. To formulate this problem, we use the CRF (conditional random field) model. Unlike conventional CRF which has only unary and binary potentials, we also use higher order potentials to favor label consistency among disconnected spatial and temporal segments. Compared to region tracking based methods, the main advantages of the proposed algorithm are two fold: (1) the label consistency constraints are imposed on multiple regions but in a soft manner, and (2) the labeling decision is postponed until the confidence in the labeling is high. We compare our results with a recent state-of-the-art video segmentation algorithm and show that our results are quantitatively and qualitatively better.},
keywords={Image segmentation;Labeling;Electron tubes;Random variables;Merging;Redundancy;Robustness},
doi={10.1109/CVPR.2012.6247744},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247745,
author={Rubio, Jose C. and Serrat, Joan and López, Antonio and Paragios, Nikos},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised co-segmentation through region matching},
year={2012},
volume={},
number={},
pages={749-756},
abstract={Co-segmentation is defined as jointly partitioning multiple images depicting the same or similar object, into foreground and background. Our method consists of a multiple-scale multiple-image generative model, which jointly estimates the foreground and background appearance distributions from several images, in a non-supervised manner. In contrast to other co-segmentation methods, our approach does not require the images to have similar foregrounds and different backgrounds to function properly. Region matching is applied to exploit inter-image information by establishing correspondences between the common objects that appear in the scene. Moreover, computing many-to-many associations of regions allow further applications, like recognition of object parts across images. We report results on iCoseg, a challenging dataset that presents extreme variability in camera viewpoint, illumination and object deformations and poses. We also show that our method is robust against large intra-class variability in the MSRC database.},
keywords={Image segmentation;Labeling;Image color analysis;Dictionaries;Minimization;Visualization;Computational modeling},
doi={10.1109/CVPR.2012.6247745},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247746,
author={Aghazadeh, Omid and Sullivan, Josephine and Carlsson, Stefan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi view registration for novelty/background separation},
year={2012},
volume={},
number={},
pages={757-764},
abstract={We propose a system for the automatic segmentation of novelties from the background in scenarios where multiple images of the same environment are available e.g. obtained by wearable visual cameras. Our method finds the pixels in a query image corresponding to the underlying background environment by comparing it to reference images of the same scene. This is achieved despite the fact that all the images may have different viewpoints, significantly different illumination conditions and contain different objects - cars, people, bicycles, etc. - occluding the background. We estimate the probability of each pixel, in the query image, belonging to the background by computing its appearance inconsistency to the multiple reference images. We then, produce multiple segmentations of the query image using an iterated graph cuts algorithm, initializing from these estimated probabilities and consecutively combine these segmentations to come up with a final segmentation of the background. Detection of the background in turn highlights the novel pixels. We demonstrate the effectiveness of our approach on a challenging outdoors data set.},
keywords={Image segmentation;Vectors;Labeling;Cameras;Lighting;Estimation;Logistics},
doi={10.1109/CVPR.2012.6247746},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247747,
author={Straehle, Christoph-N. and Koethe, Ullrich and Knott, Graham and Briggman, Kevin and Denk, Winfried and Hamprecht, Fred A.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Seeded watershed cut uncertainty estimators for guided interactive segmentation},
year={2012},
volume={},
number={},
pages={765-772},
abstract={Watershed cuts are among the fastest segmentation algorithms and therefore well suited for interactive segmentation of very large 3D data sets. To minimize the number of user interactions (“seeds”) required until the result is correct, we want the computer to actively query the human for input at the most critical locations, in analogy to active learning. These locations are found by means of suitable uncertainty measures. We propose various such measures for watershed cuts along with a theoretical analysis of some of their properties. Extensive evaluation on two types of 3D electron microscopic volumes of neural tissue shows that measures which estimate the non-local consequences of new user inputs achieve performance close to an oracle endowed with complete knowledge of the ground truth.},
keywords={Uncertainty;Measurement uncertainty;Robots;Electron microscopy;Image segmentation;Labeling},
doi={10.1109/CVPR.2012.6247747},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247748,
author={Huang, Hsin-Chien and Chuang, Yung-Yu and Chen, Chu-Song},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Affinity aggregation for spectral clustering},
year={2012},
volume={},
number={},
pages={773-780},
abstract={Spectral clustering makes use of spectral-graph structure of an affinity matrix to partition data into disjoint meaningful groups. Because of its elegance, efficiency and good performance, spectral clustering has become one of the most popular clustering methods. Traditional spectral clustering assumes a single affinity matrix. However, in many applications, there could be multiple potentially useful features and thereby multiple affinity matrices. To apply spectral clustering for these cases, a possible way is to aggregate the affinity matrices into a single one. Unfortunately, affinity measures constructed from different features could have different characteristics. Careless aggregation might make even worse clustering performance. This paper proposes an affinity aggregation spectral clustering (AASC) algorithm which extends spectral clustering to a setting with multiple affinities available. AASC seeks for an optimal combination of affinity matrices so that it is more immune to ineffective affinities and irrelevant features. This enables the construction of similarity or distance-metric measures for clustering less crucial. Experiments show that AASC is effective in simultaneous clustering and feature fusion, thus enhancing the performance of spectral clustering by employing multiple affinities.},
keywords={Equations;Kernel;Clustering algorithms;Vectors;Face;Optimization;Measurement},
doi={10.1109/CVPR.2012.6247748},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247749,
author={Yu, Zhiding and Li, Ang and Au, Oscar C. and Xu, Chunjing},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Bag of textons for image segmentation via soft clustering and convex shift},
year={2012},
volume={},
number={},
pages={781-788},
abstract={We propose an unsupervised image segmentation method based on texton similarity and mode seeking. The input image is first convolved with a filter-bank, followed by soft clustering on its filter response to generate textons. The input image is then superpixelized where each belonging pixel is regarded as a voter and a soft voting histogram is constructed for each superpixel by averaging its voters' posterior texton probabilities. We further propose a modified mode seeking method - called convex shift - to group superpixels and generate segments. The distribution of superpixel histograms is modeled nonparametrically in the histogram space, using Kullback-Leibler divergence (K-L divergence) and kernel density estimation. We show that each kernel shift step can be formulated as a convex optimization problem with linear constraints. Experiment on image segmentation shows that convex shift performs mode seeking effectively on an enforced histogram structure, grouping visually similar superpixels. With the incorporation of texton and soft voting, our method generates reasonably good segmentation results on natural images with relatively complex contents, showing significant superiority over traditional mode seeking based segmentation methods, while outperforming or being comparable to state of the art methods.},
keywords={Kernel;Histograms;Image segmentation;Feature extraction;Measurement;Complexity theory;Humans},
doi={10.1109/CVPR.2012.6247749},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247750,
author={Li, Zhenguo and Wu, Xiao-Ming and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Segmentation using superpixels: A bipartite graph partitioning approach},
year={2012},
volume={},
number={},
pages={789-796},
abstract={Grouping cues can affect the performance of segmentation greatly. In this paper, we show that superpixels (image segments) can provide powerful grouping cues to guide segmentation, where superpixels can be collected easily by (over)-segmenting the image using any reasonable existing segmentation algorithms. Generated by different algorithms with varying parameters, superpixels can capture diverse and multi-scale visual patterns of a natural image. Successful integration of the cues from a large multitude of superpixels presents a promising yet not fully explored direction. In this paper, we propose a novel segmentation framework based on bipartite graph partitioning, which is able to aggregate multi-layer superpixels in a principled and very effective manner. Computationally, it is tailored to unbalanced bipartite graph structure and leads to a highly efficient, linear-time spectral algorithm. Our method achieves significantly better performance on the Berkeley Segmentation Database compared to state-of-the-art techniques.},
keywords={Image segmentation;Bipartite graph;Synthetic aperture sonar;Partitioning algorithms;Vectors;Databases;Image color analysis},
doi={10.1109/CVPR.2012.6247750},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247751,
author={Gu, Jinwei and Liu, Chao},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminative illumination: Per-pixel classification of raw materials based on optimal projections of spectral BRDF},
year={2012},
volume={},
number={},
pages={797-804},
abstract={Classifying raw, unpainted materials - metal, plastic, ceramic, fabric, etc. - is an important yet challenging task for computer vision. Previous works measure subsets of surface spectral reflectance as features for classification. However, acquiring the full spectral reflectance is time-consuming and error-prone. In this paper, we propose to use coded illumination to directly measure discriminative features for material classification. Optimal illumination patterns - which we call “discriminative illumination” - are learned from training samples, after projecting to which, the spectral reflectance of different materials are maximally separated. This projection is automatically realized by the integration of incident light for surface reflection. While a single discriminative illumination is capable of linear, two-class classification, we show that multiple discriminative illuminations can be used for nonlinear and multi-class classification. We also show theoretically the proposed method has higher signal-to-noise ratio than previous methods due to light multiplexing. Finally, we construct a LED-based multi-spectral dome and use the discriminative illumination method for classifying a variety of raw materials, including metal (aluminum, alloy, steel, stainless steel, brass and copper), plastic, ceramic, fabric and wood. Experimental results demonstrate the effectiveness of the proposed method.},
keywords={Lighting;Support vector machines;Raw materials;Metals;Image color analysis;Light emitting diodes},
doi={10.1109/CVPR.2012.6247751},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247752,
author={Han, Shuai and Matsushita, Yasuyuki and Sato, Imari and Okabe, Takahiro and Sato, Yoichi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Camera spectral sensitivity estimation from a single image under unknown illumination by using fluorescence},
year={2012},
volume={},
number={},
pages={805-812},
abstract={Camera spectral sensitivity plays an important role for various color-based computer vision tasks. Although several methods have been proposed to estimate it, their applicability is severely restricted by the requirement for a known illumination spectrum. In this work, we present a single-image estimation method using fluorescence with no requirement for a known illumination spectrum. Under different illuminations, the spectral distributions of fluorescence emitted from the same material remain unchanged up to a certain scale. Thus, a camera's response to the fluorescence would have the same chromaticity. Making use of this chromaticity invariance, the camera spectral sensitivity can be estimated under an arbitrary illumination whose spectrum is unknown. Through extensive experiments, we proved that our method is accurate under different illuminations. Moreover, we show how to recover the spectra of daylight from the estimated results. Finally, we use the estimated camera spectral sensitivities and daylight spectra to solve color correction problems.},
keywords={Sensitivity;Cameras;Materials;Lighting;Image color analysis;Estimation;Fluorescence},
doi={10.1109/CVPR.2012.6247752},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247753,
author={Gupta, Mohit and Nayar, Shree K.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Micro Phase Shifting},
year={2012},
volume={},
number={},
pages={813-820},
abstract={We consider the problem of shape recovery for real world scenes, where a variety of global illumination (inter-reflections, subsurface scattering, etc.) and illumination defocus effects are present. These effects introduce systematic and often significant errors in the recovered shape. We introduce a structured light technique called Micro Phase Shifting, which overcomes these problems. The key idea is to project sinusoidal patterns with frequencies limited to a narrow, high-frequency band. These patterns produce a set of images over which global illumination and defocus effects remain constant for each point in the scene. This enables high quality reconstructions of scenes which have traditionally been considered hard, using only a small number of images. We also derive theoretical lower bounds on the number of input images needed for phase shifting and show that Micro PS achieves the bound.},
keywords={Lighting;Cameras;Frequency modulation;Shape;Scattering;Time frequency analysis;Systematics},
doi={10.1109/CVPR.2012.6247753},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247754,
author={Favaro, Paolo and Papadhimitri, Thoma},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A closed-form solution to uncalibrated photometric stereo via diffuse maxima},
year={2012},
volume={},
number={},
pages={821-828},
abstract={In this paper we propose a novel solution to uncalibrated photometric stereo. Our approach is to eliminate the so-called generalized bas relief (GBR) ambiguity by exploiting points where the Lambertian reflection is maximal. We demonstrate several noteworthy properties of these maxima: 1) Closed-form solution: A single diffuse maximum constrains the GBR ambiguity to a semi-circle in 3D space; 2) Efficiency: As few as two diffuse maxima in different images identify a unique solution; 3) GBR-invariance: The estimation error of the GBR parameters is completely independent of the true parameters. Furthermore, our algorithm is remarkably robust: It can obtain an accurate estimate of the GBR parameters even with extremely high levels of outliers in the detected maxima (up to 80% of the observations). The method is validated on real data and achieves state-of-the-art results.},
keywords={Lighting;Vectors;Noise;Uninterruptible power systems;Light sources;Closed-form solutions;Stereo vision},
doi={10.1109/CVPR.2012.6247754},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247755,
author={Ming, Yansheng and Li, Hongdong and He, Xuming},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Connected contours: A new contour completion model that respects the closure effect},
year={2012},
volume={},
number={},
pages={829-836},
abstract={Contour Completion plays an important role in visual perception, where the goal is to group fragmented low-level edge elements into perceptually coherent and salient contours. This process is often considered as guided by some middle-level Gestalt principles. Most existing methods for contour completion have focused on utilizing rather local Gestalt laws such as good-continuity and proximity. In contrast, much fewer methods have addressed the global contour closure effect, despite that many psychological evidences have shown the usefulness of closure in perceptual grouping. This paper proposes a novel higher-order CRF model to address the contour closure effect, through local connectedness approximation. This leads to a simplified problem structure, where the higher-order inference can be formulated as an integer linear program (ILP) and solved by an efficient cutting-plane variant. Tested on the BSDS benchmark, our method achieves a comparable precision-recall performance, a superior contour grouping ability (measured by Rand index), and more visually pleasing results, compared with existing methods.},
keywords={Image edge detection;Junctions;Humans;Lead;Mathematical model;Detectors;Complexity theory},
doi={10.1109/CVPR.2012.6247755},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247756,
author={Kim, Gunhee and Xing, Eric P.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On multiple foreground cosegmentation},
year={2012},
volume={},
number={},
pages={837-844},
abstract={In this paper, we address a challenging image segmentation problem called multiple foreground cosegmentation (MFC), which concerns a realistic scenario in general Webuser photo sets where a finite number of K foregrounds of interest repeatedly occur cross the entire photo set, but only an unknown subset of them is presented in each image. This contrasts the classical cosegmentation problem dealt with by most existing algorithms, which assume a much simpler but less realistic setting where the same set of foregrounds recurs in every image. We propose a novel optimization method for MFC, which makes no assumption on foreground configurations and does not suffer from the aforementioned limitation, while still leverages all the benefits of having co-occurring or (partially) recurring contents across images. Our method builds on an iterative scheme that alternates between a foreground modeling module and a region assignment module, both highly efficient and scalable. In particular, our approach is flexible enough to integrate any advanced region classifiers for foreground modeling, and our region assignment employs a combinatorial auction framework that enjoys several intuitively good properties such as optimality guarantee and linear complexity. We show the superior performance of our method in both segmentation quality and scalability in comparison with other state-of-the-art techniques on a newly introduced FlickrMFC dataset and the standard ImageNet dataset.},
keywords={Bismuth;Image segmentation;Silicon;Vegetation;Computational modeling;Heuristic algorithms;Optimization},
doi={10.1109/CVPR.2012.6247756},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247757,
author={Vezhnevets, Alexander and Ferrari, Vittorio and Buhmann, Joachim M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Weakly supervised structured output learning for semantic segmentation},
year={2012},
volume={},
number={},
pages={845-852},
abstract={We address the problem of weakly supervised semantic segmentation. The training images are labeled only by the classes they contain, not by their location in the image. On test images instead, the method must predict a class label for every pixel. Our goal is to enable segmentation algorithms to use multiple visual cues in this weakly supervised setting, analogous to what is achieved by fully supervised methods. However, it is difficult to assess the relative usefulness of different visual cues from weakly supervised training data. We define a parametric family of structured models, were each model weights visual cues in a different way. We propose a Maximum Expected Agreement model selection principle that evaluates the quality of a model from the family without looking at superpixel labels. Searching for the best model is a hard optimization problem, which has no analytic gradient and multiple local optima. We cast it as a Bayesian optimization problem and propose an algorithm based on Gaussian processes to efficiently solve it. Our second contribution is an Extremely Randomized Hashing Forest that represents diverse superpixel features as a sparse binary vector. It enables using appearance models of visual classes that are fast at training and testing and yet accurate. Experiments on the SIFT-flow dataset show a significant improvement over previous weakly supervised methods and even over some fully supervised methods.},
keywords={Training;Image segmentation;Visualization;Semantics;Optimization;Kernel;Measurement},
doi={10.1109/CVPR.2012.6247757},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247758,
author={Shen, Xiaohui and Wu, Ying},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A unified approach to salient object detection via low rank matrix recovery},
year={2012},
volume={},
number={},
pages={853-860},
abstract={Salient object detection is not a pure low-level, bottom-up process. Higher-level knowledge is important even for task-independent image saliency. We propose a unified model to incorporate traditional low-level features with higher-level guidance to detect salient objects. In our model, an image is represented as a low-rank matrix plus sparse noises in a certain feature space, where the non-salient regions (or background) can be explained by the low-rank matrix, and the salient regions are indicated by the sparse noises. To ensure the validity of this model, a linear transform for the feature space is introduced and needs to be learned. Given an image, its low-level saliency is then extracted by identifying those sparse noises when recovering the low-rank matrix. Furthermore, higher-level knowledge is fused to compose a prior map, and is treated as a prior term in the objective function to improve the performance. Extensive experiments show that our model can comfortably achieves comparable performance to the existing methods even without the help from high-level knowledge. The integration of top-down priors further improves the performance and achieves the state-of-the-art. Moreover, the proposed model can be considered as a prototype framework not only for general salient object detection, but also for potential task-dependent saliency applications.},
keywords={Feature extraction;Sparse matrices;Image color analysis;Noise;Image segmentation;Matrix decomposition;Vectors},
doi={10.1109/CVPR.2012.6247758},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247759,
author={Yang, Fei and Bourdev, Lubomir and Shechtman, Eli and Wang, Jue and Metaxas, Dimitris},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Facial expression editing in video using a temporally-smooth factorization},
year={2012},
volume={},
number={},
pages={861-868},
abstract={We address the problem of editing facial expression in video, such as exaggerating, attenuating or replacing the expression with a different one in some parts of the video. To achieve this we develop a tensor-based 3D face geometry reconstruction method, which fits a 3D model for each video frame, with the constraint that all models have the same identity and requiring temporal continuity of pose and expression. With the identity constraint, the differences between the underlying 3D shapes capture only changes in expression and pose. We show that various expression editing tasks in video can be achieved by combining face reordering with face warping, where the warp is induced by projecting differences in 3D face shapes into the image plane. Analogously, we show how the identity can be manipulated while fixing expression and pose. Experimental results show that our method can effectively edit expressions and identity in video in a temporally-coherent way with high fidelity.},
keywords={Face;Shape;Vectors;Solid modeling;Tensile stress;Geometry;Active appearance model},
doi={10.1109/CVPR.2012.6247759},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247760,
author={Chen, Qifeng and Li, Dingzeyu and Tang, Chi-Keung},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={KNN matting},
year={2012},
volume={},
number={},
pages={869-876},
abstract={We are interested in a general alpha matting approach for the simultaneous extraction of multiple image layers; each layer may have disjoint segments for material matting not limited to foreground mattes typical of natural image matting. The estimated alphas also satisfy the summation constraint. Our approach does not assume the local color-line model, does not need sophisticated sampling strategies, and generalizes well to any color or feature space in any dimensions. Our matting technique, aptly called KNN matting, capitalizes on the nonlocal principle by using K nearest neighbors (KNN) in matching nonlocal neighborhoods, and contributes a simple and fast algorithm giving competitive results with sparse user markups. KNN matting has a closed-form solution that can leverage on the preconditioned conjugate gradient method to produce an efficient implementation. Experimental evaluation on benchmark datasets indicates that our matting results are comparable to or of higher quality than state of the art methods.},
keywords={Kernel;Materials;Laplace equations;Vectors;Image color analysis;Image segmentation;Closed-form solutions},
doi={10.1109/CVPR.2012.6247760},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247761,
author={Lee, Hyunjoon and Shechtman, Eli and Wang, Jue and Lee, Seungyong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automatic upright adjustment of photographs},
year={2012},
volume={},
number={},
pages={877-884},
abstract={Man-made structures often appear to be distorted in photos captured by casual photographers, as the scene layout often conflicts with how it is expected by human perception. In this paper we propose an automatic approach for straightening up slanted man-made structures in an input image to improve its perceptual quality. We call this type of correction upright adjustment. We propose a set of criteria for upright adjustment based on human perception studies, and develop an optimization framework which yields an optimal homography for adjustment. We also develop a new optimization-based camera calibration method that performs favorably to previous methods and allows the proposed system to work reliably for a wide variety of images. The effectiveness of our system is demonstrated by both quantitative comparisons and qualitative user studies.},
keywords={Cameras;Calibration;Nonlinear distortion;Distortion measurement;Humans;Buildings;Robustness},
doi={10.1109/CVPR.2012.6247761},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247762,
author={Luo, Wei and Lu, Zheng and Wang, Xiaogang and Xu, Ying-Qing and Ben-Ezra, Moshe and Tang, Xiaoou and Brown, Michael S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Synthesizing oil painting surface geometry from a single photograph},
year={2012},
volume={},
number={},
pages={885-892},
abstract={We present an approach to synthesize the subtle 3D relief and texture of oil painting brush strokes from a single photograph. This task is unique from traditional synthesize algorithms due to its mixed modality between the input and output; i.e., our goal is to synthesize surface normals given an intensity image input. To accomplish this task, we propose a framework that first applies intrinsic image decomposition to produce a pair of initial normal maps. These maps are combined into a conditional random field (CRF) optimization framework that incorporates additional information derived from a training set consisting of normals captured using photometric stereo on oil paintings with similar brush styles. Additional constraints are incorporated into the CRF framework to further ensures smoothness and preserve brush stroke edges. Our results show that this approach can produce compelling reliefs that are often indistinguishable from results captured using photometric stereo.},
keywords={Painting;Lighting;Training data;Surface reconstruction;Training;Image color analysis},
doi={10.1109/CVPR.2012.6247762},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247763,
author={Jiménez, David and Pizarro, Daniel and Mazo, Manuel and Palazuelos, Sira},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Modelling and correction of multipath interference in time of flight cameras},
year={2012},
volume={},
number={},
pages={893-900},
abstract={This paper presents an algorithm that automatically corrects the distortion caused by multipath interference (MpI) in depth measurements obtained with time of flight cameras (ToF cameras). A radiometric model that explains, under some mild simplifications, the working principle of a ToF camera including a model for MpI is proposed. Using this model we demonstrate that all the information needed for compensating the influence of MpI on the scene captured by the camera is self-contained in the measurements (depth and amplitude of infrared signal). We propose an iterative optimization method that, based on the measurements contaminated with MpI, gives depth correction for each pixel. Results are shown in artificially generated time of flight scenes using the radiometric model. In addition, the system has been validated in real scenes using a commercial ToF camera providing good results.},
keywords={Cameras;Pollution measurement;Radiometry;Interference;Phase measurement;Distortion measurement;Iterative methods;Time of Flight (ToF);Multipath Interference (MpI);Iterative Method},
doi={10.1109/CVPR.2012.6247763},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247764,
author={Yu, Zhan and Yu, Jingyi and Lumsdaine, Andrew and Georgiev, Todor},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={An analysis of color demosaicing in plenoptic cameras},
year={2012},
volume={},
number={},
pages={901-908},
abstract={A plenoptic camera captures the 4D radiance about a scene. Recent practical solutions mount a microlens array on top of a commodity SLR to directly acquire these rays. However, they suffer from low resolution as hundreds of thousands of views need to be captured in a single shot. In this paper, we develop a simple but effective technique for improving the image resolution of the plenoptic camera by maneuvering the demosaicing process. We first show that the traditional solution by demosaicing each individual microlens image and then blending them for view synthesis is suboptimal. In particular, this demosaicing process often suffers from aliasing artifacts, and it damages high frequency information recorded by each microlens image hence degrades the image quality. We instead propose to de-mosaic the synthesized view at the rendering stage. Specifically, we first transform the radiance to the desired focal plane and then apply frequency domain plenoptic resampling. A full resolution color filtered image is then created by performing a 2D integral projection from the reparam-eterized radiance. Finally, we conduct demosacing to obtain the color result. We show that our solution can achieve visible resolution enhancement on dynamic refocusing and depth-assisted deep focus rendering.},
keywords={Lenses;Microoptics;Image resolution;Image color analysis;Cameras;Arrays;Rendering (computer graphics)},
doi={10.1109/CVPR.2012.6247764},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247765,
author={Wang, Hongzhi and Yushkevich, Paul A.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Spatial bias in multi-atlas based segmentation},
year={2012},
volume={},
number={},
pages={909-916},
abstract={Multi-atlas segmentation has been widely applied in medical image analysis. With deformable registration, this technique realizes label transfer from pre-labeled atlases to unknown images. When deformable registration produces error, label fusion that combines results produced by multiple atlases is an effective way for reducing segmentation errors. Among the existing label fusion strategies, similarity-weighted voting strategies with spatially varying weight distributions have been particularly successful. We show that, weighted voting based label fusion produces a spatial bias that under-segments structures with convex shapes. The bias can be approximated as applying spatial convolution to the ground truth spatial label probability maps, where the convolution kernel combines the distribution of residual registration errors and the function producing similarity-based voting weights. To reduce this bias, we apply a standard spatial deconvolution to the spatial probability maps obtained from weighted voting. In a brain image segmentation experiment, we demonstrate the spatial bias and show that our technique substantially reduces this spatial bias.},
keywords={Image segmentation;Kernel;Convolution;Deconvolution;Hippocampus;Accuracy;Magnetic resonance imaging},
doi={10.1109/CVPR.2012.6247765},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247766,
author={Liu, Kun and Wang, Qing and Driever, Wolfgang and Ronneberger, Olaf},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={2D/3D rotation-invariant detection using equivariant filters and kernel weighted mapping},
year={2012},
volume={},
number={},
pages={917-924},
abstract={In many vision problems, rotation-invariant analysis is necessary or preferred. Popular solutions are mainly based on pose normalization or brute-force learning, neglecting the intrinsic properties of rotations. In this paper, we present a rotation invariant detection approach built on the equivariant filter framework, with a new model for learning the filtering behavior. The special properties of the harmonic basis, which is related to the irreducible representation of the rotation group, directly guarantees rotation invariance of the whole approach. The proposed kernel weighted mapping ensures high learning capability while respecting the invariance constraint. We demonstrate its performance on 2D object detection with in-plane rotations, and a 3D application on rotation-invariant landmark detection in microscopic volumetric data.},
keywords={Kernel;Harmonic analysis;Computational modeling;Training;Vectors;Feature extraction;Estimation},
doi={10.1109/CVPR.2012.6247766},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247767,
author={Zhang, Yu and Wu, Guorong and Yap, Pew-Thian and Feng, Qianjin and Lian, Jun and Chen, Wufan and Shen, Dinggang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Reconstruction of super-resolution lung 4D-CT using patch-based sparse representation},
year={2012},
volume={},
number={},
pages={925-931},
abstract={4D-CT plays an important role in lung cancer treatment. However, due to the inherent high-dose exposure associated with CT, dense sampling along superior-inferior direction is often not practical. As a result, artifacts such as lung vessel discontinuity and partial volume are typical in 4D-CT images and might mislead dose administration in radiation therapy. In this paper, we present a novel patch-based technique for super-resolution enhancement of the 4D-CT images along the superior-inferior direction. Our working premise is that the anatomical information that is missing at one particular phase can be recovered from other phases. Based on this assumption, we employ a patch-based mechanism for guided reconstruction of super-resolution axial slices. Specifically, to reconstruct each targeted super-resolution slice for a CT image at a particular phase, we agglomerate a dictionary of patches from images of all other phases in the 4D-CT sequence. Then we perform a sparse combination of the patches in this dictionary to reconstruct details of a super-resolution patch, under constraint of similarity to the corresponding patches in the neighboring slices. By iterating this procedure over all possible patch locations, a superresolution 4D-CT image sequence with enhanced anatomical details can be eventually reconstructed. Our method was extensively evaluated using a public dataset. In all experiments, our method outperforms the conventional linear and cubic-spline interpolation methods in terms of preserving image details and suppressing misleading artifacts.},
keywords={Image resolution;Dictionaries;Image reconstruction;Interpolation;PSNR;Lungs;Biomedical applications of radiation},
doi={10.1109/CVPR.2012.6247767},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247768,
author={Ni, Jie and Singh, Maneesh K. and Bahlmann, Claus},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast radial symmetry detection under affine transformations},
year={2012},
volume={},
number={},
pages={932-939},
abstract={The fast radial symmetry (FRS) transform has been very popular for detecting interest points based on local radial symmetry1. Although FRS delivers good performance at a relatively low computational cost and is very well suited for a variety of real-time computer vision applications, it is not invariant to perspective distortions. Moreover, even perfectly (radially) symmetric visual patterns in the real world are perceived by us after a perspective projection. In this paper, we propose a systematic extension to the FRS transform to make it invariant to (bounded) cases of perspective projection - we call this transform the generalized FRS or GFRS transform. We show that GFRS inherits the basic characteristics of FRS and retains its computational efficiency. We demonstrate the wide applicability of GFRS by applying it to a variety of natural images to detect radially symmetric patterns that have undergone significant perspective distortions. Subsequently, we build a nucleus detector based on the GFRS transform and apply it to the important problem of digital histopathology. We demonstrate superior performance over state-of-the-art nuclei detection algorithms, validated using ROC curves.},
keywords={Transforms;Detectors;Vectors;Visualization;Shape;Humans;Wheels},
doi={10.1109/CVPR.2012.6247768},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247769,
author={Wan, Jing and Zhang, Zhilin and Yan, Jingwen and Li, Taiyong and Rao, Bhaskar D. and Fang, Shiaofen and Kim, Sungeun and Risacher, Shannon L. and Saykin, Andrew J. and Shen, Li},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sparse Bayesian multi-task learning for predicting cognitive outcomes from neuroimaging measures in Alzheimer's disease},
year={2012},
volume={},
number={},
pages={940-947},
abstract={Alzheimer's disease (AD) is the most common form of dementia that causes progressive impairment of memory and other cognitive functions. Multivariate regression models have been studied in AD for revealing relationships between neuroimaging measures and cognitive scores to understand how structural changes in brain can influence cognitive status. Existing regression methods, however, do not explicitly model dependence relation among multiple scores derived from a single cognitive test. It has been found that such dependence can deteriorate the performance of these methods. To overcome this limitation, we propose an efficient sparse Bayesian multi-task learning algorithm, which adaptively learns and exploits the dependence to achieve improved prediction performance. The proposed algorithm is applied to a real world neuroimaging study in AD to predict cognitive performance using MRI scans. The effectiveness of the proposed algorithm is demonstrated by its superior prediction performance over multiple state-of-the-art competing methods and accurate identification of compact sets of cognition-relevant imaging biomarkers that are consistent with prior knowledge.},
keywords={Correlation;Algorithm design and analysis;Neuroimaging;Prediction algorithms;Kernel;Bayesian methods;Magnetic resonance imaging},
doi={10.1109/CVPR.2012.6247769},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247770,
author={Carneiro, Gustavo and Nascimento, Jacinto C.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The use of on-line co-training to reduce the training set size in pattern recognition methods: Application to left ventricle segmentation in ultrasound},
year={2012},
volume={},
number={},
pages={948-955},
abstract={The use of statistical pattern recognition models to segment the left ventricle of the heart in ultrasound images has gained substantial attention over the last few years. The main obstacle for the wider exploration of this methodology lies in the need for large annotated training sets, which are used for the estimation of the statistical model parameters. In this paper, we present a new on-line co-training methodologythat reduces the need for large training sets for such parameter estimation. Our approach learns the initial parameters of two different models using a small manually annotated training set. Then, given each frame of a test sequence, the methodology not only produces the segmentation of the current frame, but it also uses the results of both classifiers to retrain each other incrementally. This on-line aspect of our approach has the advantages of producing segmentation results and retraining the classifiers on the fly as frames of a test sequence are presented, but it introduces a harder learning setting compared to the usual off-line co-training, where the algorithm has access to the whole set of un-annotated training samples from the beginning. Moreover, we introduce the use of the following new types of classifiers in the co-training framework: deep belief network and multiple model probabilistic data association. We show that our method leads to a fully automatic left ventricle segmentation system that achieves state-of-the-art accuracy on a public database with training sets containing at least twenty annotated images.},
keywords={Training;Image segmentation;Pattern recognition;Probabilistic logic;Ultrasonic imaging;Data models;Vectors},
doi={10.1109/CVPR.2012.6247770},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247771,
author={Yushkevich, Paul A. and Wang, Hongzhi and Pluta, John and Avants, Brian B.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={From label fusion to correspondence fusion: A new approach to unbiased groupwise registration},
year={2012},
volume={},
number={},
pages={956-963},
abstract={Label fusion strategies are used in multi-atlas image segmentation approaches to compute a consensus segmentation of an image, given a set of candidate segmentations produced by registering the image to a set of atlases [19, 11, 8]. Effective label fusion strategies, such as local similarity-weighted voting [1, 13] substantially reduce segmentation errors compared to single-atlas segmentation. This paper extends the label fusion idea to the problem of finding correspondences across a set of images. Instead of computing a consensus segmentation, weighted voting is used to estimate a consensus coordinate map between a target image and a reference space. Two variants of the problem are considered: (1) where correspondences between a set of atlases are known and are propagated to the target image; (2) where correspondences are estimated across a set of images without prior knowledge. Evaluation in synthetic data shows that correspondences recovered by fusion methods are more accurate than those based on registration to a population template. In a 2D example in real MRI data, fusion methods result in more consistent mappings between manual segmentations of the hippocampus.},
keywords={Image segmentation;Standards;Measurement;Image resolution;Magnetic resonance imaging;Image registration;Kernel},
doi={10.1109/CVPR.2012.6247771},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247772,
author={Xu, Yan and Zhu, Jun-Yan and Chang, Eric and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multiple clustered instance learning for histopathology cancer image classification, segmentation and clustering},
year={2012},
volume={},
number={},
pages={964-971},
abstract={Cancer tissues in histopathology images exhibit abnormal patterns; it is of great clinical importance to label a histopathology image as having cancerous regions or not and perform the corresponding image segmentation. However, the detailed annotation of cancer cells is often an ambiguous and challenging task. In this paper, we propose a new learning method, multiple clustered instance learning (MCIL), to classify, segment and cluster cancer cells in colon histopathology images. The proposed MCIL method simultaneously performs image-level classification (cancer vs. non-cancer image), pixel-level segmentation (cancer vs. non-cancer tissue), and patch-level clustering (cancer subclasses). We embed the clustering concept into the multiple instance learning (MIL) setting and derive a principled solution to perform the above three tasks in an integrated framework. Experimental results demonstrate the efficiency and effectiveness of MCIL in analyzing colon cancers.},
keywords={Cancer;Image segmentation;Biomedical imaging;Colon;Boosting;Standards;Clustering algorithms},
doi={10.1109/CVPR.2012.6247772},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247773,
author={Zhou, Xiaowei and Yang, Can and Yu, Weichuan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automatic mitral leaflet tracking in echocardiography by outlier detection in the low-rank representation},
year={2012},
volume={},
number={},
pages={972-979},
abstract={Tracking the mitral valve leaflet in an ultrasound sequence is a challenging task because of the poor image quality and fast and irregular leaflet motion. Previous algorithms usually applied standard segmentation methods based on edges, object intensity and anatomical information to segment the mitral leaflet in static frames. However, they are limited in practical applications due to the requirement of manual input for initialization or large annotated datasets for training. In this paper we present a completely automatic and unsupervised algorithm for mitral leaflet detection and tracking. We demonstrate that the image sequence of a cardiac cycle can be well approximated with a low-rank matrix, except for the mitral leaflet region with fast motion and tissue deformation. Based on this difference, we propose to track the mitral leaflet by detecting contiguous outliers in the low-rank representation. With this formulation, the leaflet is tracked using the motion cue, but the complicated motion computation is avoided. To the best of our knowledge, the proposed algorithm is the first unsupervised method for mitral leaflet tracking. The algorithm was tested on both 2D and 3D echocardiography, which achieved accurate segmentation with an average distance of 0.87 ± 0.42mm compared to the manual tracing.},
keywords={Valves;Tracking;Ultrasonic imaging;Image segmentation;Motion segmentation;Myocardium;Image edge detection},
doi={10.1109/CVPR.2012.6247773},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247774,
author={Wu, Dijia and Liu, David and Puskas, Zoltan and Lu, Chao and Wimmer, Andreas and Tietjen, Christian and Soza, Grzegorz and Zhou, S. Kevin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A learning based deformable template matching method for automatic rib centerline extraction and labeling in CT images},
year={2012},
volume={},
number={},
pages={980-987},
abstract={The automatic extraction and labeling of the rib centerlines is a useful yet challenging task in many clinical applications. In this paper, we propose a new approach integrating rib seed point detection and template matching to detect and identify each rib in chest CT scans. The bottom-up learning based detection exploits local image cues and top-down deformable template matching imposes global shape constraints. To adapt to the shape deformation of different rib cages whereas maintain high computational efficiency, we employ a Markov Random Field (MRF) based articulated rigid transformation method followed by Active Contour Model (ACM) deformation. Compared with traditional methods that each rib is individually detected, traced and labeled, the new approach is not only much more robust due to prior shape constraints of the whole rib cage, but removes tedious post-processing such as rib pairing and ordering steps because each rib is automatically labeled during the template matching. For experimental validation, we create an annotated database of 112 challenging volumes with ribs of various sizes, shapes, and pathologies such as metastases and fractures. The proposed approach shows orders of magnitude higher detection and labeling accuracy than state-of-the-art solutions and runs about 40 seconds for a complete rib cage on the average.},
keywords={Ribs;Labeling;Robustness;Computed tomography;Shape;Pathology;Feature extraction},
doi={10.1109/CVPR.2012.6247774},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247775,
author={Parisot, Sarah and Duffau, Hugues and Chemouny, Stéphane and Paragios, Nikos},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Graph-based detection, segmentation amp; characterization of brain tumors},
year={2012},
volume={},
number={},
pages={988-995},
abstract={In this paper we propose a novel approach for detection, segmentation and characterization of brain tumors. Our method exploits prior knowledge in the form of a sparse graph representing the expected spatial positions of tumor classes. Such information is coupled with image-based classification techniques along with spatial smoothness constraints towards producing a reliable detection map within a unified graphical model formulation. Towards optimal use of prior knowledge, a two layer interconnected graph is considered with one layer corresponding to the low-grade glioma type (characterization) and the second layer to voxel-based decisions of tumor presence. Efficient linear programming both in terms of performance as well as in terms of computational load is considered to recover the lowest potential of the objective function. The outcome of the method refers to both tumor segmentation as well as their characterization. Promising results on substantial data sets demonstrate the extreme potentials of our method.},
keywords={Tumors;Image segmentation;Indexes;Brain models;Magnetic resonance imaging},
doi={10.1109/CVPR.2012.6247775},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247776,
author={Serradell, Eduard and Glowacki, Przemyslaw and Kybic, Jan and Moreno-Noguer, Francesc and Fua, Pascal},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust non-rigid registration of 2D and 3D graphs},
year={2012},
volume={},
number={},
pages={996-1003},
abstract={We present a new approach to matching graphs embedded in ℝ2 or ℝ3. Unlike earlier methods, our approach does not rely on the similarity of local appearance features, does not require an initial alignment, can handle partial matches, and can cope with non-linear deformations and topological differences. To handle arbitrary non-linear deformations, we represent them as Gaussian Processes. In the absence of appearance information, we iteratively establish correspondences between graph nodes, update the structure accordingly, and use the current mapping estimate to find the most likely correspondences that will be used in the next iteration. This makes the computation tractable. We demonstrate the effectiveness of our approach first on synthetic cases and then on angiography data, retinal fundus images, and microscopy image stacks acquired at very different resolutions.},
keywords={Image resolution;Microscopy;Biomedical imaging;Joining processes;Robustness;Gaussian processes;Retina},
doi={10.1109/CVPR.2012.6247776},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247777,
author={Funke, Jan and Andres, Bjoern and Hamprecht, Fred A. and Cardona, Albert and Cook, Matthew},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient automatic 3D-reconstruction of branching neurons from EM data},
year={2012},
volume={},
number={},
pages={1004-1011},
abstract={We present an approach for the automatic reconstruction of neurons from 3D stacks of electron microscopy sections. The core of our system is a set of possible assignments, each of which proposes with some cost a link between neuron regions in consecutive sections. These can model the continuation, branching, and end of neurons. The costs are trainable on positive assignment samples. An optimal and consistent set of assignments is found for the whole volume at once by solving an integer linear program. This set of assignments determines both the segmentation into neuron regions and the correspondence between such regions in neighboring slices. For each picked assignment, a confidence value helps to prioritize decisions to be reviewed by a human expert. We evaluate the performance of our method on an annotated volume of neural tissue and compare to the current state of the art [26]. Our method is superior in accuracy and can be trained using a small number of samples. The observed inference times are linear with about 2 milliseconds per neuron and section.},
keywords={Neurons;Image segmentation;Training;Image reconstruction;Vegetation;Visualization;Vectors},
doi={10.1109/CVPR.2012.6247777},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247778,
author={Lou, Xinghua and Koethe, Ullrich and Wittbrodt, Jochen and Hamprecht, Fred A.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning to segment dense cell nuclei with shape prior},
year={2012},
volume={},
number={},
pages={1012-1018},
abstract={We study the problem of segmenting multiple cell nuclei from GFP or Hoechst stained microscope images with a shape prior. This problem is encountered ubiquitously in cell biology and developmental biology. Our work is motivated by the observation that segmentations with loose boundary or shrinking bias not only jeopardize feature extraction for downstream tasks (e.g. cell tracking), but also prevent robust statistical analysis (e.g. modeling of fluorescence distribution). We therefore propose a novel extension to the graph cut framework that incorporates a “blob”-like shape prior. The corresponding energy terms are parameterized via structured learning. Extensive evaluation and comparison on 2D/3D datasets show substantial quantitative improvement over other state-of-the-art methods. For example, our method achieves an 8.2% Rand index increase and a 4.3 Hausdorff distance decrease over the second best method on a public hand-labeled 2D benchmark.},
keywords={Shape;Image segmentation;Vectors;Biology;Imaging;Labeling;Indexes},
doi={10.1109/CVPR.2012.6247778},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247779,
author={Krajsek, Kai and Scharr, Hanno},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A Riemannian approach for estimating orientation distribution function (ODF) images from high-angular resolution diffusion imaging (HARDI)},
year={2012},
volume={},
number={},
pages={1019-1026},
abstract={High-angular resolution diffusion imaging (HARDI) is a magnetic resonance technique estimating the direction of self-diffusion of water molecules in biological tissue. HARDI encodes at each pixel (voxel) the orientation distribution function (ODF) of water diffusion molecules, i.e. the probability distribution function of finding a water molecule which moved in a certain direction during the observation time. As a consequence ODF images differ from usual gray scale images with respect to their underlying geometry as well as with respect to their error distribution. We present a Bayesian estimator for ODF images considering these differences. To this end, we derive a likelihood function based on the Rician distribution of the NMR signals and propose prior distributions considering ODFs as Riemanian manifolds. Utilizing properties of spherical harmonics and the square root representation of ODFs allows us to effectively reconstruct and regularize ODF images in one step within this Riemannian framework. Experiments demonstrate the merits of our approach on synthetic as well as on real data.},
keywords={Nuclear magnetic resonance;Harmonic analysis;Manifolds;Measurement;Noise;Vectors;Mathematical model},
doi={10.1109/CVPR.2012.6247779},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247780,
author={Muralidharan, Prasanna and Fletcher, P. Thomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sasaki metrics for analysis of longitudinal data on manifolds},
year={2012},
volume={},
number={},
pages={1027-1034},
abstract={Longitudinal data arises in many applications in which the goal is to understand changes in individual entities over time. In this paper, we present a method for analyzing longitudinal data that take values in a Riemannian manifold. A driving application is to characterize anatomical shape changes and to distinguish between trends in anatomy that are healthy versus those that are due to disease. We present a generative hierarchical model in which each individual is modeled by a geodesic trend, which in turn is considered as a perturbation of the mean geodesic trend for the population. Each geodesic in the model can be uniquely parameterized by a starting point and velocity, i.e., a point in the tangent bundle. Comparison between these parameters is achieved through the Sasaki metric, which provides a natural distance metric on the tangent bundle. We develop a statistical hypothesis test for differences between two groups of longitudinal data by generalizing the Hotelling T2 statistic to manifolds. We demonstrate the ability of these methods to distinguish differences in shape changes in a comparison of longitudinal corpus callosum data in subjects with dementia versus healthily aging controls.},
keywords={Manifolds;Vectors;Measurement;Data models;Shape;Tensile stress;Equations},
doi={10.1109/CVPR.2012.6247780},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247781,
author={Ruland, Thomas and Pajdla, Tomas and Krüger, Lars},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Globally optimal hand-eye calibration},
year={2012},
volume={},
number={},
pages={1035-1042},
abstract={This paper introduces simultaneous globally optimal hand-eye self-calibration in both its rotational and translational components. The main contributions are new feasibility tests to integrate the hand-eye calibration problem into a branch-and-bound parameter space search. The presented method constitutes the first guaranteed globally optimal estimator for simultaneous optimization of both components with respect to a cost function based on reprojection errors. The system is evaluated in both synthetic and real world scenarios. The employed benchmark dataset is published online1 to create a common point of reference for evaluation of hand-eye self-calibration algorithms.},
keywords={Calibration;Cameras;Vectors;Upper bound;Robot vision systems;Robot kinematics},
doi={10.1109/CVPR.2012.6247781},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247782,
author={Lim, Hyon and Sinha, Sudipta N. and Cohen, Michael F. and Uyttendaele, Matthew},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Real-time image-based 6-DOF localization in large-scale environments},
year={2012},
volume={},
number={},
pages={1043-1050},
abstract={We present a real-time approach for image-based localization within large scenes that have been reconstructed offline using structure from motion (Sfm). From monocular video, our method continuously computes a precise 6-DOF camera pose, by efficiently tracking natural features and matching them to 3D points in the Sfm point cloud. Our main contribution lies in efficiently interleaving a fast keypoint tracker that uses inexpensive binary feature descriptors with a new approach for direct 2D-to-3D matching. The 2D-to-3D matching avoids the need for online extraction of scale-invariant features. Instead, offline we construct an indexed database containing multiple DAISY descriptors per 3D point extracted at multiple scales. The key to the efficiency of our method lies in invoking DAISY descriptor extraction and matching sparingly during localization, and in distributing this computation over a window of successive frames. This enables the algorithm to run in real-time, without fluctuations in the latency over long durations. We evaluate the method in large indoor and outdoor scenes. Our algorithm runs at over 30 Hz on a laptop and at 12 Hz on a low-power, mobile computer suitable for onboard computation on a quadrotor micro aerial vehicle.},
keywords={Cameras;Real time systems;Feature extraction;Databases;Image reconstruction;Streaming media;Simultaneous localization and mapping},
doi={10.1109/CVPR.2012.6247782},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247783,
author={Takahashi, Kosuke and Nobuhara, Shohei and Matsuyama, Takashi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A new mirror-based extrinsic camera calibration using an orthogonality constraint},
year={2012},
volume={},
number={},
pages={1051-1058},
abstract={This paper is aimed at calibrating the relative posture and position, i.e. extrinsic parameters, of a stationary camera against a 3D reference object which is not directly visible from the camera. We capture the reference object via a mirror under three different unknown poses, and then calibrate the extrinsic parameters from 2D appearances of reflections of the reference object in the mirrors. The key contribution of this paper is to present a new algorithm which returns a unique solution of three P3P problems from three mirrored images. While each P3P problem has up to four solutions and therefore a set of three P3P problems has up to 64 solutions, our method can select a solution based on an orthogonality constraint which should be satisfied by all families of reflections of a single reference object. In addition we propose a new scheme to compute the extrinsic parameters by solving a large system of linear equations. These two points enable us to provide a unique and robust solution. We demonstrate the advantages of the proposed method against a state-of-the-art by qualitative and quantitative evaluations using synthesized and real data.},
keywords={Mirrors;Cameras;Vectors;Calibration;Eigenvalues and eigenfunctions;Equations;Mathematical model},
doi={10.1109/CVPR.2012.6247783},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247784,
author={Hansen, Peter and Alismail, Hatem and Rander, Peter and Browning, Brett},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Online continuous stereo extrinsic parameter estimation},
year={2012},
volume={},
number={},
pages={1059-1066},
abstract={Stereo visual odometry and dense scene reconstruction depend critically on accurate calibration of the extrinsic (relative) stereo camera poses. We present an algorithm for continuous, online stereo extrinsic re-calibration operating only on sparse stereo correspondences on a per-frame basis. We obtain the 5 degree of freedom extrinsic pose for each frame, with a fixed baseline, making it possible to model time-dependent variations. The initial extrinsic estimates are found by minimizing epipolar errors, and are refined via a Kalman Filter (KF). Observation covariances are derived from the Cramer-Rao lower bound of the solution uncertainty. The algorithm operates at frame rate with unoptimized Matlab code with over 1000 correspondences per frame. We validate its performance using a variety of real stereo datasets and simulations.},
keywords={Cameras;Calibration;Visualization;Noise;Kalman filters;Image reconstruction;Mathematical model},
doi={10.1109/CVPR.2012.6247784},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247785,
author={Lu, Yao and Zhang, Wei and Jin, Cheng and Xue, Xiangyang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning attention map from images},
year={2012},
volume={},
number={},
pages={1067-1074},
abstract={While bottom-up and top-down processes have shown effectiveness during predicting attention and eye fixation maps on images, in this paper, inspired by the perceptual organization mechanism before attention selection, we propose to utilize figure-ground maps for the purpose. So as to take both pixel-wise and region-wise interactions into consideration when predicting label probabilities for each pixel, we develop a context-aware model based on multiple segmentation to obtain final results. The MIT attention dataset [14] is applied finally to evaluate both new features and model. Quantitative experiments demonstrate that figure-ground cues are valid in predicting attention selection, and our proposed model produces improvements over baseline method.},
keywords={Image segmentation;Context;Visualization;Context modeling;Predictive models;Training;Humans},
doi={10.1109/CVPR.2012.6247785},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247786,
author={Chang, Che-Han and Chuang, Yung-Yu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A line-structure-preserving approach to image resizing},
year={2012},
volume={},
number={},
pages={1075-1082},
abstract={This paper proposes a content-aware image resizing method which simultaneously preserves both salient image features and important line structure properties: parallelism, collinearity and orientation. When there are prominent line structures in the image, image resizing methods without explicitly taking these properties into account could produce line structure distortions in their results. Since the human visual system is very sensitive to line structures, such distortions often become noticeable and disturbing. Our method couples mesh deformations for image resizing with similarity transforms for line features. Mesh deformations are used to control content preservation while similarity transforms are analyzed in the Hough space to maintain line structure properties. Our method strikes a good balance between preserving content and maintaining line structure properties. Experiments show the proposed method often outperforms methods without taking line structures into account, especially for scenes with prominent line structures.},
keywords={Transforms;Image segmentation;Couplings;Vectors;Parallel processing;Optimization;Shape},
doi={10.1109/CVPR.2012.6247786},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247787,
author={Yao, Cong and Bai, Xiang and Liu, Wenyu and Ma, Yi and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Detecting texts of arbitrary orientations in natural images},
year={2012},
volume={},
number={},
pages={1083-1090},
abstract={With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes.},
keywords={Shape;Image edge detection;Algorithm design and analysis;Clutter;Joining processes;Histograms;Robustness},
doi={10.1109/CVPR.2012.6247787},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247788,
author={Zhou, Hao and Kuang, Zhanghui and Wong, Kwan-Yee K.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Markov Weight Fields for face sketch synthesis},
year={2012},
volume={},
number={},
pages={1091-1097},
abstract={Great progress has been made in face sketch synthesis in recent years. State-of-the-art methods commonly apply a Markov Random Fields (MRF) model to select local sketch patches from a set of training data. Such methods, however, have two major drawbacks. Firstly, the MRF model used cannot synthesize new sketch patches. Secondly, the optimization problem in solving the MRF is NP-hard. In this paper, we propose a novel Markov Weight Fields (MWF) model that is capable of synthesizing new sketch patches. We formulate our model into a convex quadratic programming (QP) problem to which the optimal solution is guaranteed. Based on the Markov property of our model, we further propose a cascade decomposition method (CDM) for solving such a large scale QP problem efficiently. Experimental results on the CUHK face sketch database and celebrity photos show that our model outperforms the common MRF model used in other state-of-the-art methods.},
keywords={Face;Training data;Optimization;Markov processes;Computational modeling;Vectors;Databases},
doi={10.1109/CVPR.2012.6247788},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247789,
author={Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised feature learning framework for no-reference image quality assessment},
year={2012},
volume={},
number={},
pages={1098-1105},
abstract={In this paper, we present an efficient general-purpose objective no-reference (NR) image quality assessment (IQA) framework based on unsupervised feature learning. The goal is to build a computational model to automatically predict human perceived image quality without a reference image and without knowing the distortion present in the image. Previous approaches for this problem typically rely on hand-crafted features which are carefully designed based on prior knowledge. In contrast, we use raw-image-patches extracted from a set of unlabeled images to learn a dictionary in an unsupervised manner. We use soft-assignment coding with max pooling to obtain effective image representations for quality estimation. The proposed algorithm is very computationally appealing, using raw image patches as local descriptors and using soft-assignment for encoding. Furthermore, unlike previous methods, our unsupervised feature learning strategy enables our method to adapt to different domains. CORNIA (Codebook Representation for No-Reference Image Assessment) is tested on LIVE database and shown to perform statistically better than the full-reference quality measure, structural similarity index (SSIM) and is shown to be comparable to state-of-the-art general purpose NR-IQA algorithms.},
keywords={Databases;Encoding;Image quality;Feature extraction;Image coding;Noise;Transform coding},
doi={10.1109/CVPR.2012.6247789},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247790,
author={Wang, Jing and Wang, Jingdong and Zeng, Gang and Tu, Zhuowen and Gan, Rui and Li, Shipeng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Scalable k-NN graph construction for visual descriptors},
year={2012},
volume={},
number={},
pages={1106-1113},
abstract={The k-NN graph has played a central role in increasingly popular data-driven techniques for various learning and vision tasks; yet, finding an efficient and effective way to construct k-NN graphs remains a challenge, especially for large-scale high-dimensional data. In this paper, we propose a new approach to construct approximate k-NN graphs with emphasis in: efficiency and accuracy. We hierarchically and randomly divide the data points into subsets and build an exact neighborhood graph over each subset, achieving a base approximate neighborhood graph; we then repeat this process for several times to generate multiple neighborhood graphs, which are combined to yield a more accurate approximate neighborhood graph. Furthermore, we propose a neighborhood propagation scheme to further enhance the accuracy. We show both theoretical and empirical accuracy and efficiency of our approach to k-NN graph construction and demonstrate significant speed-up in dealing with large scale visual data.},
keywords={Accuracy;Complexity theory;Approximation algorithms;Indexing;Nearest neighbor searches;Visualization},
doi={10.1109/CVPR.2012.6247790},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247791,
author={Zhang, Kaibing and Gao, Xinbo and Tao, Dacheng and Li, Xuelong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-scale dictionary for single image super-resolution},
year={2012},
volume={},
number={},
pages={1114-1121},
abstract={Reconstruction- and example-based super-resolution (SR) methods are promising for restoring a high-resolution (HR) image from low-resolution (LR) image(s). Under large magnification, reconstruction-based methods usually fail to hallucinate visual details while example-based methods sometimes introduce unexpected details. Given a generic LR image, to reconstruct a photo-realistic SR image and to suppress artifacts in the reconstructed SR image, we introduce a multi-scale dictionary to a novel SR method that simultaneously integrates local and non-local priors. The local prior suppresses artifacts by using steering kernel regression to predict the target pixel from a small local area. The non-local prior enriches visual details by taking a weighted average of a large neighborhood as an estimate of the target pixel. Essentially, these two priors are complementary to each other. Experimental results demonstrate that the proposed method can produce high quality SR recovery both quantitatively and perceptually.},
keywords={Strontium;Image reconstruction;Dictionaries;Image edge detection;Kernel;Redundancy;Training},
doi={10.1109/CVPR.2012.6247791},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247792,
author={Yang, Chunlei and Peng, Jinye and Fan, Jianping},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image collection summarization via dictionary learning for sparse representation},
year={2012},
volume={},
number={},
pages={1122-1129},
abstract={In this paper, a novel framework is developed to achieve effective summarization of large-scale image collection by treating the problem of automatic image summarization as the problem of dictionary learning for sparse representation, e.g., the summarization task can be treated as a dictionary learning task (i.e., the given image set can be reconstructed sparsely with this dictionary). For image set of a specific category or a mixture of multiple categories, we have built a sparsity model to reconstruct all its images by using a subset of most representative images (i.e., image summary); and we adopted the simulated annealing algorithm to learn such sparse dictionary by minimizing an explicit optimization function. By investigating their reconstruction ability under sparsity constrain and diversity constrain, we have quantitatively measure the performance of various summarization algorithms. Our experimental results have shown that our dictionary learning for sparse representation algorithm can obtain more accurate summary as compared with other baseline algorithms.},
keywords={Dictionaries;Image reconstruction;Visualization;Clustering algorithms;Encoding;Simulated annealing},
doi={10.1109/CVPR.2012.6247792},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247793,
author={Friedman, Itamar and Zelnik-Manor, Lihi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Icon scanning: Towards next generation QR codes},
year={2012},
volume={},
number={},
pages={1130-1137},
abstract={Undoubtedly, a key feature in the popularity of smartmobile devices is the numerous applications one can install. Frequently, we learn about an application we desire by seeing it on a review site, someone else's device, or a magazine. A user-friendly way to obtain this particular application could be by taking a snapshot of its corresponding icon and being directed automatically to its download link. Such a solution exists today for QR codes, which can be thought of as icons with a binary pattern. In this paper we extend this to App-icons and propose a complete system for automatic icon-scanning: it first detects the icon in a snapshot and then recognizes it. Icon scanning is a highly challenging problem due to the large variety of icons (~500K in App-Store) and background wallpapers. In addition, our system should further deal with the challenges introduced by taking pictures of a screen. Nevertheless, the novel solution proposed in this paper provides high detection and recognition rates. We test our complete icon-scanning system on icon snapshots taken by independent users, and search them within the entire set of icons in App-Store. Our success rates are high and improve significantly on other methods.},
keywords={Image edge detection;Image color analysis;Visualization;Shape;Databases;Accuracy;Robustness},
doi={10.1109/CVPR.2012.6247793},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247794,
author={Davis, Darren R. and Hayes, Wayne B.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automated quantitative description of spiral galaxy arm-segment structure},
year={2012},
volume={},
number={},
pages={1138-1145},
abstract={We describe a system that builds quantitative structural descriptions of spiral galaxies. This enables translation of sky survey images into data needed to help address fundamental astrophysical questions such as the origin of spiral structure - a phenomenon that has eluded full theoretical description despite 150 years of study. The difficulty of automated measurement is underscored by the fact that, to date, only manually-guided efforts (such as the citizen science project Galaxy Zoo) have been able to extract structural information about spiral galaxies. An automated approach is needed to eliminate measurement subjectivity and handle the otherwise-overwhelming image quantities (up to billions of images) from near-future surveys. Our approach automatically describes spiral galaxy structure as a set of arcs fit to pixel clusters, precisely characterizing spiral arm segment arrangement while retaining the flexibility needed to accommodate the observed wide variety of spiral galaxy structure. The largest existing quantitative measurements were manually-guided and encompassed fewer than 100 galaxies, while we have already applied our method to nearly 30,000 galaxies. Our output is consistent with previous information, both quantitatively over small existing samples, and qualitatively with human classifications.},
keywords={Spirals;Humans;Windings;Brightness;Transforms;Shape;Extraterrestrial measurements},
doi={10.1109/CVPR.2012.6247794},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247795,
author={He, Lihuo and Tao, Dacheng and Li, Xuelong and Gao, Xinbo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sparse representation for blind image quality assessment},
year={2012},
volume={},
number={},
pages={1146-1153},
abstract={Blind image quality assessment (BIQA) is an important yet difficult task in image processing related applications. Existing algorithms for universal BIQA learn a mapping from features of an image to the corresponding subjective quality or divide the image into different distortions before mapping. Although these algorithms are promising, they face the following problems: (1) they require a large number of samples (pairs of distorted image and its subjective quality) to train a robust mapping; (2) they are sensitive to different datasets; and (3) they have to be retrained when new training samples are available. In this paper, we introduce a simple yet effective algorithm based upon the sparse representation of natural scene statistics (NSS) feature. It consists of three key steps: extracting NSS features in the wavelet domain, representing features via sparse coding, and weighting differential mean opinion scores by the sparse coding coefficients to obtain the final visual quality values. Thorough experiments on standard databases show that the proposed algorithm outperforms representative BIQA algorithms and some full-reference metrics.},
keywords={Measurement;Databases;Training;Feature extraction;Dictionaries;Image quality;Transform coding},
doi={10.1109/CVPR.2012.6247795},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247796,
author={Han, Dongjin and Cooper, David B. and Hahn, Hern-soo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast axis estimation from a segment of rotationally symmetric object},
year={2012},
volume={},
number={},
pages={1154-1161},
abstract={This paper proposes a new method for estimating the symmetric axis of a pottery from its small fragment using surface geometry. For the automatic assembly of broken sherds, the axis estimation is an important measure [2]. When a fragment is small, it is difficult to estimate axis orientation since it looks like a patch of a sphere and conventional methods mostly fail, but the proposed method provides reliable axis estimation by using multiple constraints. The computational cost is also much lowered. To estimate the symmetric axis, the proposed algorithm uses three constraints: (1) the curvature is constant on a circumference CH. (2) the curvature is invariant in any scale. (3) also the principal curvatures does not vary on CH. CH is a planar circle which is one of all the possible circumferences of a pottery or sherd. A hypothesis test for axis is performed using maximum likelihood. The variance of curvature, multi-scale curvature and principal curvatures are computed in the likelihood function. We also show that the principal curvatures can be used for grouping of sherds. The grouping of sherds will reduce the computation significantly by omitting impossible configurations in pottery assembly.},
keywords={Shape;Assembly;Histograms;Maximum likelihood estimation;Face;Vectors},
doi={10.1109/CVPR.2012.6247796},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247797,
author={Gao, Dashan and Yao, Yi and Pan, Feng and Yu, Ting and Yu, Bing and Guan, Li and Dixon, Walter and Yanoff, Brian and Tian, Tai-Peng and Krahnstoever, Nils},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Computer vision aided target linked radiation imaging},
year={2012},
volume={},
number={},
pages={1162-1169},
abstract={In this paper, we demonstrated an application of video tracking to radiation detection, where a vision-based tracking system enables a traditional CZT (cadmium zinc telluride)-based radiation imaging device to detect radioactive targets that are in motion. An integrated real-time system consisting of multiple fixed cameras and radiation detectors was implemented and tested. The multi-camera tracking system combines multiple feature cues (such as silhouette, appearance, and geometry) from different viewing angles to ensure consistent target identities under challenging tracking conditions. Experimental results show that both the video tracking and the integrated systems perform accurately and persistently under various scenarios involving multiple vehicles, driving speeds, and driving patterns. The results also validate and reiterate the importance of video tracking as an enabling technology in the field of radiation imaging.},
keywords={Vehicles;Target tracking;Detectors;Radiation imaging;Cameras;Streaming media},
doi={10.1109/CVPR.2012.6247797},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247798,
author={Beijbom, Oscar and Edmunds, Peter J. and Kline, David I. and Mitchell, B. Greg and Kriegman, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automated annotation of coral reef survey images},
year={2012},
volume={},
number={},
pages={1170-1177},
abstract={With the proliferation of digital cameras and automatic acquisition systems, scientists can acquire vast numbers of images for quantitative analysis. However, much image analysis is conducted manually, which is both time consuming and prone to error. As a result, valuable scientific data from many domains sit dormant in image libraries awaiting annotation. This work addresses one such domain: coral reef coverage estimation. In this setting, the goal, as defined by coral reef ecologists, is to determine the percentage of the reef surface covered by rock, sand, algae, and corals; it is often desirable to resolve these taxa at the genus level or below. This is challenging since the data exhibit significant within class variation, the borders between classes are complex, and the viewpoints and image quality vary. We introduce Moorea Labeled Corals, a large multi-year dataset with 400,000 expert annotations, to the computer vision community, and argue that this type of ecological data provides an excellent opportunity for performance benchmarking. We also propose a novel algorithm using texture and color descriptors over multiple scales that outperforms commonly used techniques from the texture classification literature. We show that the proposed algorithm accurately estimates coral coverage across locations and years, thereby taking a significant step towards reliable automated coral reef image annotation.},
keywords={Image color analysis;Algae;Benchmark testing;Histograms;Computer vision;Vectors;Shape},
doi={10.1109/CVPR.2012.6247798},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247799,
author={Lang, Haitao and Ling, Haibin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Classifying covert photographs},
year={2012},
volume={},
number={},
pages={1178-1185},
abstract={The advances in image acquisition techniques make recording images never easier and brings a great convenience to our daily life. It raises at the same time the issue of privacy protection in the photographs. One particular problem addressed in this paper is about covert photographs, which are taken secretly and often violate the subjects' willingness. We study the task of automatic covert photograph classification, which can be used to help inhibiting distribution of such images (e.g., Internet image filtering). By carefully collecting and investigating a large covert vs. non-covert photographs dataset, we observed that there are many features (e.g., degree of blur) that seem to be correlated with covert photographs, but counter examples always exist. In addition, we observed that image visual attributes (e.g., photo composition) play an important role in distinguishing covert photographs. These observations motivate us to fuse both low level images statistics and middle level attribute features for classifying covert images. In particular, we propose a solution using multiple kernel learning to combine 10 different image features and 31 image attributes. We evaluated thoroughly the proposed approach together with many different solutions including some state-of-the-art image classifiers. The effectiveness of the proposed solution is clearly demonstrated in the results. Furthermore, as the first study to this problem, we expect our study to motivate further research investigations.},
keywords={Kernel;Photography;Image color analysis;Histograms;Privacy;Internet;Feature extraction},
doi={10.1109/CVPR.2012.6247799},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247800,
author={Vaca-Castano, Gonzalo and Zamir, Amir Roshan and Shah, Mubarak},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={City scale geo-spatial trajectory estimation of a moving camera},
year={2012},
volume={},
number={},
pages={1186-1193},
abstract={This paper presents a novel method for estimating the geospatial trajectory of a moving camera with unknown intrinsic parameters, in a city-scale urban environment. The proposed method is based on a three step process that includes: 1) finding the best visual matches of individual images to a dataset of geo-referenced street view images, 2) Bayesian tracking to estimate the frame localization and its temporal evolution, and 3) a trajectory reconstruction algorithm to eliminate inconsistent estimations. As a result of matching features in query image with the features in the reference geo-taged images, in the first step, we obtain a distribution of geolocated votes of matching features which is interpreted as the likelihood of the location (latitude and longitude) given the current observation. In the second step, Bayesian tracking framework is used to estimate the temporal evolution of frame geolocalization based on the previous state probabilities and current likelihood. Finally, once a trajectory is estimated, we perform a Minimum Spanning Trees (MST) based trajectory reconstruction algorithm to eliminate trajectory loops or noisy estimations. The proposed method was tested on sixty minutes of video, which included footage downloaded from YouTube and footage captured by random users in Orlando and Pittsburgh.},
keywords={Trajectory;Geology;Cameras;Bayesian methods;Mathematical model;Estimation;Cities and towns},
doi={10.1109/CVPR.2012.6247800},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247801,
author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A database for fine grained activity detection of cooking activities},
year={2012},
volume={},
number={},
pages={1194-1201},
abstract={While activity recognition is a current focus of research the challenging problem of fine-grained activity recognition is largely overlooked. We thus propose a novel database of 65 cooking activities, continuously recorded in a realistic setting. Activities are distinguished by fine-grained body motions that have low inter-class variability and high intra-class variability due to diverse subjects and ingredients. We benchmark two approaches on our dataset, one based on articulated pose tracks and the second using holistic video features. While the holistic approach outperforms the pose-based approach, our evaluation suggests that fine-grained activities are more difficult to detect and the body model can help in those cases. Providing high-resolution videos as well as an intermediate pose representation we hope to foster research in fine-grained activity recognition.},
keywords={Joints;Databases;Humans;Estimation;Feature extraction;Trajectory;Surveillance},
doi={10.1109/CVPR.2012.6247801},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247802,
author={Xu, Chenliang and Corso, Jason J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Evaluation of super-voxel methods for early video processing},
year={2012},
volume={},
number={},
pages={1202-1209},
abstract={Supervoxel segmentation has strong potential to be incorporated into early video analysis as superpixel segmentation has in image analysis. However, there are many plausible supervoxel methods and little understanding as to when and where each is most appropriate. Indeed, we are not aware of a single comparative study on supervoxel segmentation. To that end, we study five supervoxel algorithms in the context of what we consider to be a good supervoxel: namely, spatiotemporal uniformity, object/region boundary detection, region compression and parsimony. For the evaluation we propose a comprehensive suite of 3D volumetric quality metrics to measure these desirable supervoxel characteristics. We use three benchmark video data sets with a variety of content-types and varying amounts of human annotations. Our findings have led us to conclusive evidence that the hierarchical graph-based and segmentation by weighted aggregation methods perform best and almost equally-well on nearly all the metrics and are the methods of choice given our proposed assumptions.},
keywords={Image segmentation;Measurement;Spatiotemporal phenomena;Accuracy;Image color analysis;Benchmark testing;Humans},
doi={10.1109/CVPR.2012.6247802},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247803,
author={O'Hara, Stephen and Draper, Bruce A.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Scalable action recognition with a subspace forest},
year={2012},
volume={},
number={},
pages={1210-1217},
abstract={We present a novel structure, called a Subspace Forest, designed to provide an efficient approximate nearest neighbor query of subspaces represented as points on Grassmann manifolds. We apply this structure to action recognition by representing actions as subspaces spanning a sequence of thumbnail image tiles extracted from a tracked entity. The Subspace Forest lifts the concept of randomized decision forests from classifying vectors to classifying subspaces, and employs a splitting method that respects the underlying manifold geometry. The Subspace Forest is an inherently parallel structure and is highly scalable due to O(log N) recognition time complexity. Our experimental results demonstrate state-of-the-art classification accuracies on the well-known KTH Actions and UCF Sports benchmarks, and a competitive score on Cambridge Gestures. In addition to being both highly accurate and scalable, the Subspace Forest is built without supervision and requires no extensive validation stage for model selection. Conceptually, the Subspace Forest could be used anywhere set-to-set feature matching is desired.},
keywords={Vegetation;Manifolds;Entropy;Vectors;Accuracy;Scalability;Decision trees},
doi={10.1109/CVPR.2012.6247803},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247804,
author={Khamis, Sameh and Morariu, Vlad I. and Davis, Larry S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A flow model for joint action recognition and identity maintenance},
year={2012},
volume={},
number={},
pages={1218-1225},
abstract={We propose a framework that performs action recognition and identity maintenance of multiple targets simultaneously. Instead of first establishing tracks using an appearance model and then performing action recognition, we construct a network flow-based model that links detected bounding boxes across video frames while inferring activities, thus integrating identity maintenance and action recognition. Inference in our model reduces to a constrained minimum cost flow problem, which we solve exactly and efficiently. By leveraging both appearance similarity and action transition likelihoods, our model improves on state-of-the-art results on action recognition for two datasets.},
keywords={Humans;Mathematical model;Training;Context;Maintenance engineering;Equations;Joints},
doi={10.1109/CVPR.2012.6247804},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247805,
author={Fathi, Alircza and Hodgins, Jessica K. and Rehg, James M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Social interactions: A first-person perspective},
year={2012},
volume={},
number={},
pages={1226-1233},
abstract={This paper presents a method for the detection and recognition of social interactions in a day-long first-person video of u social event, like a trip to an amusement park. The location and orientation of faces are estimated and used to compute the line of sight for each face. The context provided by all the faces in a frame is used to convert the lines of sight into locations in space to which individuals attend. Further, individuals are assigned roles based on their patterns of attention. The rotes and locations of individuals are analyzed over time to detect and recognize the types of social interactions. In addition to patterns of face locations and attention, the head movements of the first-person can provide additional useful cues as to their attentional focus. We demonstrate encouraging results on detection and recognition of social interactions in first-person videos captured from multiple days of experience in amusement parks.},
keywords={Cameras;Context;Face recognition;Social network services;Inference algorithms;Computer vision;Vectors},
doi={10.1109/CVPR.2012.6247805},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247806,
author={Sadanand, Sreemanananth and Corso, Jason J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Action bank: A high-level representation of activity in video},
year={2012},
volume={},
number={},
pages={1234-1241},
abstract={Activity recognition in video is dominated by low- and mid-level features, and while demonstrably capable, by nature, these features carry little semantic meaning. Inspired by the recent object bank approach to image representation, we present Action Bank, a new high-level representation of video. Action bank is comprised of many individual action detectors sampled broadly in semantic space as well as viewpoint space. Our representation is constructed to be semantically rich and even when paired with simple linear SVM classifiers is capable of highly discriminative performance. We have tested action bank on four major activity recognition benchmarks. In all cases, our performance is better than the state of the art, namely 98.2% on KTH (better by 3.3%), 95.0% on UCF Sports (better by 3.7%), 57.9% on UCF50 (baseline is 47.9%), and 26.9% on HMDB51 (baseline is 23.2%). Furthermore, when we analyze the classifiers, we find strong transfer of semantics from the constituent action detectors to the bank classifier.},
keywords={Detectors;Humans;Semantics;Support vector machines;Spatiotemporal phenomena;Correlation;Vectors},
doi={10.1109/CVPR.2012.6247806},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247807,
author={Raptis, Michalis and Kokkinos, Iasonas and Soatto, Stefano},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discovering discriminative action parts from mid-level video representations},
year={2012},
volume={},
number={},
pages={1242-1249},
abstract={We describe a mid-level approach for action recognition. From an input video, we extract salient spatio-temporal structures by forming clusters of trajectories that serve as candidates for the parts of an action. The assembly of these clusters into an action class is governed by a graphical model that incorporates appearance and motion constraints for the individual parts and pairwise constraints for the spatio-temporal dependencies among them. During training, we estimate the model parameters discriminatively. During classification, we efficiently match the model to a video using discrete optimization. We validate the model's classification ability in standard benchmark datasets and illustrate its potential to support a fine-grained analysis that not only gives a label to a video, but also identifies and localizes its constituent parts.},
keywords={Trajectory;Training;Vectors;Video sequences;Histograms;Support vector machines;Biological system modeling},
doi={10.1109/CVPR.2012.6247807},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247808,
author={Tang, Kevin and Fei-Fei, Li and Koller, Daphne},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning latent temporal structure for complex event detection},
year={2012},
volume={},
number={},
pages={1250-1257},
abstract={In this paper, we tackle the problem of understanding the temporal structure of complex events in highly varying videos obtained from the Internet. Towards this goal, we utilize a conditional model trained in a max-margin framework that is able to automatically discover discriminative and interesting segments of video, while simultaneously achieving competitive accuracies on difficult detection and recognition tasks. We introduce latent variables over the frames of a video, and allow our algorithm to discover and assign sequences of states that are most discriminative for the event. Our model is based on the variable-duration hidden Markov model, and models durations of states in addition to the transitions between states. The simplicity of our model allows us to perform fast, exact inference using dynamic programming, which is extremely important when we set our sights on being able to process a very large number of videos quickly and efficiently. We show promising results on the Olympic Sports dataset [16] and the 2011 TRECVID Multimedia Event Detection task [18]. We also illustrate and visualize the semantic understanding capabilities of our model.},
keywords={Videos;Hidden Markov models;Motion segmentation;Internet;Histograms;Vectors;Event detection},
doi={10.1109/CVPR.2012.6247808},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247809,
author={Kim, Kihwan and Lee, Dongryeol and Essa, Irfan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Detecting regions of interest in dynamic scenes with camera motions},
year={2012},
volume={},
number={},
pages={1258-1265},
abstract={We present a method to detect the regions of interests in moving camera views of dynamic scenes with multiple moving objects. We start by extracting a global motion tendency that reflects the scene context by tracking movements of objects in the scene. We then use Gaussian process regression to represent the extracted motion tendency as a stochastic vector field. The generated stochastic field is robust to noise and can handle a video from an uncalibrated moving camera. We use the stochastic field for predicting important future regions of interest as the scene evolves dynamically. We evaluate our approach on a variety of videos of team sports and compare the detected regions of interest to the camera motion generated by actual camera operators. Our experimental results demonstrate that our approach is computationally efficient and provides better predictions than previously proposed RBF-based approaches.},
keywords={Cameras;Vectors;Tracking;Ground penetrating radar;Gaussian processes;Dynamics},
doi={10.1109/CVPR.2012.6247809},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247810,
author={Kwon, Junseok and Lee, Kyoung Mu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A unified framework for event summarization and rare event detection},
year={2012},
volume={},
number={},
pages={1266-1273},
abstract={In this paper, we have proposed an unified framework for event summarization and rare event detection and presented the graph-structure learning and editing method to solve these problems efficiently. The experimental results demonstrated that the proposed method outperformed conventional algorithms in complex and crowded public scenes by exploiting and utilizing causality, frequency, and significance of relations of events.},
keywords={Event detection;Proposals;Frequency measurement;Density functional theory;Data mining;Markov processes;Hidden Markov models},
doi={10.1109/CVPR.2012.6247810},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247811,
author={Chen, Chao-Yeh and Grauman, Kristen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient activity detection with max-subgraph search},
year={2012},
volume={},
number={},
pages={1274-1281},
abstract={We propose an efficient approach that unifies activity categorization with space-time localization. The main idea is to pose activity detection as a maximum-weight connected subgraph problem over a learned space-time graph constructed on the test sequence. We show this permits an efficient branch-and-cut solution for the best-scoring - and possibly non-cubically shaped - portion of the video for a given activity classifier. The upshot is a fast method that can evaluate a broader space of candidates than was previously practical, which we find often leads to more accurate detection. We demonstrate the proposed algorithm on three datasets, and show its speed and accuracy advantages over multiple existing search strategies.},
keywords={Detectors;Training;Search problems;Shape;Tracking;Support vector machines;Histograms},
doi={10.1109/CVPR.2012.6247811},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247812,
author={Zhou, Feng and De la Torre, Fernando},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Generalized time warping for multi-modal alignment of human motion},
year={2012},
volume={},
number={},
pages={1282-1289},
abstract={Temporal alignment of human motion has been a topic of recent interest due to its applications in animation, telerehabilitation and activity recognition among others. This paper presents generalized time warping (GTW), an extension of dynamic time warping (DTW) for temporally aligning multi-modal sequences from multiple subjects performing similar activities. GTW solves three major drawbacks of existing approaches based on DTW: (1) GTW provides a feature weighting layer to adapt different modalities (e.g., video and motion capture data), (2) GTW extends DTW by allowing a more flexible time warping as combination of monotonic functions, (3) unlike DTW that typically incurs in quadratic cost, GTW has linear complexity. Experimental results demonstrate that GTW can efficiently solve the multi-modal temporal alignment problem and outperforms state-of-the-art DTW methods for temporal alignment of time series within the same modality.},
keywords={Time series analysis;Humans;Computational complexity;Dynamics;Computer vision;Computer graphics},
doi={10.1109/CVPR.2012.6247812},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247813,
author={Wang, Jiang and Liu, Zicheng and Wu, Ying and Yuan, Junsong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Mining actionlet ensemble for action recognition with depth cameras},
year={2012},
volume={},
number={},
pages={1290-1297},
abstract={Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.},
keywords={Joints;Humans;Hidden Markov models;Cameras;Robustness;Noise;Feature extraction},
doi={10.1109/CVPR.2012.6247813},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247814,
author={Natarajan, Pradeep and Wu, Shuang and Vitaladevuni, Shiv and Zhuang, Xiaodan and Tsakalidis, Stavros and Park, Unsang and Prasad, Rohit and Natarajan, Premkumar},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multimodal feature fusion for robust event detection in web videos},
year={2012},
volume={},
number={},
pages={1298-1305},
abstract={Combining multiple low-level visual features is a proven and effective strategy for a range of computer vision tasks. However, limited attention has been paid to combining such features with information from other modalities, such as audio and videotext, for large scale analysis of web videos. In our work, we rigorously analyze and combine a large set of low-level features that capture appearance, color, motion, audio and audio-visual co-occurrence patterns in videos. We also evaluate the utility of high-level (i.e., semantic) visual information obtained from detecting scene, object, and action concepts. Further, we exploit multimodal information by analyzing available spoken and videotext content using state-of-the-art automatic speech recognition (ASR) and videotext recognition systems. We combine these diverse features using a two-step strategy employing multiple kernel learning (MKL) and late score level fusion methods. Based on the TRECVID MED 2011 evaluations for detecting 10 events in a large benchmark set of ~45000 videos, our system showed the best performance among the 19 international teams.},
keywords={Videos;Feature extraction;Speech;Vectors;Kernel;Encoding;Image color analysis},
doi={10.1109/CVPR.2012.6247814},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247815,
author={Derpanis, Konstantinos G. and Lecce, Matthieu and Daniilidis, Kostas and Wildes, Richard P.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Dynamic scene understanding: The role of orientation features in space and time in scene classification},
year={2012},
volume={},
number={},
pages={1306-1313},
abstract={Natural scene classification is a fundamental challenge in computer vision. By far, the majority of studies have limited their scope to scenes from single image stills and thereby ignore potentially informative temporal cues. The current paper is concerned with determining the degree of performance gain in considering short videos for recognizing natural scenes. Towards this end, the impact of multiscale orientation measurements on scene classification is systematically investigated, as related to: (i) spatial appearance, (ii) temporal dynamics and (iii) joint spatial appearance and dynamics. These measurements in visual space, x-y, and spacetime, x-y-t, are recovered by a bank of spatiotemporal oriented energy filters. In addition, a new data set is introduced that contains 420 image sequences spanning fourteen scene categories, with temporal scene information due to objects and surfaces decoupled from camera-induced ones. This data set is used to evaluate classification performance of the various orientation-related representations, as well as state-of-the-art alternatives. It is shown that a notable performance increase is realized by spatiotemporal approaches in comparison to purely spatial or purely temporal methods.},
keywords={Spatiotemporal phenomena;Dynamics;Videos;Layout;Image sequences;Energy measurement;Visualization},
doi={10.1109/CVPR.2012.6247815},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247816,
author={Amer, Mohamed R. and Todorovic, Sinisa},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sum-product networks for modeling activities with stochastic structure},
year={2012},
volume={},
number={},
pages={1314-1321},
abstract={This paper addresses recognition of human activities with stochastic structure, characterized by variable spacetime arrangements of primitive actions, and conducted by a variable number of actors. We demonstrate that modeling aggregate counts of visual words is surprisingly expressive enough for such a challenging recognition task. An activity is represented by a sum-product network (SPN). SPN is a mixture of bags-of-words (BoWs) with exponentially many mixture components, where subcomponents are reused by larger ones. SPN consists of terminal nodes representing BoWs, and product and sum nodes organized in a number of layers. The products are aimed at encoding particular configurations of primitive actions, and the sums serve to capture their alternative configurations. The connectivity of SPN and parameters of BoW distributions are learned under weak supervision using the EM algorithm. SPN inference amounts to parsing the SPN graph, which yields the most probable explanation (MPE) of the video in terms of activity detection and localization. SPN inference has linear complexity in the number of nodes, under fairly general conditions, enabling fast and scalable recognition. A new Volleyball dataset is compiled and annotated for evaluation. Our classification accuracy and localization precision and recall are superior to those of the state-of-the-art on the benchmark and our Volleyball datasets.},
keywords={Visualization;Probabilistic logic;Hidden Markov models;Feature extraction;Inference algorithms;Spatiotemporal phenomena;Graphical models},
doi={10.1109/CVPR.2012.6247816},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247817,
author={Burgos-Artizzu, Xavier P. and Dollár, Piotr and Lin, Dayu and Anderson, David J. and Perona, Pietro},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Social behavior recognition in continuous video},
year={2012},
volume={},
number={},
pages={1322-1329},
abstract={We present a novel method for analyzing social behavior. Continuous videos are segmented into action `bouts' by building a temporal context model that combines features from spatio-temporal energy and agent trajectories. The method is tested on an unprecedented dataset of videos of interacting pairs of mice, which was collected as part of a state-of-the-art neurophysiological study of behavior. The dataset comprises over 88 hours (8 million frames) of annotated videos. We find that our novel trajectory features, used in a discriminative framework, are more informative than widely used spatio-temporal features; furthermore, temporal context plays an important role for action recognition in continuous videos. Our approach may be seen as a baseline method on this dataset, reaching a mean recognition rate of 61.2% compared to the expert's agreement rate of about 70%.},
keywords={Mice;Humans;Context;Trajectory;Benchmark testing;Standards},
doi={10.1109/CVPR.2012.6247817},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247818,
author={Wang, Zhaowen and Wang, Jinjun and Xiao, Jing and Lin, Kai-Hsiang and Huang, Thomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Substructure and boundary modeling for continuous action recognition},
year={2012},
volume={},
number={},
pages={1330-1337},
abstract={This paper introduces a probabilistic graphical model for continuous action recognition with two novel components: substructure transition model and discriminative boundary model. The first component encodes the sparse and global temporal transition prior between action primitives in state-space model to handle the large spatial-temporal variations within an action class. The second component enforces the action duration constraint in a discriminative way to locate the transition boundaries between actions more accurately. The two components are integrated into a unified graphical structure to enable effective training and inference. Our comprehensive experimental results on both public and in-house datasets show that, with the capability to incorporate additional information that had not been explicitly or efficiently modeled by previous methods, our proposed algorithm achieved significantly improved performance for continuous action recognition.},
keywords={Hidden Markov models;Logistics;Training;Superluminescent diodes;Humans;Estimation;Markov processes},
doi={10.1109/CVPR.2012.6247818},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247819,
author={Duan, Lixin and Xu, Dong and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach},
year={2012},
volume={},
number={},
pages={1338-1345},
abstract={Recent work has demonstrated the effectiveness of domain adaptation methods for computer vision applications. In this work, we propose a new multiple source domain adaptation method called Domain Selection Machine (DSM) for event recognition in consumer videos by leveraging a large number of loosely labeled web images from different sources (e.g., Flickr.com and Photosig.com), in which there are no labeled consumer videos. Specifically, we first train a set of SVM classifiers (referred to as source classifiers) by using the SIFT features of web images from different source domains. We propose a new parametric target decision function to effectively integrate the static SIFT features from web images/video keyframes and the spacetime (ST) features from consumer videos. In order to select the most relevant source domains, we further introduce a new data-dependent regularizer into the objective of Support Vector Regression (SVR) using the ϵ-insensitive loss, which enforces the target classifier shares similar decision values on the unlabeled consumer videos with the selected source classifiers. Moreover, we develop an alternating optimization algorithm to iteratively solve the target decision function and a domain selection vector which indicates the most relevant source domains. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed method DSM over the state-of-the-art by a performance gain up to 46.41%.},
keywords={Videos;Vectors;Optimization;Feature extraction;Support vector machines;Image recognition;YouTube},
doi={10.1109/CVPR.2012.6247819},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247820,
author={Lee, Yong Jae and Ghosh, Joydeep and Grauman, Kristen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discovering important people and objects for egocentric video summarization},
year={2012},
volume={},
number={},
pages={1346-1353},
abstract={We developed an approach to summarize egocentric video. We introduced novel egocentric features to train a regressor that predicts important regions. Using the discovered important regions, our approach produces significantly more informative summaries than traditional methods that often include irrelevant or redundant information.},
keywords={Cameras;Visualization;Predictive models;Image color analysis;Training;Context;Humans},
doi={10.1109/CVPR.2012.6247820},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247821,
author={Lan, Tian and Sigal, Leonid and Mori, Greg},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Social roles in hierarchical models for human activity recognition},
year={2012},
volume={},
number={},
pages={1354-1361},
abstract={We present a hierarchical model for human activity recognition in entire multi-person scenes. Our model describes human behaviour at multiple levels of detail, ranging from low-level actions through to high-level events. We also include a model of social roles, the expected behaviours of certain people, or groups of people, in a scene. The hierarchical model includes these varied representations, and various forms of interactions between people present in a scene. The model is trained in a discriminative max-margin framework. Experimental results demonstrate that this model can improve performance at all considered levels of detail, on two challenging datasets.},
keywords={Humans;Support vector machines;Video sequences;Context;Vectors;Context modeling;Surveillance},
doi={10.1109/CVPR.2012.6247821},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247822,
author={Li, Binlong and Camps, Octavia I. and Sznaier, Mario},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Cross-view activity recognition using Hankelets},
year={2012},
volume={},
number={},
pages={1362-1369},
abstract={Human activity recognition is central to many practical applications, ranging from visual surveillance to gaming interfacing. Most approaches addressing this problem are based on localized spatio-temporal features that can vary significantly when the viewpoint changes. As a result, their performances rapidly deteriorate as the difference between the viewpoints of the training and testing data increases. In this paper, we introduce a new type of feature, the “Hankelet” that captures dynamic properties of short tracklets. While Hankelets do not carry any spatial information, they bring invariant properties to changes in viewpoint that allow for robust cross-view activity recognition, i.e. when actions are recognized using a classifier trained on data from a different viewpoint. Our experiments on the IXMAS dataset show that using Hanklets improves the state of the art performance by over 20%.},
keywords={Trajectory;Vectors;Training;Noise measurement;Histograms;Testing;Cameras},
doi={10.1109/CVPR.2012.6247822},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247823,
author={Wang, Sen and Yang, Yi and Ma, Zhigang and Li, Xue and Pang, Chaoyi and Hauptmann, Alexander G.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Action recognition by exploring data distribution and feature correlation},
year={2012},
volume={},
number={},
pages={1370-1377},
abstract={Human action recognition in videos draws strong research interest in computer vision because of its promising applications for video surveillance, video annotation, interactive gaming, etc. However, the amount of video data containing human actions is increasing exponentially, which makes the management of these resources a challenging task. Given a database with huge volumes of unlabeled videos, it is prohibitive to manually assign specific action types to these videos. Considering that it is much easier to obtain a small number of labeled videos, a practical solution for organizing them is to build a mechanism which is able to conduct action annotation automatically by leveraging the limited labeled videos. Motivated by this intuition, we propose an automatic video annotation algorithm by integrating semi-supervised learning and shared structure analysis into a joint framework for human action recognition. We apply our algorithm on both synthetic and realistic video datasets, including KTH [20], CareMedia dataset [1], Youtube action [12] and its extended version, UCF50 [2]. Extensive experiments demonstrate that the proposed algorithm outperforms the compared algorithms for action recognition. Most notably, our method has a very distinct advantage over other compared algorithms when we have only a few labeled samples.},
keywords={Videos;Humans;Correlation;Manifolds;Algorithm design and analysis;Training data;Feature extraction},
doi={10.1109/CVPR.2012.6247823},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247824,
author={Packer, Ben and Saenko, Kate and Koller, Daphne},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A combined pose, object, and feature model for action understanding},
year={2012},
volume={},
number={},
pages={1378-1385},
abstract={Understanding natural human activity involves not only identifying the action being performed, but also locating the semantic elements of the scene and describing the person's interaction with them. We present a system that is able to recognize complex, fine-grained human actions involving the manipulation of objects in realistic action sequences. Our method takes advantage of recent advances in sensors and pose trackers in learning an action model that draws on successful discriminative techniques while explicitly modeling both pose trajectories and object manipulations. By combining these elements in a single model, we are able to simultaneously recognize actions and track the location and manipulation of objects. To showcase this ability, we introduce a novel Cooking Action Dataset that contains video, depth readings, and pose tracks from a Kinect sensor. We show that our model outperforms existing state of the art techniques on this dataset as well as the VISINT dataset with only video sequences.},
keywords={Trajectory;Visualization;Sensors;Humans;Dynamics;Feature extraction;Training},
doi={10.1109/CVPR.2012.6247824},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247825,
author={Klowsky, Ronny and Kuijper, Arjan and Goesele, Michael},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Modulation transfer function of patch-based stereo systems},
year={2012},
volume={},
number={},
pages={1386-1393},
abstract={A widely used technique to recover a 3D surface from photographs is patch-based (multi-view) stereo reconstruction. Current methods are able to reproduce fine surface details, they are however limited by the sampling density and the patch size used for reconstruction. We show that there is a systematic error in the reconstruction depending on the details in the unknown surface (frequencies) and the reconstruction resolution. For this purpose we present a theoretical analysis of patch-based depth reconstruction. We prove that our model of the reconstruction process yields a linear system, allowing us to apply the transfer (or system) function concept. We derive the modulation transfer function theoretically and validate it experimentally on synthetic examples using rendered images as well as on photographs of a 3D test target. Our analysis proves that there is a significant but predictable amplitude loss in reconstructions of fine scale details. In a first experiment on real-world data we show how this can be compensated for within the limits of noise and reconstruction accuracy by an inverse transfer function in frequency space.},
keywords={Image reconstruction;Geometry;Surface reconstruction;Transfer functions;Image resolution;Cameras;Fourier transforms},
doi={10.1109/CVPR.2012.6247825},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247826,
author={Valmadre, Jack and Lucey, Simon},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={General trajectory prior for Non-Rigid reconstruction},
year={2012},
volume={},
number={},
pages={1394-1401},
abstract={Trajectory basis Non-Rigid Structure From Motion (NRSFM) currently faces two problems: the limit of reconstructability and the need to tune the basis size for different sequences. This paper provides a novel theoretical bound on 3D reconstruction error, arguing that the existing definition of reconstructability is fundamentally flawed in that it fails to consider system condition. This insight motivates a novel strategy whereby the trajectory's response to a set of high-pass filters is minimised. The new approach eliminates the need to tune the basis size and is more efficient for long sequences. Additionally, the truncated DCT basis is shown to have a dual interpretation as a high-pass filter. The success of trajectory filter reconstruction is demonstrated quantitatively on synthetic projections of real motion capture sequences and qualitatively on real image sequences.},
keywords={Trajectory;Cameras;Image reconstruction;Discrete cosine transforms;Equations;Shape;Encoding},
doi={10.1109/CVPR.2012.6247826},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247827,
author={Yang, Qingxiong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A non-local cost aggregation method for stereo matching},
year={2012},
volume={},
number={},
pages={1402-1409},
abstract={Matching cost aggregation is one of the oldest and still popular methods for stereo correspondence. While effective and efficient, cost aggregation methods typically aggregate the matching cost by summing/averaging over a user-specified, local support region. This is obviously only locally-optimal, and the computational complexity of the full-kernel implementation usually depends on the region size. In this paper, the cost aggregation problem is re-examined and a non-local solution is proposed. The matching cost values are aggregated adaptively based on pixel similarity on a tree structure derived from the stereo image pair to preserve depth edges. The nodes of this tree are all the image pixels, and the edges are all the edges between the nearest neighboring pixels. The similarity between any two pixels is decided by their shortest distance on the tree. The proposed method is non-local as every node receives supports from all other nodes on the tree. As can be expected, the proposed non-local solution outperforms all local cost aggregation methods on the standard (Middlebury) benchmark. Besides, it has great advantage in extremely low computational complexity: only a total of 2 addition/subtraction operations and 3 multiplication operations are required for each pixel at each disparity level. It is very close to the complexity of unnormalized box filtering using integral image which requires 6 addition/subtraction operations. Unnormalized box filter is the fastest local cost aggregation method but blurs across depth edges. The proposed method was tested on a MacBook Air laptop computer with a 1.8 GHz Intel Core i7 CPU and 4 GB memory. The average runtime on the Middlebury data sets is about 90 milliseconds, and is only about 1.25× slower than unnormalized box filter. A non-local disparity refinement method is also proposed based on the non-local cost aggregation method.},
keywords={Image edge detection;Computational complexity;Runtime;Image color analysis;Stereo vision;Portable computers},
doi={10.1109/CVPR.2012.6247827},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247828,
author={Zheng, Yinqiang and Liu, Guangcan and Sugimoto, Shigeki and Yan, Shuicheng and Okutomi, Masatoshi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Practical low-rank matrix approximation under robust L1-norm},
year={2012},
volume={},
number={},
pages={1410-1417},
abstract={A great variety of computer vision tasks, such as rigid/nonrigid structure from motion and photometric stereo, can be unified into the problem of approximating a low-rank data matrix in the presence of missing data and outliers. To improve robustness, the L1-norm measurement has long been recommended. Unfortunately, existing methods usually fail to minimize the L1-based nonconvex objective function sufficiently. In this work, we propose to add a convex trace-norm regularization term to improve convergence, without introducing too much heterogenous information. We also customize a scalable first-order optimization algorithm to solve the regularized formulation on the basis of the augmented Lagrange multiplier (ALM) method. Extensive experimental results verify that our regularized formulation is reasonable, and the solving algorithm is very efficient, insensitive to initialization and robust to high percentage of missing data and/or outliers1.},
keywords={Robustness;Optimization;Convergence;Computer vision;Approximation algorithms;Least squares approximation},
doi={10.1109/CVPR.2012.6247828},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247829,
author={Agudo, Antonio and Calvo, Begoña and Montiel, J. M. M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Finite Element based sequential Bayesian Non-Rigid Structure from Motion},
year={2012},
volume={},
number={},
pages={1418-1425},
abstract={Navier's equations modelling linear elastic solid deformations are embedded within an Extended Kalman Filter (EKF) to compute a sequential Bayesian estimate for the Non-Rigid Structure from Motion problem. The algorithm processes every single frame of a sequence gathered with a full perspective camera. No prior data association is assumed because matches are computed within the EKF prediction-match-update cycle. Scene is coded as a Finite Element Method (FEM) elastic thin-plate solid, where the discretization nodes are the sparse set of scene points salient in the image. It is assumed a set of Gaussian forces acting on solid nodes to cause scene deformation. The EKF combines in a feedback loop an approximate FEM model and the frame rate measurements from the camera, resulting in an efficient method to embed Navier's equations without resorting to expensive non-linear FEM models. Classical FEM modelling has implied an interactive identification of boundary points to constrain the scene rigid motion, in this work this dissatisfying prior knowledge is no longer needed. The scene and camer rigid motion are combined in a unique pose vector and the estimation is coded relative to the camera. Additionally, the deforming effect of the Gaussian forces on the thin-plate is computed by means of the Moore-Penrose pseudoinverse of the FEM stiffness matrix. The proposed algorithm is validated with three real sequences gathered with hand-held camera observing isometric and non-isometric deformations. It is also shown the consistency of the EKF estimation with respect to ground truth computed from stereo.},
keywords={Cameras;Finite element methods;Vectors;Mathematical model;Equations;Deformable models;Computational modeling},
doi={10.1109/CVPR.2012.6247829},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247830,
author={Basha, Tali and Avidan, Shai and Hornung, Alexander and Matusik, Wojciech},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Structure and motion from scene registration},
year={2012},
volume={},
number={},
pages={1426-1433},
abstract={We propose a method for estimating the 3D structure and the dense 3D motion (scene flow) of a dynamic nonrigid 3D scene, using a camera array. The core idea is to use a dense multi-camera array to construct a novel, dense 3D volumetric representation of the 3D space where each voxel holds an estimated intensity value and a confidence measure of this value. The problem of 3D structure and 3D motion estimation of a scene is thus reduced to a nonrigid registration of two volumes - hence the term ”Scene Registration”. Registering two dense 3D scalar volumes does not require recovering the 3D structure of the scene as a preprocessing step, nor does it require explicit reasoning about occlusions. From this nonrigid registration we accurately extract the 3D scene flow and the 3D structure of the scene, and successfully recover the sharp discontinuities in both time and space. We demonstrate the advantages of our method on a number of challenging synthetic and real data sets.},
keywords={Cameras;Arrays;Estimation;Optical imaging;Robustness;Atmospheric measurements;Particle measurements},
doi={10.1109/CVPR.2012.6247830},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247831,
author={Hedborg, Johan and Forssén, Per-Erik and Felsberg, Michael and Ringaby, Erik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Rolling shutter bundle adjustment},
year={2012},
volume={},
number={},
pages={1434-1441},
abstract={This paper introduces a bundle adjustment (BA) method that obtains accurate structure and motion from rolling shutter (RS) video sequences: RSBA. When a classical BA algorithm processes a rolling shutter video, the resultant camera trajectory is brittle, and complete failures are not uncommon. We exploit the temporal continuity of the camera motion to define residuals of image point trajectories with respect to the camera trajectory. We compare the camera trajectories from RSBA to those from classical BA, and from classical BA on rectified videos. The comparisons are done on real video sequences from an iPhone 4, with ground truth obtained from a global shutter camera, rigidly mounted to the iPhone 4. Compared to classical BA, the rolling shutter model requires just six extra parameters. It also degrades the sparsity of the system Jacobian slightly, but as we demonstrate, the increase in computation time is moderate. Decisive advantages are that RSBA succeeds in cases where competing methods diverge, and consistently produces more accurate results.},
keywords={Cameras;Jacobian matrices;Barium;Trajectory;Indexes;Solid modeling;Geometry},
doi={10.1109/CVPR.2012.6247831},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247832,
author={Kushal, Avanish and Agarwal, Sameer},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Visibility Based Preconditioning for bundle adjustment},
year={2012},
volume={},
number={},
pages={1442-1449},
abstract={We present Visibility Based Preconditioning (VBP) a new technique for efficiently solving the linear least squares problems that arise in bundle adjustment (Triggs et al., 1999). Using the camera-point visibility structure of the scene, we describe the construction of two preconditioners. These preconditioners when combined with an inexact step Levenberg-Marquardt algorithm (Wright and Holt, 1985) offer state of the art performance on the BAL data set (Agarwal et al., 2010), with 3-5× reduction in execution time over currently available methods while delivering comparable or better solution quality.},
keywords={Cameras;Jacobian matrices;Vectors;Symmetric matrices;Vegetation;Approximation methods;Clustering algorithms},
doi={10.1109/CVPR.2012.6247832},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247833,
author={Wendel, Andreas and Maurer, Michael and Graber, Gottfried and Pock, Thomas and Bischof, Horst},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Dense reconstruction on-the-fly},
year={2012},
volume={},
number={},
pages={1450-1457},
abstract={We present a novel system that is capable of generating live dense volumetric reconstructions based on input from a micro aerial vehicle. The distributed reconstruction pipeline is based on state-of-the-art approaches to visual SLAM and variational depth map fusion, and is designed to exploit the individual capabilities of the system components. Results are visualized in real-time on a tablet interface, which gives the user the opportunity to interact. We demonstrate the performance of our approach by capturing several indoor and outdoor scenes on-the-fly and by evaluating our results with respect to a ground-truth model.},
keywords={Image reconstruction;Mobile handsets;Servers;Cameras;Simultaneous localization and mapping;Visualization;Mobile communication},
doi={10.1109/CVPR.2012.6247833},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247834,
author={Jiang, Nianjuan and Tan, Ping and Cheong, Loong-Fah},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Seeing double without confusion: Structure-from-motion in highly ambiguous scenes},
year={2012},
volume={},
number={},
pages={1458-1465},
abstract={3D reconstruction from an unordered set of images may fail due to incorrect epipolar geometries (EG) between image pairs arising from ambiguous feature correspondences. Previous methods often analyze the consistency between different EGs, and regard the largest subset of self-consistent EGs as correct. However, as demonstrated in [14], such a largest self-consistent set often corresponds to incorrect result, especially when there are duplicate structures in the scene. We propose a novel optimization criteria based on the idea of `missing correspondences'. The global minimum of our optimization objective function is associated with the correct solution. We then design an efficient algorithm for minimization, whose convergence to a local minimum is guaranteed. Experimental results show our method outperforms the state-of-the-art.},
keywords={Cameras;Three dimensional displays;Image reconstruction;Equations;Image edge detection;Erbium;Convergence},
doi={10.1109/CVPR.2012.6247834},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247835,
author={Yang, Wenzhuo and Zhang, Guofeng and Bao, Hujun and Kim, Jiwon and Lee, Ho Young},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Consistent depth maps recovery from a trinocular video sequence},
year={2012},
volume={},
number={},
pages={1466-1473},
abstract={In this paper, we propose a novel dense depth recovery method for a trinocular video sequence. Specifically, we contribute a novel trinocular stereo matching model, which can effectively utilize the advantages of trinocular stereo images, and incorporate the visibility term with segmentation prior for robust depth estimate. In order to make the recovered depth maps more accurate and temporally consistent, we propose to first classify the pixels to static and dynamic ones, and then perform spatio-temporal depth optimization for them in different ways. Especially, we propose two motion models for handling dynamic pixels. The traditional bundle optimization model and our spatio-temporal optimization model are softly combined in a probabilistic way, so that the depths of both static and dynamic pixels can be effectively refined. Our automatic depth recovery approach is evaluated using a variety of challenging trinocular video sequences.},
keywords={Cameras;Adaptive optics;Image color analysis;Optical imaging;Optimization;Stereo vision;Optical variables measurement},
doi={10.1109/CVPR.2012.6247835},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247836,
author={Song, Ran and Liu, Yonghuai and Martin, Ralph R. and Rosin, Paul L.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Saliency-guided integration of multiple scans},
year={2012},
volume={},
number={},
pages={1474-1481},
abstract={We present a novel method to integrate multiple 3D scans captured from different viewpoints. Saliency information is used to guide the integration process. The multi-scale saliency of a point is specifically designed to reflect its sensitivity to registration errors. Then scans are partitioned into salient and non-salient regions through an Markov Random Field (MRF) framework where neighbourhood consistency is incorporated to increase the robustness against potential scanning errors. We then develop different schemes to discriminatively integrate points in the two regions. For the points in salient regions which are more sensitive to registration errors, we employ the Iterative Closest Point algorithm to compensate the local registration error and find the correspondences for the integration. For the points in non-salient regions which are less sensitive to registration errors, we integrate them via an efficient and effective point-shifting scheme. A comparative study shows that the proposed method delivers improved surface integration.},
keywords={Vectors;Robustness;Iterative closest point algorithm;Noise;Kernel;Labeling;Sensitivity},
doi={10.1109/CVPR.2012.6247836},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247837,
author={Zhou, Zihan and Jin, Hailin and Ma, Yi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust plane-based structure from motion},
year={2012},
volume={},
number={},
pages={1482-1489},
abstract={We introduce a new approach to structure and motion recovery directly from one or more large planes in the scene. When such a plane exists, we demonstrate how to automatically detect and track it robustly and consistently over a long video sequence, and how to efficiently self-calibrate the camera using the homographies induced by this plane. We build a complete structure from motion system which does not use any additional off-the-plane information about the scene, and show its advantage over conventional systems in handling two important issues which often occur in real world videos, namely, the plane degeneracy and the dynamic foreground problems. Experimental results on a variety of real video sequences verify the effectiveness and efficiency of our system.},
keywords={Trajectory;Cameras;Robustness;Image reconstruction;Video sequences;Tin;Dynamics},
doi={10.1109/CVPR.2012.6247837},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247838,
author={Luo, Linjie and Li, Hao and Paris, Sylvain and Weise, Thibaut and Pauly, Mark and Rusinkiewicz, Szymon},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-view hair capture using orientation fields},
year={2012},
volume={},
number={},
pages={1490-1497},
abstract={Reconstructing realistic 3D hair geometry is challenging due to omnipresent occlusions, complex discontinuities and specular appearance. To address these challenges, we propose a multi-view hair reconstruction algorithm based on orientation fields with structure-aware aggregation. Our key insight is that while hair's color appearance is view-dependent, the response to oriented filters that captures the local hair orientation is more stable. We apply the structure-aware aggregation to the MRF matching energy to enforce the structural continuities implied from the local hair orientations. Multiple depth maps from the MRF optimization are then fused into a globally consistent hair geometry with a template refinement procedure. Compared to the state-of-the-art color-based methods, our method faithfully reconstructs detailed hair structures. We demonstrate the results for a number of hair styles, ranging from straight to curly, and show that our framework is suitable for capturing hair in motion.},
keywords={Hair;Geometry;Image reconstruction;Cameras;Surface reconstruction;Stereo vision;Standards},
doi={10.1109/CVPR.2012.6247838},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247839,
author={Wu, Changchang and Agarwal, Sameer and Curless, Brian and Seitz, Steven M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Schematic surface reconstruction},
year={2012},
volume={},
number={},
pages={1498-1505},
abstract={This paper introduces a schematic representation for architectural scenes together with robust algorithms for reconstruction from sparse 3D point cloud data. The schematic models architecture as a network of transport curves, approximating a floorplan, with associated profile curves, together comprising an interconnected set of swept surfaces. The representation is extremely concise, composed of a handful of planar curves, and easily interpretable by humans. The approach also provides a principled mechanism for interpolating a dense surface, and enables filling in holes in the data, by means of a pipeline that employs a global optimization over all parameters. By incorporating a displacement map on top of the schematic surface, it is possible to recover fine details. Experiments show the ability to reconstruct extremely clean and simple models from sparse structure-from-motion point clouds of complex architectural scenes.},
keywords={Surface reconstruction;Image reconstruction;Robustness;Merging;Noise;Optimization;Shape},
doi={10.1109/CVPR.2012.6247839},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247840,
author={Bujnak, Martin and Kukelova, Zuzana and Pajdla, Tomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Making minimal solvers fast},
year={2012},
volume={},
number={},
pages={1506-1513},
abstract={In this paper we propose methods for speeding up minimal solvers based on Gröbner bases and action matrix eigenvalue computations. Almost all existing Gröbner basis solvers spend most time in the eigenvalue computation. We present two methods which speed up this phase of Gröbner basis solvers: (1) a method based on a modified FGLM algorithm for transforming Gröbner bases which results in a single-variable polynomial followed by direct calculation of its roots using Sturm-sequences and, for larger problems, (2) fast calculation of the characteristic polynomial of an action matrix, again solved using Sturm-sequences. We enhanced the FGLM method by replacing time consuming polynomial division performed in standard FGLM algorithm with efficient matrix-vector multiplication and we show how this method is related to the characteristic polynomial method. Our approaches allow computing roots only in some feasible interval and in desired precision. Proposed methods can significantly speedup many existing solvers. We demonstrate them on three important minimal computer vision problems.},
keywords={Polynomials;Eigenvalues and eigenfunctions;Standards;Vectors;Computer vision;Sparse matrices},
doi={10.1109/CVPR.2012.6247840},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247841,
author={Cohen, Andrea and Zach, Christopher and Sinha, Sudipta N. and Pollefeys, Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discovering and exploiting 3D symmetries in structure from motion},
year={2012},
volume={},
number={},
pages={1514-1521},
abstract={Many architectural scenes contain symmetric or repeated structures, which can generate erroneous image correspondences during structure from motion (Sfm) computation. Prior work has shown that the detection and removal of these incorrect matches is crucial for accurate and robust recovery of scene structure. In this paper, we point out that these incorrect matches, in fact, provide strong cues to the existence of symmetries and structural regularities in the unknown 3D structure. We make two key contributions. First, we propose a method to recover various symmetry relations in the structure using geometric and appearance cues. A set of structural constraints derived from the symmetries are imposed within a new constrained bundle adjustment formulation, where symmetry priors are also incorporated. Second, we show that the recovered symmetries enable us to choose a natural coordinate system for the 3D structure where gauge freedom in rotation is held fixed. Furthermore, based on the symmetries, 3D structure completion is also performed. Our approach significantly reduces drift through ”structural” loop closures and improves the accuracy of reconstructions in urban scenes.},
keywords={Solid modeling;Image reconstruction;Cameras;Buildings;Periodic structures;Mirrors;Feature extraction},
doi={10.1109/CVPR.2012.6247841},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247842,
author={Hassner, Tal and Mayzels, Viki and Zelnik-Manor, Lihi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On SIFTs and their scales},
year={2012},
volume={},
number={},
pages={1522-1528},
abstract={Scale invariant feature detectors often find stable scales in only a few image pixels. Consequently, methods for feature matching typically choose one of two extreme options: matching a sparse set of scale invariant features, or dense matching using arbitrary scales. In this paper we turn our attention to the overwhelming majority of pixels, those where stable scales are not found by standard techniques. We ask, is scale-selection necessary for these pixels, when dense, scale-invariant matching is required and if so, how can it be achieved? We make the following contributions: (i) We show that features computed over different scales, even in low-contrast areas, can be different; selecting a single scale, arbitrarily or otherwise, may lead to poor matches when the images have different scales. (ii) We show that representing each pixel as a set of SIFTs, extracted at multiple scales, allows for far better matches than single-scale descriptors, but at a computational price. Finally, (iii) we demonstrate that each such set may be accurately represented by a low-dimensional, linear subspace. A subspace-to-point mapping may further be used to produce a novel descriptor representation, the Scale-Less SIFT (SLS), as an alternative to single-scale descriptors. These claims are verified by quantitative and qualitative tests, demonstrating significant improvements over existing methods.},
keywords={Feature extraction;Detectors;Robustness;Estimation;Image resolution;Standards;Laplace equations},
doi={10.1109/CVPR.2012.6247842},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247843,
author={Kazik, Tim and Kneip, Laurent and Nikolic, Janosch and Pollefeys, Marc and Siegwart, Roland},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Real-time 6D stereo Visual Odometry with non-overlapping fields of view},
year={2012},
volume={},
number={},
pages={1529-1536},
abstract={In this paper, we present a framework for 6D absolute scale motion and structure estimation of a multi-camera system in challenging indoor environments. It operates in real-time and employs information from two cameras with non-overlapping fields of view. Monocular Visual Odometry supplying up-to-scale 6D motion information is carried out in each of the cameras, and the metric scale is recovered via a linear solution by imposing the known static transformation between both sensors. The redundancy in the motion estimates is finally exploited by a statistical fusion to an optimal 6D metric result. The proposed technique is robust to outliers and able to continuously deliver a reasonable measurement of the scale factor. The quality of the framework is demonstrated by a concise evaluation on indoor datasets, including a comparison to accurate ground truth data provided by an external motion tracking system.},
keywords={Cameras;Motion estimation;Calibration;Estimation;Quaternions;Robustness},
doi={10.1109/CVPR.2012.6247843},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247844,
author={Akae, Naoki and Mansur, Al and Makihara, Yasushi and Yagi, Yasushi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Video from nearly still: An application to low frame-rate gait recognition},
year={2012},
volume={},
number={},
pages={1537-1543},
abstract={In this paper, we propose a temporal super resolution approach for quasi-periodic image sequence such as human gait. The proposed method effectively combines example-based and reconstruction-based temporal super resolution approaches. A periodic image sequence is expressed as a manifold parameterized by a phase and a standard manifold is learned from multiple high frame-rate sequences in the training stage. In the test stage, an initial phase for each frame of an input low frame-rate image sequence is estimated based on the standard manifold at first, and the manifold reconstruction and the phase estimation are then iterated to generate better high frame-rate images in the energy minimization framework that ensures the fitness to both the input images and the standard manifold. The proposed method is applied to low frame-rate gait recognition and experiments with real data of 100 subjects demonstrate a significant improvement by the proposed method, particularly for quite low frame-rate videos (e.g., 1 fps).},
keywords={Manifolds;Image sequences;Standards;Strontium;Image reconstruction;Humans;Interpolation},
doi={10.1109/CVPR.2012.6247844},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247845,
author={Chen, Cheng and Odobez, Jean-Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={We are not contortionists: Coupled adaptive learning for head and body orientation estimation in surveillance video},
year={2012},
volume={},
number={},
pages={1544-1551},
abstract={In this paper, we deal with the estimation of body and head poses (i.e orientations) in surveillance videos, and we make three main contributions. First, we address this issue as a joint model adaptation problem in a semi-supervised framework. Second, we propose to leverage the adaptation on multiple information sources (external labeled datasets, weak labels provided by the motion direction, data structure manifold), and in particular, on the coupling at the output level of the head and body classifiers, accounting for the restriction in the configurations that the head and body pose can jointly take. Third, we propose a kernel-formulation of this principle that can be efficiently solved using a global optimization scheme. The method is applied to body and head features computed from automatically extracted body and head location tracks. Thorough experiments on several datasets demonstrate the validity of our approach, the benefit of the coupled adaptation, and that the method performs similarly or better than a state-of-the-art algorithm.},
keywords={Head;Couplings;Estimation;Surveillance;Feature extraction;Manifolds;Kernel},
doi={10.1109/CVPR.2012.6247845},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247846,
author={Sun, Xiaoshuai and Yao, Hongxun and Ji, Rongrong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={What are we looking for: Towards statistical modeling of saccadic eye movements and visual saliency},
year={2012},
volume={},
number={},
pages={1552-1559},
abstract={In this paper, we present a unified statistical framework for modeling both saccadic eye movements and visual saliency. By analyzing the statistical properties of human eye fixations on natural images, we found that human attention is sparsely distributed and usually deployed to locations with abundant structural information. This new observations inspired us to model saccadic behavior and visual saliency based on Super Gaussian Component (SGC) analysis. The model sequentially obtains SGC using projection pursuit, and generates eye-movements by selecting the location with maximum SGC response. Beside human saccadic behavior simulation, we also demonstrated our superior effectiveness and robustness over state-of-the-arts by carrying out dense experiments on psychological patterns and human eye fixation benchmarks. These results also show promising potentials of statistical approaches for human behavior research.},
keywords={Humans;Visualization;Mathematical model;Statistical analysis;Equations;Computational modeling;Vectors},
doi={10.1109/CVPR.2012.6247846},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247847,
author={Loy, Chen Change and Hospedales, Timothy M. and Xiang, Tao and Gong, Shaogang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Stream-based joint exploration-exploitation active learning},
year={2012},
volume={},
number={},
pages={1560-1567},
abstract={Learning from streams of evolving and unbounded data is an important problem, for example in visual surveillance or internet scale data. For such large and evolving real-world data, exhaustive supervision is impractical, particularly so when the full space of classes is not known in advance therefore joint class discovery (exploration) and boundary learning (exploitation) becomes critical. Active learning has shown promise in jointly optimising exploration-exploitation with minimal human supervision. However, existing active learning methods either rely on heuristic multi-criteria weighting or are limited to batch processing. In this paper, we present a new unified framework for joint exploration-exploitation active learning in streams without any heuristic weighting. Extensive evaluation on classification of various image and surveillance video datasets demonstrates the superiority of our framework over existing methods.},
keywords={Labeling;Joints;Streaming media;Humans;Uncertainty;Bayesian methods;Sun},
doi={10.1109/CVPR.2012.6247847},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247848,
author={He, Jun and Balzano, Laura and Szlam, Arthur},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Incremental gradient on the Grassmannian for online foreground and background separation in subsampled video},
year={2012},
volume={},
number={},
pages={1568-1575},
abstract={It has recently been shown that only a small number of samples from a low-rank matrix are necessary to reconstruct the entire matrix. We bring this to bear on computer vision problems that utilize low-dimensional subspaces, demonstrating that subsampling can improve computation speed while still allowing for accurate subspace learning. We present GRASTA, Grassmannian Robust Adaptive Subspace Tracking Algorithm, an online algorithm for robust subspace estimation from randomly subsampled data. We consider the specific application of background and foreground separation in video, and we assess GRASTA on separation accuracy and computation time. In one benchmark video example [16], GRASTA achieves a separation rate of 46.3 frames per second, even when run in MATLAB on a personal laptop.},
keywords={Vectors;Robustness;Streaming media;Equations;Heuristic algorithms;Lighting;Real time systems},
doi={10.1109/CVPR.2012.6247848},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247849,
author={Olsson, Carl and Boykov, Yuri},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Curvature-based regularization for surface approximation},
year={2012},
volume={},
number={},
pages={1576-1583},
abstract={We propose an energy-based framework for approximating surfaces from a cloud of point measurements corrupted by noise and outliers. Our energy assigns a tangent plane to each (noisy) data point by minimizing the squared distances to the points and the irregularity of the surface implicitly defined by the tangent planes. In order to avoid the well-known ”shrinking” bias associated with first-order surface regularization, we choose a robust smoothing term that approximates curvature of the underlying surface. In contrast to a number of recent publications estimating curvature using discrete (e.g. binary) labellings with triple-cliques we use higher-dimensional labels that allows modeling curvature with only pair-wise interactions. Hence, many standard optimization algorithms (e.g. message passing, graph cut, etc) can minimize the proposed curvature-based regularization functional. The accuracy of our approach for representing curvature is demonstrated by theoretical and empirical results on synthetic and real data sets from multiview reconstruction and stereo.},
keywords={Approximation methods;Noise measurement;Noise;Estimation;Optimization;Shape;Graphical models},
doi={10.1109/CVPR.2012.6247849},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247850,
author={Strelow, Dennis},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={General and nested Wiberg minimization},
year={2012},
volume={},
number={},
pages={1584-1591},
abstract={Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U, linearizing V (U) about U, and iteratively minimizing ∥Y-UV (U)∥with respect to U only. This approach factors the matrix while effectively removing V from the minimization. Recently Eriksson and van den Hengel extended this approach to L1, minimizing ∥Y-UV (U)∥1. We generalize their approach beyond factorization to minimize an arbitrary function that is nonlinear in each of two sets of variables. We demonstrate the idea with a practical Wiberg algorithm for L1 bundle adjustment. We also show that one Wiberg minimization can be nested inside another, effectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L1 projective bundle adjustment, solving for camera matrices, points, and projective depths. We also revisit L1 factorization, giving a greatly simplified presentation of Wiberg L1 factorization, and presenting a successive linear programming factorization algorithm. Successive linear programming outperforms L1 Wiberg for most large inputs, establishing a new state-of-the-art for for those cases.},
keywords={Minimization;Linear programming;Convergence;Cameras;Algorithm design and analysis;Maximum likelihood estimation;Programming},
doi={10.1109/CVPR.2012.6247850},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247851,
author={Liu, Haifeng and Yang, Zheng and Wu, Zhaohui and Li, Xuelong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A-Optimal Non-negative Projection for image representation},
year={2012},
volume={},
number={},
pages={1592-1599},
abstract={As a central problem in computer vision and pattern recognition, data representation has attracted great attention in the past years. Non-negative matrix factorization (NMF) which is a useful data representation method makes great contribution on finding the latent structure of the data and leads to a parts-based representation by decomposing the data matrix into a few bases and encodings with nonnegative constraints. However, non-negative constraint is insufficient for getting more robust data representation. In this paper, we propose a novel method, called A-Optimal Non-negative Projection (ANP) for image data representation and further analysis. ANP imposes a constraint on the encoding factor as a regularizer during matrix factorization. In this way, the learned data representation leads to a stable linear model no matter what kind of data label is selected for further processing. Thus, it can preserve more intrinsic characteristics of the data regardless of any specific labels. We demonstrate the effectiveness of this novel algorithm through a set of evaluations on real world applications.},
keywords={Matrix decomposition;Optimization;Vectors;Encoding;Principal component analysis;Covariance matrix;Databases},
doi={10.1109/CVPR.2012.6247851},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247852,
author={Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, René},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={See all by looking at a few: Sparse modeling for finding representative objects},
year={2012},
volume={},
number={},
pages={1600-1607},
abstract={We consider the problem of finding a few representatives for a dataset, i.e., a subset of data points that efficiently describes the entire dataset. We assume that each data point can be expressed as a linear combination of the representatives and formulate the problem of finding the representatives as a sparse multiple measurement vector problem. In our formulation, both the dictionary and the measurements are given by the data matrix, and the unknown sparse codes select the representatives via convex optimization. In general, we do not assume that the data are low-rank or distributed around cluster centers. When the data do come from a collection of low-rank models, we show that our method automatically selects a few representatives from each low-rank model. We also analyze the geometry of the representatives and discuss their relationship to the vertices of the convex hull of the data. We show that our framework can be extended to detect and reject outliers in datasets, and to efficiently deal with new observations and large datasets. The proposed framework and theoretical foundations are illustrated with examples in video summarization and image classification using representatives.},
keywords={Dictionaries;Optimization;Sparse matrices;Clustering algorithms;Distributed databases;Data models;Geometry},
doi={10.1109/CVPR.2012.6247852},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247853,
author={Heller, Jan and Havlena, Michal and Pajdla, Tomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A branch-and-bound algorithm for globally optimal hand-eye calibration},
year={2012},
volume={},
number={},
pages={1608-1615},
abstract={This paper introduces a novel solution to hand-eye calibration problem. It is the first method that uses camera measurements directly and at the same time requires neither prior knowledge of the external camera calibrations nor a known calibration device. Our algorithm uses branch-and-bound approach to minimize an objective function based on the epipolar constraint. Further, it employs Linear Programming to decide the bounding step of the algorithm. The presented technique is able to recover both the unknown rotation and translation simultaneously and the solution is guaranteed to be globally optimal with respect to the L∞-norm.},
keywords={Cameras;Vectors;Calibration;Robot vision systems;Apertures;Grippers},
doi={10.1109/CVPR.2012.6247853},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247854,
author={Sun, Min and Telaprolu, Murali and Lee, Honglak and Savarese, Silvio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={An efficient branch-and-bound algorithm for optimal human pose estimation},
year={2012},
volume={},
number={},
pages={1616-1623},
abstract={Human pose estimation in a static image is a challenging problem in computer vision in that body part configurations are often subject to severe deformations and occlusions. Moreover, efficient pose estimation is often a desirable requirement in many applications. The trade-off between accuracy and efficiency has been explored in a large number of approaches. On the one hand, models with simple representations (like tree or star models) can be efficiently applied in pose estimation problems. However, these models are often prone to body part misclassification errors. On the other hand, models with rich representations (i.e., loopy graphical models) are theoretically more robust, but their inference complexity may increase dramatically. In this work, we propose an efficient and exact inference algorithm based on branch-and-bound to solve the human pose estimation problem on loopy graphical models. We show that our method is empirically much faster (about 74 times) than the state-of-the-art exact inference algorithm [21]. By extending a state-of-the-art tree model [16] to a loopy graphical model, we show that the estimation accuracy improves for most of the body parts (especially lower arms) on popular datasets such as Buffy [7] and Stickmen [5] datasets. Finally, our method can be used to exactly solve most of the inference problems on Stretchable Models [18] (which contains a few hundreds of variables) in just a few minutes.},
keywords={Gold;Nickel;Indexes;Torso},
doi={10.1109/CVPR.2012.6247854},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247855,
author={Balzer, Jonathan and Mörwald, Thomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Isogeometric finite-elements methods and variational reconstruction tasks in vision — A perfect match},
year={2012},
volume={},
number={},
pages={1624-1631},
abstract={Inverse problems are abundant in vision. A common way to deal with their inherent ill-posedness is reformulating them within the framework of the calculus of variations. This always leads to partial differential equations as conditions of (local) optimality. In this paper, we propose solving such equations numerically by isogeometric analysis, a special kind of finite-elements method. We will expose its main advantages including superior computational performance, a natural ability to facilitate multi-scale reconstruction, and a high degree of compatibility with the spline geometries encountered in modern computer-aided design systems. To animate these fairly general arguments, their impact on the well-known depth-from-gradients problem is discussed, which amounts to solving a Poisson equation on the image plane. Experiments suggest that, by the isogeometry principle, reconstructions of unprecedented quality can be obtained without any prefiltering of the data.},
keywords={Splines (mathematics);Image reconstruction;Vectors;Solid modeling;Finite element methods;Shape;Surface reconstruction},
doi={10.1109/CVPR.2012.6247855},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247856,
author={Vineet, Vibhav and Warrell, Jonathan and Torr, Philip H.S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A tiered move-making algorithm for general pairwise MRFs},
year={2012},
volume={},
number={},
pages={1632-1639},
abstract={A large number of problems in computer vision can be modeled as energy minimization problems in a markov random field (MRF) framework. Many methods have been developed over the years for efficient inference, especially in pairwise MRFs. In general there is a trade-off between the complexity/efficiency of the algorithm and its convergence properties, with certain problems requiring more complex inference to handle general pairwise potentials. Graphcuts based α-expansion performs well on certain classes of energies, and sequential tree reweighted message passing (TRWS) and loopy belief propagation (LBP) can be used for non-submodular cases. These methods though suffer from poor convergence and often oscillate between solutions. In this paper, we propose a tiered move making algorithm which is an iterative method. Each move to the next configuration is based on the current labeling and an optimal tiered move, where each tiered move requires one application of the dynamic programming based tiered labeling method introduced in Felzenszwalb et. al. [2]. The algorithm converges to a local minimum for any general pairwise potential, and we give a theoretical analysis of the properties of the algorithm, characterizing the situations in which we can expect good performance. We evaluate the algorithm on many benchmark labeling problems such as stereo, image segmentation, image stitching and image denoising, as well as random energy minimization. Our method consistently gets better energy values than α-expansion, LBP, quadratic pseudo-boolean optimization (QPBO), and is competitive with TRWS.},
keywords={Labeling;Algorithm design and analysis;Complexity theory;Heuristic algorithms;Radio frequency;Belief propagation;Stereo vision},
doi={10.1109/CVPR.2012.6247856},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247857,
author={Riemenschneider, Hayko and Krispel, Ulrich and Thaller, Wolfgang and Donoser, Michael and Havemann, Sven and Fellner, Dieter and Bischof, Horst},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Irregular lattices for complex shape grammar facade parsing},
year={2012},
volume={},
number={},
pages={1640-1647},
abstract={High-quality urban reconstruction requires more than multi-view reconstruction and local optimization. The structure of facades depends on the general layout, which has to be optimized globally. Shape grammars are an established method to express hierarchical spatial relationships, and are therefore suited as representing constraints for semantic facade interpretation. Usually inference uses numerical approximations, or hard-coded grammar schemes. Existing methods inspired by classical grammar parsing are not applicable on real-world images due to their prohibitively high complexity. This work provides feasible generic facade reconstruction by combining low-level classifiers with mid-level object detectors to infer an irregular lattice. The irregular lattice preserves the logical structure of the facade while reducing the search space to a manageable size. We introduce a novel method for handling symmetry and repetition within the generic grammar. We show competitive results on two datasets, namely the Paris 2010 and the Graz 50. The former includes only Hausmannian, while the latter includes Classicism, Biedermeier, Historicism, Art Nouveau and post-modern architectural styles.},
keywords={Grammar;Shape;Lattices;Semantics;Tiles;Cost function;Layout},
doi={10.1109/CVPR.2012.6247857},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247858,
author={Lu, Xiaoqiang and Yuan, Haoliang and Yan, Pingkun and Yuan, Yuan and Li, Xuelong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Geometry constrained sparse coding for single image super-resolution},
year={2012},
volume={},
number={},
pages={1648-1655},
abstract={The choice of the over-complete dictionary that sparsely represents data is of prime importance for sparse coding-based image super-resolution. Sparse coding is a typical unsupervised learning method to generate an over-complete dictionary. However, most of the sparse coding methods for image super-resolution fail to simultaneously consider the geometrical structure of the dictionary and corresponding coefficients, which may result in noticeable super-resolution reconstruction artifacts. In this paper, a novel sparse coding method is proposed to preserve the geometrical structure of the dictionary and the sparse coefficients of the data. Moreover, the proposed method can preserve the incoherence of dictionary entries, which is critical for sparse representation. Inspired by the development on non-local self-similarity and manifold learning, the proposed sparse coding method can provide the sparse coefficients and learned dictionary from a new perspective, which have both reconstruction and discrimination properties to enhance the learning performance. Extensive experimental results on image super-resolution have demonstrated the effectiveness of the proposed method.},
keywords={Dictionaries;Encoding;Strontium;Sparse matrices;Image coding;Image resolution;Vectors},
doi={10.1109/CVPR.2012.6247858},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247859,
author={Collins, Maxwell D. and Xu, Jia and Grady, Leo and Singh, Vikas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Random walks based multi-image segmentation: Quasiconvexity results and GPU-based solutions},
year={2012},
volume={},
number={},
pages={1656-1663},
abstract={We recast the Cosegmentation problem using Random Walker (RW) segmentation as the core segmentation algorithm, rather than the traditional MRF approach adopted in the literature so far. Our formulation is similar to previous approaches in the sense that it also permits Cosegmentation constraints (which impose consistency between the extracted objects from ≥ 2 images) using a nonparametric model. However, several previous nonparametric cosegmentation methods have the serious limitation that they require adding one auxiliary node (or variable) for every pair of pixels that are similar (which effectively limits such methods to describing only those objects that have high entropy appearance models). In contrast, our proposed model completely eliminates this restrictive dependence - the resulting improvements are quite significant. Our model further allows an optimization scheme exploiting quasiconvexity for model-based segmentation with no dependence on the scale of the segmented foreground. Finally, we show that the optimization can be expressed in terms of linear algebra operations on sparse matrices which are easily mapped to GPU architecture. We provide a highly specialized CUDA library for Cosegmentation exploiting this special structure, and report experimental results showing these advantages.},
keywords={Image segmentation;Histograms;Graphics processing unit;Optimization;Face;Sparse matrices;Computational modeling},
doi={10.1109/CVPR.2012.6247859},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247860,
author={Zach, Christopher and Häne, Christian and Pollefeys, Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={What is optimized in tight convex relaxations for multi-label problems?},
year={2012},
volume={},
number={},
pages={1664-1671},
abstract={In this work we present a unified view on Markov random fields and recently proposed continuous tight convex relaxations for multi-label assignment in the image plane. These relaxations are far less biased towards the grid geometry than Markov random fields. It turns out that the continuous methods are non-linear extensions of the local polytope MRF relaxation. In view of this result a better understanding of these tight convex relaxations in the discrete setting is obtained. Further, a wider range of optimization methods is now applicable to find a minimizer of the tight formulation. We propose two methods to improve the efficiency of minimization. One uses a weaker, but more efficient continuously inspired approach as initialization and gradually refines the energy where it is necessary. The other one reformulates the dual energy enabling smooth approximations to be used for efficient optimization. We demonstrate the utility of our proposed minimization schemes in numerical experiments.},
keywords={Standards;Markov processes;Optimization methods;Minimization;Labeling;Materials},
doi={10.1109/CVPR.2012.6247860},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247861,
author={Dai, Zhijun and Zhang, Fengjun and Wang, Hongan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust Maximum Likelihood estimation by sparse bundle adjustment using the L1 norm},
year={2012},
volume={},
number={},
pages={1672-1679},
abstract={Sparse bundle adjustment is widely used in many computer vision applications. In this paper, we propose a method for performing bundle adjustments using the L1 norm. After linearizing the mapping function in bundle adjustment on its first order, the kernel step is to compute the L1 norm equations. Considering the sparsity of the Jacobian matrix in linearizing, we find two practical methods to solve the L1 norm equations. The first one is an interior-point method, which transfer the original problem to a problem of solving a sequence of L2 norm equations, and the second one is a decomposition method which uses the differentiability of linear programs and represents the optimal updating of parameters of 3D points by the updating variables of camera parameters. The experiments show that the method performs better for both synthetically generated and real data sets in the presence of outliers or Laplacian noise compared with the L2 norm bundle adjustment, and the method is efficient among the state of the art L1 minimization methods.},
keywords={Cameras;Noise;Equations;Mathematical model;Laplace equations;Minimization;Sparse matrices},
doi={10.1109/CVPR.2012.6247861},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247862,
author={Saito, Masaki and Okatani, Takayuki and Deguchi, Koichiro},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Application of the mean field methods to MRF optimization in computer vision},
year={2012},
volume={},
number={},
pages={1680-1687},
abstract={The mean field (MF) methods are an energy optimization method for Markov random fields (MRFs). These methods, which have their root in solid state physics, estimate the marginal density of each site of an MRF graph by iterative computation, similarly to loopy belief propagation (LBP). It appears that, being shadowed by LBP, the MF methods have not been seriously considered in the computer vision community. This study investigates whether these methods are useful for practical problems, particularly MPM (Maximum Posterior Marginal) inference, in computer vision. To be specific, we apply the naive MF equations and the TAP (Thouless-Anderson-Palmer) equations to interactive segmentation and stereo matching. In this paper, firstly, we show implementation of these methods for computer vision problems. Next, we discuss advantages of the MF methods to LBP. Finally, we present experimental results that the MF methods are well comparable to LBP in terms of accuracy and global convergence; furthermore, the 3rd-order TAP equation often outperforms LBP in terms of accuracy.},
keywords={Equations;Mathematical model;Computer vision;Accuracy;Estimation;Convergence;Computational complexity},
doi={10.1109/CVPR.2012.6247862},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247863,
author={Kappes, Jörg Hendrik and Savchynskyy, Bogdan and Schnörr, Christoph},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A bundle approach to efficient MAP-inference by Lagrangian relaxation},
year={2012},
volume={},
number={},
pages={1688-1695},
abstract={Approximate inference by decomposition of discrete graphical models and Lagrangian relaxation has become a key technique in computer vision. The resulting dual objective function is convenient from the optimization point-of-view, in principle. Due to its inherent non-smoothness, however, it is not directly amenable to efficient convex optimization. Related work either weakens the relaxation by smoothing or applies variations of the inefficient projected subgradient methods. In either case, heuristic choices of tuning parameters influence the performance and significantly depend on the specific problem at hand. In this paper, we introduce a novel approach based on bundle methods from the field of combinatorial optimization. It is directly based on the non-smooth dual objective function, requires no tuning parameters and showed a markedly improved efficiency uniformly over a large variety of problem instances including benchmark experiments. Our code will be publicly available after publication of this paper.},
keywords={Convergence;Upper bound;Tuning;Computer vision;Optimization;Benchmark testing;Standards},
doi={10.1109/CVPR.2012.6247863},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247864,
author={Zheng, Yun and Chen, Pei and Cao, Jiang-Zhong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={MAP-MRF inference based on extended junction tree representation},
year={2012},
volume={},
number={},
pages={1696-1703},
abstract={Maximum a-posteriori (MAP) inference in Markov random fields (MRF) is an important topic in machine learning, computer vision and other fields. Message passing algorithms based on linear programming (LP) relaxation are powerful tools for the MAP-MRF problems. However, current message passing algorithms are usually based on simple subgraphs, resulting in slow convergence, local optimum and untightness of the LP relaxation for many problems. By extending the junction tree representation, we propose a general convergent message passing algorithm, which can work on arbitrary tractable bounded treewidth subgraphs. In the extended junction tree representation, the minimization and summation operators are commutable so that the proposed algorithm based on the extended junction tree is guaranteed to converge. Based on the treewidth-2 decomposition, better performance of the proposed algorithm is demonstrated on stereo matching, optical flow and panorama.},
keywords={Junctions;Message passing;Particle separators;Approximation algorithms;Algorithm design and analysis;Mercury (metals);Inference algorithms},
doi={10.1109/CVPR.2012.6247864},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247865,
author={Ayazoglu, Mustafa and Sznaier, Mario and Camps, Octavia I.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast algorithms for structured robust principal component analysis},
year={2012},
volume={},
number={},
pages={1704-1711},
abstract={A large number of problems arising in computer vision can be reduced to the problem of minimizing the nuclear norm of a matrix, subject to additional structural and sparsity constraints on its elements. Examples of relevant applications include, among others, robust tracking in the presence of outliers, manifold embedding, event detection, in-painting and tracklet matching across occlusion. In principle, these problems can be reduced to a convex semi-definite optimization form and solved using interior point methods. However, the poor scaling properties of these methods limit the use of this approach to relatively small sized problems. The main result of this paper shows that structured nuclear norm minimization problems can be efficiently solved by using an iterative Augmented Lagrangian Type (ALM) method that only requires performing at each iteration a combination of matrix thresholding and matrix inversion steps. As we illustrate in the paper with several examples, the proposed algorithm results in a substantial reduction of computational time and memory requirements when compared against interior-point methods, opening up the possibility of solving realistic, large sized problems.},
keywords={Helium;Robustness;Optimization;Minimization;Image restoration;Convergence;Vectors},
doi={10.1109/CVPR.2012.6247865},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247866,
author={Strekalovskiy, Evgeny and Chambolle, Antonin and Cremers, Daniel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A convex representation for the vectorial Mumford-Shah functional},
year={2012},
volume={},
number={},
pages={1712-1719},
abstract={We propose the first tractable convex formulation of the vectorial Mumford-Shah functional which allows to compute high-quality solutions independent of the initialization. To this end, we generalize recently introduced convex formulations for scalar functionals to the vector-valued scenario in such a way that discontinuities in the different color channels preferably coincide. Furthermore, we propose an efficient solution which makes the overall optimization problem as tractable as in the scalar-valued case. Numerous experimental comparisons with the naive channel-wise approach, with the well-known Ambrosio-Tortorelli approximation, and with the classical total variation confirm the advantages of the proposed relaxation for contrast-preserving and edge-enhancing regularization.},
keywords={Image color analysis;Approximation methods;TV;Couplings;Image edge detection;Color;Complexity theory},
doi={10.1109/CVPR.2012.6247866},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247867,
author={Yang, Chao and Han, Tian and Quan, Long and Tai, Chiew-Lan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Parsing façade with rank-one approximation},
year={2012},
volume={},
number={},
pages={1720-1727},
abstract={The binary split grammar is powerful to parse façade in a broad range of types, whose structure is characterized by repetitive patterns with different layouts. We notice that, as far as two labels are concerned, BSG parsing is equivalent to approximating a façade by a matrix with multiple rank-one patterns. Then, we propose an efficient algorithm to decompose an arbitrary matrix into a rank-one matrix and a residual matrix, whose magnitude is small in the sense of l0-norm. Next, we develop a block-wise partition method to parse a more general façade. Our method leverages on the recent breakthroughs in convex optimization that can effectively decompose a matrix into a low-rank and sparse matrix pair. The rank-one block-wise parsing not only leads to the detection of repetitive patterns, but also gives an accurate façade segmentation. Experiments on intensive façade data sets have demonstrated that our method outperforms the state-of-the-art techniques and benchmarks both in robustness and efficiency.},
keywords={Approximation methods;Approximation algorithms;Sparse matrices;Robustness;Matrix decomposition;Shape;Grammar},
doi={10.1109/CVPR.2012.6247867},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247868,
author={Bai, Junjie and Song, Qi and Veksler, Olga and Wu, Xiaodong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast dynamic programming for labeling problems with ordering constraints},
year={2012},
volume={},
number={},
pages={1728-1735},
abstract={Many computer vision applications can be formulated as labeling problems. However, multilabeling problems are usually very challenging to solve, especially when some ordering constraints are enforced. We solve in this paper a five-parts labeling problem proposed in [6, 7]. In this model, one wants to find an optimal labeling for an image with five possible parts: “left”, “right”, “top”, “bottom” and “center”. The geometric ordering constraints can be read naturally from the names. No previous method can solve the problem with globally optimal solutions in a linear space complexity. We propose an efficient dynamic programming based algorithm which guarantees the global optimal labeling for the five-parts model. The time complexity is O(N1.5) and the space complexity is O(N), with N being the number of pixels in the image. In practice, it runs faster than previous methods. Moreover, it works for both 4-neighborhood and 8-neighborhood settings, and can be easily parallelized for GPU.},
keywords={Labeling;Heuristic algorithms;Complexity theory;Green products;Optimization;Memory management;Dynamic programming},
doi={10.1109/CVPR.2012.6247868},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247869,
author={Mobahi, Hossein and Zitnick, C. Lawrence and Ma, Yi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Seeing through the blur},
year={2012},
volume={},
number={},
pages={1736-1743},
abstract={This paper addresses the problem of image alignment using direct intensity-based methods for affine and homography transformations. Direct methods often employ scale-space smoothing (Gaussian blur) of the images to avoid local minima. Although, it is known that the isotropic blur used is not optimal for some motion models, the correct blur kernels have not been rigorously derived for motion models beyond translations. In this work, we derive blur kernels that result from smoothing the alignment objective function for some common motion models such as affine and homography. We show the derived kernels remove poor local minima and reach lower energy solutions in practice.},
keywords={Kernel;Smoothing methods;Optimization;Convolution;Transforms;Visualization;Computer vision},
doi={10.1109/CVPR.2012.6247869},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247870,
author={Collins, Robert T.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multitarget data association with higher-order motion models},
year={2012},
volume={},
number={},
pages={1744-1751},
abstract={We present an iterative approximate solution to the multidimensional assignment problem under general cost functions. The method maintains a feasible solution at every step, and is guaranteed to converge. It is similar to the iterated conditional modes (ICM) algorithm, but applied at each step to a block of variables representing correspondences between two adjacent frames, with the optimal conditional mode being calculated exactly as the solution to a two-frame linear assignment problem. Experiments with ground-truthed trajectory data show that the method outperforms both network-flow data association and greedy recursive filtering using a constant velocity motion model.},
keywords={Trajectory;Cost function;Target tracking;Indexes;Approximation algorithms;Kinematics},
doi={10.1109/CVPR.2012.6247870},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247871,
author={Portz, Travis and Zhang, Li and Jiang, Hongrui},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Optical flow in the presence of spatially-varying motion blur},
year={2012},
volume={},
number={},
pages={1752-1759},
abstract={This paper extends the classical warping-based optical flow method to achieve accurate flow in the presence of spatially-varying motion blur. Our idea is to parameterize the appearance of each frame as a function of both the pixel motion and the motion-induced blur. We search for the flows that best match two consecutive frames, which amounts to finding the derivative of a blurred frame with respect to both the motion and the blur, where the blur itself is a function of the motion. We propose an efficient technique to calculate the derivatives using prefiltering. Our technique avoids performing spatially-varying filtering (which can be computationally expensive) during the optimization iterations. In the end, our derivative calculation technique can be easily incorporated with classical flow code to handle video with non-uniform motion blur with little performance penalty. Our method is evaluated on both synthetic and real videos and outperforms conventional flow methods in the presence of motion blur.},
keywords={Kernel;Optical imaging;Adaptive optics;Piecewise linear approximation;Measurement;Optical filters;Approximation methods},
doi={10.1109/CVPR.2012.6247871},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247872,
author={Li, Xi and Shen, Chunhua and Shi, Qinfeng and Dick, Anthony and van den Hengel, Anton},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Non-sparse linear representations for visual tracking with online reservoir metric learning},
year={2012},
volume={},
number={},
pages={1760-1767},
abstract={Most sparse linear representation-based trackers need to solve a computationally expensive li-regularized optimization problem. To address this problem, we propose a visual tracker based on non-sparse linear representations, which admit an efficient closed-form solution without sacrificing accuracy. Moreover, in order to capture the correlation information between different feature dimensions, we learn a Mahalanobis distance metric in an online fashion and incorporate the learned metric into the optimization problem for obtaining the linear representation. We show that online metric learning using proximity comparison significantly improves the robustness of the tracking, especially on those sequences exhibiting drastic appearance changes. Furthermore, in order to prevent the unbounded growth in the number of training samples for the metric learning, we design a time-weighted reservoir sampling method to maintain and update limited-sized foreground and background sample buffers for balancing sample diversity and adaptability. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.},
keywords={Measurement;Visualization;Reservoirs;Optimization;Symmetric matrices;Correlation;Robustness},
doi={10.1109/CVPR.2012.6247872},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247873,
author={Sun, Deqing and Sudderth, Erik B. and Black, Michael J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Layered segmentation and optical flow estimation over time},
year={2012},
volume={},
number={},
pages={1768-1775},
abstract={Layered models provide a compelling approach for estimating image motion and segmenting moving scenes. Previous methods, however, have failed to capture the structure of complex scenes, provide precise object boundaries, effectively estimate the number of layers in a scene, or robustly determine the depth order of the layers. Furthermore, previous methods have focused on optical flow between pairs of frames rather than longer sequences. We show that image sequences with more frames are needed to resolve ambiguities in depth ordering at occlusion boundaries; temporal layer constancy makes this feasible. Our generative model of image sequences is rich but difficult to optimize with traditional gradient descent methods. We propose a novel discrete approximation of the continuous objective in terms of a sequence of depth-ordered MRFs and extend graph-cut optimization methods with new “moves” that make joint layer segmentation and motion estimation feasible. Our optimizer, which mixes discrete and continuous optimization, automatically determines the number of layers and reasons about their depth ordering. We demonstrate the value of layered models, our optimization strategy, and the use of more than two frames on both the Middlebury optical flow benchmark and the MIT layer segmentation benchmark.},
keywords={Motion segmentation;Optimization;Image segmentation;Optical imaging;Estimation;Adaptive optics;Robustness},
doi={10.1109/CVPR.2012.6247873},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247874,
author={Chen, Zhuoyuan and Wang, Jiang and Wu, Ying},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Decomposing and regularizing sparse/non-sparse components for motion field estimation},
year={2012},
volume={},
number={},
pages={1776-1783},
abstract={Regularizing motion field is critical to achieve accurate estimation of the motion field. As the motion field may include discontinuity (e.g., at the motion boundaries), traditional smoothness regularization may not work well. Among many approaches to handling motion discontinuity, recent attempts pursued a sparse representation of the motion field for regularization, and achieved quite encouraging results. However, statistics show that these methods tend to over-sparsify the motion field, and thus confronted by the non-sparse noise in practice. In this paper, we propose to decompose the motion field into sparse and non-sparse components for the motion boundaries and small universal noises, respectively. This separation approach regularizes these two sources differently. We propose a novel and efficient optimization algorithm to solve this problem. In addition, our study reveals the in-depth connection between this noise separation approach and the influence function approach in robust statistics. We validate and evaluate our new approach on the Middlebury benchmark, and have achieved outstanding testing performance.},
keywords={Estimation;Robustness;Adaptive optics;Image color analysis;Optimization;Fitting;Optical imaging},
doi={10.1109/CVPR.2012.6247874},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247875,
author={Shen, Wei and Deng, Ke and Bai, Xiang and Leyvand, Tommer and Guo, Baining and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Exemplar-based human action pose correction and tagging},
year={2012},
volume={},
number={},
pages={1784-1791},
abstract={The launch of Xbox Kinect has built a very successful computer vision product and made a big impact to the gaming industry; this sheds lights onto a wide variety of potential applications related to action recognition. The accurate estimation of human poses from the depth image is universally a critical step. However, existing pose estimation systems exhibit failures when faced severe occlusion. In this paper, we propose an exemplar-based method to learn to correct the initially estimated poses. We learn an inhomogeneous systematic bias by leveraging the exemplar information within specific human action domain. Our algorithm is illustrated on both joint-based skeleton correction and tag prediction. In the experiments, significant improvement is observed over the contemporary approaches, including what is delivered by the current Kinect system.},
keywords={Joints;Estimation;Vegetation;Humans;Training;Cameras},
doi={10.1109/CVPR.2012.6247875},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247876,
author={Kemelmacher-Shlizerman, Ira and Seitz, Steven M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Collection flow},
year={2012},
volume={},
number={},
pages={1792-1799},
abstract={Computing optical flow between any pair of Internet face photos is challenging for most current state of the art flow estimation methods due to differences in illumination, pose, and geometry. We show that flow estimation can be dramatically improved by leveraging a large photo collection of the same (or similar) object. In particular, consider the case of photos of a celebrity from Google Image Search. Any two such photos may have different facial expression, lighting and face orientation. The key idea is that instead of computing flow directly between the input pair (I, J), we compute versions of the images (I', J') in which facial expressions and pose are normalized while lighting is preserved. This is achieved by iteratively projecting each photo onto an appearance subspace formed from the full photo collection. The desired flow is obtained through concatenation of flows (I → I') o (J' → J). Our approach can be used with any two-frame optical flow algorithm, and significantly boosts the performance of the algorithm by providing invariance to lighting and shape changes.},
keywords={Lighting;Optical imaging;Face;Shape;Estimation;Nonlinear optics;Internet},
doi={10.1109/CVPR.2012.6247876},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247877,
author={Ricco, Susanna and Tomasi, Carlo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Dense Lagrangian motion estimation with occlusions},
year={2012},
volume={},
number={},
pages={1800-1807},
abstract={We couple occlusion modeling and multi-frame motion estimation to compute dense, temporally extended point trajectories in video with significant occlusions. Our approach combines robust spatial regularization with spatially and temporally global occlusion labeling in a variational, Lagrangian framework with subspace constraints. We track points even through ephemeral occlusions. Experiments demonstrate accuracy superior to the state of the art while tracking more points through more frames.},
keywords={Trajectory;Tracking;Equations;Motion estimation;Robustness;Accuracy;Brightness},
doi={10.1109/CVPR.2012.6247877},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247878,
author={Wu, Yi and Shen, Bin and Ling, Haibin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Online robust image alignment via iterative convex optimization},
year={2012},
volume={},
number={},
pages={1808-1814},
abstract={In this paper we study the problem of online aligning a newly arrived image to previously well-aligned images. Inspired by recent advances in batch image alignment using low rank decomposition [16], we treat the newly arrived image, after alignment, as being linearly and sparsely reconstructed by the well-aligned ones. The task is accomplished by a sequence of convex optimization that minimizes the l\-norm. After that, online basis updating is pursued in two different ways: (1) a two-stage incremental alignment for joint registration of a large image dataset which is known a prior, and (2) a greedy online alignment of dynamically increasing image sequences, such as in the tracking scenario. In (1), we first sequentially collect basis images that are easily aligned by checking their reconstruction residuals, followed by the second stage where all images are re-aligned one-by-one using the collected basis set. In (2), during the tracking process, we dynamically enrich the image basis set by the new target if it significantly distinguishes itself from existing basis images. While inheriting the benefits of sparsity, our method enjoys the great time efficiency and therefore be capable of dealing with large image set and real time tasks such as visual tracking. The efficacy of the proposed online robust alignment algorithm is verified with extensive experiments on image set alignment and visual tracking, in reference with state-of-the-art methods.},
keywords={Target tracking;Visualization;Face;Robustness;Joints;Image reconstruction;Convex functions},
doi={10.1109/CVPR.2012.6247878},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247879,
author={Shu, Guang and Dehghan, Afshin and Oreifej, Omar and Hand, Emily and Shah, Mubarak},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Part-based multiple-person tracking with partial occlusion handling},
year={2012},
volume={},
number={},
pages={1815-1821},
abstract={Single camera-based multiple-person tracking is often hindered by difficulties such as occlusion and changes in appearance. In this paper, we address such problems by proposing a robust part-based tracking-by-detection framework. Human detection using part models has become quite popular, yet its extension in tracking has not been fully explored. Our approach learns part-based person-specific SVM classifiers which capture the articulations of the human bodies in dynamically changing appearance and background. With the part-based model, our approach is able to handle partial occlusions in both the detection and the tracking stages. In the detection stage, we select the subset of parts which maximizes the probability of detection, which significantly improves the detection performance in crowded scenes. In the tracking stage, we dynamically handle occlusions by distributing the score of the learned person classifier among its corresponding parts, which allows us to detect and predict partial occlusions, and prevent the performance of the classifiers from being degraded. Extensive experiments using the proposed method on several challenging sequences demonstrate state-of-the-art performance in multiple-people tracking.},
keywords={Humans;Target tracking;Feature extraction;Trajectory;Support vector machines;Detectors;Vectors},
doi={10.1109/CVPR.2012.6247879},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247880,
author={Jia, Xu and Lu, Huchuan and Yang, Ming-Hsuan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Visual tracking via adaptive structural local sparse appearance model},
year={2012},
volume={},
number={},
pages={1822-1829},
abstract={Sparse representation has been applied to visual tracking by finding the best candidate with minimal reconstruction error using target templates. However most sparse representation based trackers only consider the holistic representation and do not make full use of the sparse coefficients to discriminate between the target and the background, and hence may fail with more possibility when there is similar object or occlusion in the scene. In this paper we develop a simple yet robust tracking method based on the structural local sparse appearance model. This representation exploits both partial information and spatial information of the target based on a novel alignment-pooling method. The similarity obtained by pooling across the local patches helps not only locate the target more accurately but also handle occlusion. In addition, we employ a template update strategy which combines incremental subspace learning and sparse representation. This strategy adapts the template to the appearance change of the target with less possibility of drifting and reduces the influence of the occluded target template as well. Both qualitative and quantitative evaluations on challenging benchmark image sequences demonstrate that the proposed tracking algorithm performs favorably against several state-of-the-art methods.},
keywords={Target tracking;Vectors;Adaptation models;Dictionaries;Mathematical model;Visualization;Robustness},
doi={10.1109/CVPR.2012.6247880},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247881,
author={Bao, Chenglong and Wu, Yi and Ling, Haibin and Ji, Hui},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Real time robust L1 tracker using accelerated proximal gradient approach},
year={2012},
volume={},
number={},
pages={1830-1837},
abstract={Recently sparse representation has been applied to visual tracker by modeling the target appearance using a sparse approximation over a template set, which leads to the so-called L1 trackers as it needs to solve an ℓ1 norm related minimization problem for many times. While these L1 trackers showed impressive tracking accuracies, they are very computationally demanding and the speed bottleneck is the solver to ℓ1 norm minimizations. This paper aims at developing an L1 tracker that not only runs in real time but also enjoys better robustness than other L1 trackers. In our proposed L1 tracker, a new ℓ1 norm related minimization model is proposed to improve the tracking accuracy by adding an ℓ1 norm regularization on the coefficients associated with the trivial templates. Moreover, based on the accelerated proximal gradient approach, a very fast numerical solver is developed to solve the resulting ℓ1 norm related minimization problem with guaranteed quadratic convergence. The great running time efficiency and tracking accuracy of the proposed tracker is validated with a comprehensive evaluation involving eight challenging sequences and five alternative state-of-the-art trackers.},
keywords={Minimization;Target tracking;Real time systems;Robustness;Accuracy;Visualization;Noise},
doi={10.1109/CVPR.2012.6247881},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247882,
author={Zhong, Wei and Lu, Huchuan and Yang, Ming-Hsuan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust object tracking via sparsity-based collaborative model},
year={2012},
volume={},
number={},
pages={1838-1845},
abstract={In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favorably against several state-of-the-art algorithms.},
keywords={Target tracking;Histograms;Vectors;Robustness;Image reconstruction;Adaptation models;Collaboration},
doi={10.1109/CVPR.2012.6247882},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247883,
author={Fragkiadaki, Katerina and Zhang, Geng and Shi, Jianbo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Video segmentation by tracing discontinuities in a trajectory embedding},
year={2012},
volume={},
number={},
pages={1846-1853},
abstract={Our goal is to segment a video sequence into moving objects and the world scene. In recent work, spectral embedding of point trajectories based on 2D motion cues accumulated from their lifespans, has shown to outperform factorization and per frame segmentation methods for video segmentation. The scale and kinematic nature of the moving objects and the background scene determine how close or far apart trajectories are placed in the spectral embedding. Such density variations may confuse clustering algorithms, causing over-fragmentation of object interiors. Therefore, instead of clustering in the spectral embedding, we propose detecting discontinuities of embedding density between spatially neighboring trajectories. Detected discontinuities are strong indicators of object boundaries and thus valuable for video segmentation. We propose a novel embedding discretization process that recovers from over-fragmentations by merging clusters according to discontinuity evidence along inter-cluster boundaries. For segmenting articulated objects, we combine motion grouping cues with a center-surround saliency operation, resulting in “context-aware”, spatially coherent, saliency maps. Figure-ground segmentation obtained from saliency thresholding, provides object connectedness constraints that alter motion based trajectory affinities, by keeping articulated parts together and separating disconnected in time objects. Finally, we introduce Gabriel graphs as effective per frame superpixel maps for converting trajectory clustering to dense image segmentation. Gabriel edges bridge large contour gaps via geometric reasoning without over-segmenting coherent image regions. We present experimental results of our method that outperform the state-of-the-art in challenging motion segmentation datasets.},
keywords={Trajectory;Motion segmentation;Detectors;Argon;Educational institutions;Image segmentation;Tin},
doi={10.1109/CVPR.2012.6247883},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247884,
author={Bai, Yancheng and Tang, Ming},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust tracking via weakly supervised ranking SVM},
year={2012},
volume={},
number={},
pages={1854-1861},
abstract={Appearance model is a key component of tracking algorithms. Most existing approaches utilize the object information contained in the current and previous frames to construct the object appearance model and locate the object with the model in frame t + 1. This method may work well if the object appearance just fluctuates in short time intervals. Nevertheless, suboptimal locations will be generated in frame t + 1 if the visual appearance changes substantially from the model. Then, continuous changes would accumulate errors and finally result in a tracking failure. To copy with this problem, in this paper we propose a novel algorithm - online Laplacian ranking support vector tracker (LRSVT) - to robustly locate the object. The LRSVT incorporates the labeled information of the object in the initial and the latest frames to resist the occlusion and adapt to the fluctuation of the visual appearance, and the weakly labeled information from frame t + 1 to adapt to substantial changes of the appearance. Extensive experiments on public benchmark sequences show the superior performance of LRSVT over some state-of-the-art tracking algorithms.},
keywords={Support vector machines;Laplace equations;Target tracking;Adaptation models;Manifolds;Robustness;Training},
doi={10.1109/CVPR.2012.6247884},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247885,
author={Oikonomidis, I. and Kyriazis, N. and Argyros, A. A.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Tracking the articulated motion of two strongly interacting hands},
year={2012},
volume={},
number={},
pages={1862-1869},
abstract={We propose a method that relies on markerless visual observations to track the full articulation of two hands that interact with each-other in a complex, unconstrained manner. We formulate this as an optimization problem whose 54-dimensional parameter space represents all possible configurations of two hands, each represented as a kinematic structure with 26 Degrees of Freedom (DoFs). To solve this problem, we employ Particle Swarm Optimization (PSO), an evolutionary, stochastic optimization method with the objective of finding the two-hands configuration that best explains observations provided by an RGB-D sensor. To the best of our knowledge, the proposed method is the first to attempt and achieve the articulated motion tracking of two strongly interacting hands. Extensive quantitative and qualitative experiments with simulated and real world image sequences demonstrate that an accurate and efficient solution of this problem is indeed feasible.},
keywords={Optimization;Joints;Humans;Skin;Visualization;Computational modeling;Tracking},
doi={10.1109/CVPR.2012.6247885},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247886,
author={Elhayek, A. and Stoll, C. and Hasler, N. and Kim, K. I. and Seidel, H.-P. and Theobalt, C.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Spatio-temporal motion tracking with unsynchronized cameras},
year={2012},
volume={},
number={},
pages={1870-1877},
abstract={We present a new spatio-temporal method for markerless motion capture. We reconstruct the pose and motion of a character from a multi-view video sequence without requiring the cameras to be synchronized and without aligning captured frames in time. By formulating the model-to-image similarity measure as a temporally continuous functional, we are also able to reconstruct motion in much higher temporal detail than was possible with previous synchronized approaches. By purposefully running cameras unsynchronized we can capture even very fast motion at speeds that off-the-shelf but high quality cameras provide.},
keywords={Cameras;Motion segmentation;Synchronization;Tracking;Joints;Image color analysis;Streaming media},
doi={10.1109/CVPR.2012.6247886},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247887,
author={Unger, Markus and Werlberger, Manuel and Pock, Thomas and Bischof, Horst},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling},
year={2012},
volume={},
number={},
pages={1878-1885},
abstract={We propose a unified variational formulation for joint motion estimation and segmentation with explicit occlusion handling. This is done by a multi-label representation of the flow field, where each label corresponds to a parametric representation of the motion. We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints. Explicit occlusion handling eliminates errors otherwise created by the regularization. As occlusions can occur only at object boundaries, a large number of objects may be required. By using a fast primal-dual algorithm we are able to handle several hundred motion segments. Results are shown on several classical motion segmentation and optical flow examples.},
keywords={Motion segmentation;Optical imaging;Computer vision;Motion estimation;Vectors;Joints;Optical sensors},
doi={10.1109/CVPR.2012.6247887},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247888,
author={Bazzani, Loris and Cristani, Marco and Murino, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Decentralized particle filter for joint individual-group tracking},
year={2012},
volume={},
number={},
pages={1886-1893},
abstract={In this paper, we address the task of tracking groups of people in surveillance scenarios. This is a major challenge in computer vision, since groups are structured entities, subjected to repeated split and merge events. Our solution is a joint individual-group tracking framework, inspired by a recent technique dubbed decentralized particle filtering. The proposed strategy factorizes the joint individual-group state space in two dependent subspaces where individuals and groups share the knowledge of the joint individual-group distribution. In practice, we establish a tight relation of mutual support between the modeling of individuals and that of groups, promoting the idea that groups are better tracked if individuals are considered, and viceversa. Extensive experiments on a published and novel dataset validate our intuition, opening up to many future developments.},
keywords={Joints;Proposals;Target tracking;Standards;Monte Carlo methods;Approximation methods;Probability distribution},
doi={10.1109/CVPR.2012.6247888},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247889,
author={Hare, Sam and Saffari, Amir and Torr, Philip H. S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient online structured output learning for keypoint-based object tracking},
year={2012},
volume={},
number={},
pages={1894-1901},
abstract={Efficient keypoint-based object detection methods are used in many real-time computer vision applications. These approaches often model an object as a collection of keypoints and associated descriptors, and detection then involves first constructing a set of correspondences between object and image keypoints via descriptor matching, and subsequently using these correspondences as input to a robust geometric estimation algorithm such as RANSAC to find the transformation of the object in the image. In such approaches, the object model is generally constructed offline, and does not adapt to a given environment at runtime. Furthermore, the feature matching and transformation estimation stages are treated entirely separately. In this paper, we introduce a new approach to address these problems by combining the overall pipeline of correspondence generation and transformation estimation into a single structured output learning framework. Following the recent trend of using efficient binary descriptors for feature matching, we also introduce an approach to approximate the learned object model as a collection of binary basis functions which can be evaluated very efficiently at runtime. Experiments on challenging video sequences show that our algorithm significantly improves over state-of-the-art descriptor matching techniques using a range of descriptors, as well as recent online learning based approaches.},
keywords={Computational modeling;Object detection;Training;Adaptation models;Estimation;Approximation algorithms;Vectors},
doi={10.1109/CVPR.2012.6247889},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247890,
author={Trinh, Hoang and Fan, Quanfu and Gabbur, Prasad and Pankanti, Sharath},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Hand tracking by binary quadratic programming and its application to retail activity recognition},
year={2012},
volume={},
number={},
pages={1902-1909},
abstract={Substantial ambiguities arise in hand tracking due to issues such as small hand size, deformable hand shapes and similar hand appearances. These issues have greatly limited the capability of current multi-target tracking techniques in hand tracking. As an example, state-of-the-art approaches for people tracking handle indentity switching by exploiting the appearance cues using advanced object detectors. For hand tracking, such approaches will fail due to similar, or even identical hand appearances. The main contribution of our work is a global optimization framework based on binary quadratic programming (BQP) that seamlessly integrates appearance, motion and complex interactions between hands. Our approach effectively handles key challenges such as occlusion, detection failure, identity switching, and robustly tracks both hands in two challenging real-life scenarios: retail surveillance and sign languages. In addition, we demonstrate that an automatic method based on hand trajectory analysis outperforms state-of-the-art on checkout-related activity recognition in grocery stores.},
keywords={Tracking;Mathematical model;Computational modeling;Trajectory;Equations;Switches;Image color analysis},
doi={10.1109/CVPR.2012.6247890},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247891,
author={Sevilla-Lara, Laura and Learned-Miller, Erik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Distribution fields for tracking},
year={2012},
volume={},
number={},
pages={1910-1917},
abstract={Visual tracking of general objects often relies on the assumption that gradient descent of the alignment function will reach the global optimum. A common technique to smooth the objective function is to blur the image. However, blurring the image destroys image information, which can cause the target to be lost. To address this problem we introduce a method for building an image descriptor using distribution fields (DFs), a representation that allows smoothing the objective function without destroying information about pixel values. We present experimental evidence on the superiority of the width of the basin of attraction around the global optimum of DFs over other descriptors. DFs also allow the representation of uncertainty about the tracked object. This helps in disregarding outliers during tracking (like occlusions or small misalignments) without modeling them explicitly. Finally, this provides a convenient way to aggregate the observations of the object through time and maintain an updated model. We present a simple tracking algorithm that uses DFs and obtains state-of-the-art results on standard benchmarks.},
keywords={Target tracking;Smoothing methods;Kernel;Standards;Histograms;Convolution},
doi={10.1109/CVPR.2012.6247891},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247892,
author={Yang, Bo and Nevatia, Ram},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-target tracking by online learning of non-linear motion patterns and robust appearance models},
year={2012},
volume={},
number={},
pages={1918-1925},
abstract={We describe an online approach to learn non-linear motion patterns and robust appearance models for multi-target tracking in a tracklet association framework. Unlike most previous approaches that use linear motion methods only, we online build a non-linear motion map to better explain direction changes and produce more robust motion affinities between tracklets. Moreover, based on the incremental learned entry/exit map, a multiple instance learning method is devised to produce strong appearance models for tracking; positive sample pairs are collected from different track-lets so that training samples have high diversity. Finally, using online learned moving groups, a tracklet completion process is introduced to deal with tracklets not reaching entry/exit points. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods.},
keywords={Target tracking;Estimation;Robustness;Training;Trajectory;Detectors},
doi={10.1109/CVPR.2012.6247892},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247893,
author={Andriyenko, Anton and Schindler, Konrad and Roth, Stefan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discrete-continuous optimization for multi-target tracking},
year={2012},
volume={},
number={},
pages={1926-1933},
abstract={The problem of multi-target tracking is comprised of two distinct, but tightly coupled challenges: (i) the naturally discrete problem of data association, i.e. assigning image observations to the appropriate target; (ii) the naturally continuous problem of trajectory estimation, i.e. recovering the trajectories of all targets. To go beyond simple greedy solutions for data association, recent approaches often perform multi-target tracking using discrete optimization. This has the disadvantage that trajectories need to be pre-computed or represented discretely, thus limiting accuracy. In this paper we instead formulate multi-target tracking as a discrete-continuous optimization problem that handles each aspect in its natural domain and allows leveraging powerful methods for multi-model fitting. Data association is performed using discrete optimization with label costs, yielding near optimality. Trajectory estimation is posed as a continuous fitting problem with a simple closed-form solution, which is used in turn to update the label costs. We demonstrate the accuracy and robustness of our approach with state-of-the-art performance on several standard datasets.},
keywords={Trajectory;Target tracking;Optimization;Labeling;Estimation;Splines (mathematics)},
doi={10.1109/CVPR.2012.6247893},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247894,
author={Dubuisson, Séverine and Gonzales, Christophe},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={An optimized DBN-based mode-focussing particle filter},
year={2012},
volume={},
number={},
pages={1934-1939},
abstract={We propose an original particle filtering-based approach combining optimization and decomposition techniques for sequential non-parametric density estimation defined in high-dimensional state spaces. Our method relies on Annealing to focus on the correct distributions and on probabilistic conditional independences defined by Dynamic Bayesian Networks to focus samples on their modes. After proving its theoretical correctness and showing its complexity, we highlight its ability to track single and multiple articulated objects both on synthetic and real video sequences. We show that our approach is particularly effective, both in terms of estimation errors and computation times.},
keywords={Joints;Video sequences;Torso;Estimation error;Annealing;Particle filters},
doi={10.1109/CVPR.2012.6247894},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247895,
author={Oron, Shaul and Bar-Hillel, Aharon and Levi, Dan and Avidan, Shai},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Locally Orderless Tracking},
year={2012},
volume={},
number={},
pages={1940-1947},
abstract={Locally Orderless Tracking (LOT) is a visual tracking algorithm that automatically estimates the amount of local (dis)order in the object. This lets the tracker specialize in both rigid and deformable objects on-line and with no prior assumptions. We provide a probabilistic model of the object variations over time. The model is implemented using the Earth Mover's Distance (EMD) with two parameters that control the cost of moving pixels and changing their color. We adjust these costs on-line during tracking to account for the amount of local (dis)order in the object. We show LOT's tracking capabilities on challenging video sequences, both commonly used and new, demonstrating performance comparable to state-of-the-art methods.},
keywords={Noise;Probabilistic logic;Target tracking;Maximum likelihood estimation;Visualization;Optimization},
doi={10.1109/CVPR.2012.6247895},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247896,
author={Wu, Zheng and Thangali, Ashwin and Sclaroff, Stan and Betke, Margrit},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Coupling detection and data association for multiple object tracking},
year={2012},
volume={},
number={},
pages={1948-1955},
abstract={We present a novel framework for multiple object tracking in which the problems of object detection and data association are expressed by a single objective function. The framework follows the Lagrange dual decomposition strategy, taking advantage of the often complementary nature of the two subproblems. Our coupling formulation avoids the problem of error propagation from which traditional “detection-tracking approaches” to multiple object tracking suffer. We also eschew common heuristics such as “nonmaximum suppression” of hypotheses by modeling the joint image likelihood as opposed to applying independent likelihood assumptions. Our coupling algorithm is guaranteed to converge and can handle partial or even complete occlusions. Furthermore, our method does not have any severe scalability issues but can process hundreds of frames at the same time. Our experiments involve challenging, notably distinct datasets and demonstrate that our method can achieve results comparable to those of state-of-art approaches, even without a heavily trained object detector.},
keywords={Couplings;Dictionaries;Joints;Detectors;Minimization;Object detection;Markov processes},
doi={10.1109/CVPR.2012.6247896},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247897,
author={Jiang, Nan and Liu, Wenyu and Wu, Ying},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Order determination and sparsity-regularized metric learning adaptive visual tracking},
year={2012},
volume={},
number={},
pages={1956-1963},
abstract={Recent attempts of integrating metric learning in visual tracking have produced encouraging results. Instead of using fixed and pre-specified metric in visual appearance matching, these methods are able to learn and adjust the metric adaptively by finding the best projection of the feature space. Such learned metric is by design the best to discriminate the target of interest and its distracters from the background. However, an important issue remained unaddressed is how we can determine the optimal dimensionality of the projection to achieve best discrimination. Using inappropriate dimensions for the projection is likely to result in larger classification error, or higher computational costs and over-fitting. This paper presents a novel solution to this structural order determination problem, by introducing sparsity regularization for metric learning (or SRML). This regularization leads to the lowest possible dimensionality of the projection and thus determining the best order. This can actually be viewed as the minimum description length regularization in metric learning. The experiments validate this new approach on standard benchmark datasets, and demonstrate its effectiveness in visual tracking applications.},
keywords={Target tracking;Visualization;Training;Learning systems;Computational efficiency;Extraterrestrial measurements},
doi={10.1109/CVPR.2012.6247897},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247898,
author={Park, Dong Woo and Kwon, Junseok and Lee, Kyoung Mu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust visual tracking using autoregressive hidden Markov Model},
year={2012},
volume={},
number={},
pages={1964-1971},
abstract={Recent studies on visual tracking have shown significant improvement in accuracy by handling the appearance variations of the target object. Whereas most studies present schemes to extract the time-invariant characteristics of the target and adaptively update the appearance model, the present paper concentrates on modeling the probabilistic dependency between sequential target appearances (Fig. 1-(a)). To actualize this interest, a new Bayesian tracking framework is formulated under the autoregressive Hidden Markov Model (AR-HMM), where the probabilistic dependency between sequential target appearances is implied. During the learning phase at each time step, the proposed tracker separates formerly seen target samples into several clusters based on their visual similarity, and learns cluster-specific classifiers as multiple appearance models, each of which represents a certain type of the target appearance. Then the dependency between these appearance models is learned. During the searching phase, the target state is estimated by inferring the most probable appearance model under the consideration of its dependency on formerly utilized appearance models. The proposed method is tested on 12 challenging video sequences containing targets with abrupt appearance variations, and demonstrates that it outperforms current state-of-the-art methods in accuracy.},
keywords={Hidden Markov models;Target tracking;Training;Detectors;Face;Adaptation models;Vectors},
doi={10.1109/CVPR.2012.6247898},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247899,
author={Qin, Zhen and Shelton, Christian R.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Improving multi-target tracking via social grouping},
year={2012},
volume={},
number={},
pages={1972-1978},
abstract={We address the problem of multi-person data-association-based tracking (DAT) in semi-crowded environments from a single camera. Existing tracklet-association-based methods using purely visual cues (like appearance and motion information) show impressive results but rely on heavy training, a number of tuned parameters, and sophisticated detectors to cope with visual ambiguities within the video and low-level processing errors. In this work, we consider clustering dynamics to mitigate such ambiguities. This leads to a general optimization framework that adds social grouping behavior (SGB) to any basic affinity model. We formulate this as a nonlinear global optimization problem to maximize the consistency of visual and grouping cues for trajectories in both tracklet-tracklet linking space and tracklet-grouping assignment space. We formulate the Lagrange dual and solve it using a two-stage iterative algorithm, employing the Hungarian algorithm and K-means clustering. We build SGB upon a simple affinity model and show very promising performance on two publicly available real-world datasets with different tracklet extraction methods.},
keywords={Joining processes;Target tracking;Trajectory;Visualization;Optimization;Clustering algorithms},
doi={10.1109/CVPR.2012.6247899},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247900,
author={Coviello, Emanuele and Mumtaz, Adeel and Chan, Antoni B. and Lanckriet, Gert R. G.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Growing a bag of systems tree for fast and accurate classification},
year={2012},
volume={},
number={},
pages={1979-1986},
abstract={The bag-of-systems (BoS) representation is a descriptor of motion in a video, where dynamic texture (DT) codewords represent the typical motion patterns in spatio-temporal patches extracted from the video. The efficacy of the BoS descriptor depends on the richness of the codebook, which directly depends on the number of codewords in the codebook. However, for even modest sized codebooks, mapping videos onto the codebook results in a heavy computational load. In this paper we propose the BoS Tree, which constructs a bottom-up hierarchy of codewords that enables efficient mapping of videos to the BoS codebook. By leveraging the tree structure to efficiently index the codewords, the BoS Tree allows for fast look-ups in the codebook and enables the practical use of larger, richer codebooks. We demonstrate the effectiveness of BoS Trees on classification of three video datasets, as well as on annotation of a music dataset.},
keywords={Histograms;Clustering algorithms;Indexing;Vectors;Heuristic algorithms;Quantization;Vegetation},
doi={10.1109/CVPR.2012.6247900},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247901,
author={Leal-Taixé, Laura and Pons-Moll, Gerard and Rosenhahn, Bodo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Branch-and-price global optimization for multi-view multi-target tracking},
year={2012},
volume={},
number={},
pages={1987-1994},
abstract={We present a new algorithm to jointly track multiple objects in multi-view images. While this has been typically addressed separately in the past, we tackle the problem as a single global optimization. We formulate this assignment problem as a min-cost problem by defining a graph structure that captures both temporal correlations between objects as well as spatial correlations enforced by the configuration of the cameras. This leads to a complex combinatorial optimization problem that we solve using Dantzig-Wolfe decomposition and branching. Our formulation allows us to solve the problem of reconstruction and tracking in a single step by taking all available evidence into account. In several experiments on multiple people tracking and 3D human pose tracking, we show our method outperforms state-of-the-art approaches.},
keywords={Cameras;Trajectory;Image edge detection;Optimization;Correlation;Image reconstruction;Linear programming},
doi={10.1109/CVPR.2012.6247901},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247902,
author={Wachinger, Christian and Navab, Nassir},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A contextual maximum likelihood framework for modeling image registration},
year={2012},
volume={},
number={},
pages={1995-2002},
abstract={We introduce a novel probabilistic framework for image registration. This framework considers, in contrast to previous ones, local neighborhood information. We integrate the neighborhood information into the framework by adding layers of latent random variables, characterizing the descriptive information of each image. This extension has multiple advantages. It allows for a unified description of geometric and iconic registration, with the consequential analysis of similarities. It enables to arrange registration techniques in a continuum, limited by pure intensity-and feature-based registration. With this wide spectrum of techniques combined, we can model hybrid registration approaches. The probabilistic coupling allows further to deduce optimal descriptors and to model the adaptation of description layers during the process, as it is done for joint registration/segmentation. Finally, we deduce a new registration algorithm that allows for a dynamic adaptation of the description layers during the registration. Excellent results confirm the advantages of the new registration method, the major contribution of this article lies, however, in the theoretical analysis.},
keywords={Probabilistic logic;Mathematical model;Random variables;Graphical models;Estimation;Context;Equations},
doi={10.1109/CVPR.2012.6247902},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247903,
author={Leichter, Ido and Krupka, Eyal},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Monotonicity and error type differentiability in performance measures for target detection and tracking in video},
year={2012},
volume={},
number={},
pages={2003-2009},
abstract={There exists an abundance of systems and algorithms for multiple target detection and tracking in video, and many measures for evaluating the quality of their output have been proposed. The contribution of this paper lies in the following: first, it argues that such performance measures should have two fundamental properties - monotonicity and error type differentiability; second, it shows that the recently proposed measures do not have either of these properties and are thus less usable; third, it composes a set of simple measures, partly built on common practice, that does have these properties. The informativeness of the proposed set of performance measures is demonstrated through their application on face detection and tracking results.},
keywords={Target tracking;Measurement uncertainty;Corporate acquisitions;Object detection;Accuracy;Context},
doi={10.1109/CVPR.2012.6247903},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247904,
author={Zheng, Yinqiang and Sugimoto, Shigeki and Yan, Shuicheng and Okutomi, Masatoshi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Generalizing Wiberg algorithm for rigid and nonrigid factorizations with missing components and metric constraints},
year={2012},
volume={},
number={},
pages={2010-2017},
abstract={In spite of intensive endeavor over decades, rigid and nonrigid factorizations under metric constraints, possibly in the presence of missing components, remain to be very challenging. In this work, we try to break the hard nut by generalizing to these problems the Wiberg algorithm, one of the most successful solutions for unconstrained bilinear factorization. To properly handle missing components, we advocate a bilinear factorization formulation with an extra mean vector. In spirit of the Wiberg algorithm, we first propose an efficient and initialization-insensitive algorithm for unconstrained factorization, posterior correction of whose solution offers reasonable initialization for metric upgrade. For factorization with metric constraints, we reformulate it into an unconstrained problem through quaternion parametrization, which merges elegantly into our unconstrained factorization algorithm. Extensive experiment results verify that our proposed methods are fast, accurate and robust to high percentage of missing components.},
keywords={Vectors;Measurement;Jacobian matrices;Quaternions;Least squares approximation;Cameras},
doi={10.1109/CVPR.2012.6247904},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247905,
author={Dai, Yuchao and Li, Hongdong and He, Mingyi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A simple prior-free method for non-rigid structure-from-motion factorization},
year={2012},
volume={},
number={},
pages={2018-2025},
abstract={This paper proposes a simple “prior-free” method for solving non-rigid structure-from-motion factorization problems. Other than using the basic low-rank condition, our method does not assume any extra prior knowledge about the nonrigid scene or about the camera motions. Yet, it runs reliably, produces optimal result, and does not suffer from the inherent basis-ambiguity issue which plagued many conventional nonrigid factorization techniques. Our method is easy to implement, which involves solving no more than an SDP (semi-definite programming) of small and fixed size, a linear Least-Squares or trace-norm minimization. Extensive experiments have demonstrated that it outperforms most of the existing linear methods of nonrigid factorization. This paper offers not only new theoretical insight, but also a practical, everyday solution, to non-rigid structure-from-motion.},
keywords={Shape;Cameras;Equations;Minimization;Shape measurement;Trajectory;Linear systems},
doi={10.1109/CVPR.2012.6247905},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247906,
author={Bartoli, A. and Gérard, Y. and Chadebecq, F. and Collins, T.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On template-based reconstruction from a single view: Analytical solutions and proofs of well-posedness for developable, isometric and conformal surfaces},
year={2012},
volume={},
number={},
pages={2026-2033},
abstract={Recovering a deformable surface's 3D shape from a single view registered to a 3D template requires one to provide additional constraints. A recent approach has been to constrain the surface to deform quasi-isometrically. This is applicable to surfaces of materials such as paper and cloth. Current `closed-form' solutions solve a convex approximation of the original problem whereby the surface's depth is maximized under the isometry constraints (this is known as the maximum depth heuristic). No such convex approximation has yet been proposed for the conformal case. We give a unified problem formulation as a system of PDEs for developable, isometric and conformal surfaces that we solve analytically. This has important consequences. First, it gives the first analytical algorithms to solve this type of reconstruction problems. Second, it gives the first algorithms to solve for the exact constraints. Third, it allows us to study the well-posedness of this type of reconstruction: we establish that isometric surfaces can be reconstructed unambiguously and that conformal surfaces can be reconstructed up to a few discrete ambiguities and a global scale. In the latter case, the candidate solution surfaces are obtained analytically. Experimental results on simulated and real data show that our methods generally perform as well as or outperform state of the art approaches in terms of reconstruction accuracy.},
keywords={Surface reconstruction;Equations;Image reconstruction;Shape;Three dimensional displays;Approximation methods;Numerical models},
doi={10.1109/CVPR.2012.6247906},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247907,
author={Yang, Bo and Nevatia, Ram},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={An online learned CRF model for multi-target tracking},
year={2012},
volume={},
number={},
pages={2034-2041},
abstract={We introduce an online learning approach for multitarget tracking. Detection responses are gradually associated into tracklets in multiple levels to produce final tracks. Unlike most previous approaches which only focus on producing discriminative motion and appearance models for all targets, we further consider discriminative features for distinguishing difficult pairs of targets. The tracking problem is formulated using an online learned CRF model, and is transformed into an energy minimization problem. The energy functions include a set of unary functions that are based on motion and appearance models for discriminating all targets, as well as a set of pairwise functions that are based on models for differentiating corresponding pairs of tracklets. The online CRF approach is more powerful at distinguishing spatially close targets with similar appearances, as well as in dealing with camera motions. An efficient algorithm is introduced for finding an association with low energy cost. We evaluate our approach on three public data sets, and show significant improvements compared with several state-of-art methods.},
keywords={Target tracking;Cameras;Minimization;Polynomials;Joining processes;Image color analysis},
doi={10.1109/CVPR.2012.6247907},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247908,
author={Zhang, Tianzhu and Ghanem, Bernard and Liu, Si and Ahuja, Narendra},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust visual tracking via multi-task sparse learning},
year={2012},
volume={},
number={},
pages={2042-2049},
abstract={In this paper, we formulate object tracking in a particle filter framework as a multi-task sparse learning problem, which we denote as Multi-Task Tracking (MTT). Since we model particles as linear combinations of dictionary templates that are updated dynamically, learning the representation of each particle is considered a single task in MTT. By employing popular sparsity-inducing ℓp, q mixed norms (p ∈ {2, ∞} and q = 1), we regularize the representation problem to enforce joint sparsity and learn the particle representations together. As compared to previous methods that handle particles independently, our results demonstrate that mining the interdependencies between particles improves tracking performance and overall computational complexity. Interestingly, we show that the popular L1 tracker [15] is a special case of our MTT formulation (denoted as the L11 tracker) when p = q = 1. The learning problem can be efficiently solved using an Accelerated Proximal Gradient (APG) method that yields a sequence of closed form updates. As such, MTT is computationally attractive. We test our proposed approach on challenging sequences involving heavy occlusion, drastic illumination changes, and large pose variations. Experimental results show that MTT methods consistently outperform state-of-the-art trackers.},
keywords={Target tracking;Dictionaries;Joints;Visualization;Robustness;Encoding;Lighting},
doi={10.1109/CVPR.2012.6247908},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247909,
author={Schmidt, Uwe and Roth, Stefan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning rotation-aware features: From invariant priors to equivariant descriptors},
year={2012},
volume={},
number={},
pages={2050-2057},
abstract={Identifying suitable image features is a central challenge in computer vision, ranging from representations for low-level to high-level vision. Due to the difficulty of this task, techniques for learning features directly from example data have recently gained attention. Despite significant benefits, these learned features often have many fewer of the desired invariances or equivariances than their hand-crafted counterparts. While translation in-/equivariance has been addressed, the issue of learning rotation-invariant or equivariant representations is hardly explored. In this paper we describe a general framework for incorporating invariance to linear image transformations into product models for feature learning. A particular benefit is that our approach induces transformation-aware feature learning, i.e. it yields features that have a notion with which specific image transformation they are used. We focus our study on rotation in-/equivariance and show the advantages of our approach in learning rotation-invariant image priors and in building rotation-equivariant and invariant descriptors of learned features, which result in state-of-the-art performance for rotation-invariant object detection.},
keywords={Feature extraction;Histograms;Data models;Image restoration;Object detection;Transforms;Training data},
doi={10.1109/CVPR.2012.6247909},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247910,
author={Zhang, Xiao and Zhang, Lei and Shum, Heung-Yeung},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={QsRank: Query-sensitive hash code ranking for efficient ∊-neighbor search},
year={2012},
volume={},
number={},
pages={2058-2065},
abstract={Although binary hash code-based image indexing methods have been recently developed for large-scale applications, the problem of ranking such hash codes has been barely studied. In this paper, we propose a query sensitive ranking algorithm (QsRank) to rank PCA-based hash codes for the ε-neighbor search problem. The QsRank algorithm takes the target neighborhood radius ε and the raw feature of a given query as input, and models the statistical properties of the target ε-neighbors in the space of hash codes. Unlike the Hamming distance, the proposed algorithm does not compress query points to hash codes. Therefore, it suffers less information loss and is more effective than Hamming distance-based approaches. Based on the QsRank method, we developed an efficient indexing structure and retrieval algorithm for large-scale ε-neighbor search. Evaluations on two datasets of 10 million web images and 10 million SIFT descriptors demonstrate that the proposed retrieval system achieves higher accuracy with less memory cost and faster speed.},
keywords={Hamming distance;Principal component analysis;Vectors;Search problems;Indexing},
doi={10.1109/CVPR.2012.6247910},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247911,
author={Gong, Boqing and Shi, Yuan and Sha, Fei and Grauman, Kristen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Geodesic flow kernel for unsupervised domain adaptation},
year={2012},
volume={},
number={},
pages={2066-2073},
abstract={In real-world applications of visual recognition, many factors - such as pose, illumination, or image quality - can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods.},
keywords={Kernel;Principal component analysis;Measurement;Vectors;Manifolds;Visualization;Training},
doi={10.1109/CVPR.2012.6247911},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247912,
author={Liu, Wei and Wang, Jun and Ji, Rongrong and Jiang, Yu-Gang and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Supervised hashing with kernels},
year={2012},
volume={},
number={},
pages={2074-2081},
abstract={Recent years have witnessed the growing popularity of hashing in large-scale vision problems. It has been shown that the hashing quality could be boosted by leveraging supervised information into hash function learning. However, the existing supervised methods either lack adequate performance or often incur cumbersome model training. In this paper, we propose a novel kernel-based supervised hashing model which requires a limited amount of supervised information, i.e., similar and dissimilar data pairs, and a feasible training cost in achieving high quality hashing. The idea is to map the data to compact binary codes whose Hamming distances are minimized on similar pairs and simultaneously maximized on dissimilar pairs. Our approach is distinct from prior works by utilizing the equivalence between optimizing the code inner products and the Hamming distances. This enables us to sequentially and efficiently train the hash functions one bit at a time, yielding very short yet discriminative codes. We carry out extensive experiments on two image benchmarks with up to one million samples, demonstrating that our approach significantly outperforms the state-of-the-arts in searching both metric distance neighbors and semantically similar neighbors, with accuracy gains ranging from 13% to 46%.},
keywords={Kernel;Measurement;Optimization;Vectors;Training;Hamming distance;Semantics},
doi={10.1109/CVPR.2012.6247912},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247913,
author={Feng, Shikun and Lei, Zhen and Yi, Dong and Li, Stan Z.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Online content-aware video condensation},
year={2012},
volume={},
number={},
pages={2082-2087},
abstract={Explosive growth of surveillance video data presents formidable challenges to its browsing, retrieval and storage. Video synopsis, an innovation proposed by Peleg and his colleagues, is aimed for fast browsing by shortening the video into a synopsis while keeping activities in video captured by a camera. However, the current techniques are offline methods requiring that all the video data be ready for the processing, and are expensive in time and space. In this paper, we propose an online and efficient solution, and its supporting algorithms to overcome the problems. The method adopts an online content-aware approach in a step-wise manner, hence applicable to endless video, with less computational cost. Moreover, we propose a novel tracking method, called sticky tracking, to achieve high-quality visualization. The system can achieve a faster-than-real-time speed with a multi-core CPU implementation. The advantages are demonstrated by extensive experiments with a wide variety of videos. The proposed solution and algorithms could be integrated with surveillance cameras, and impact the way that surveillance videos are recorded.},
keywords={Electron tubes;Streaming media;Surveillance;Optimization;Games;Target tracking;Yarn},
doi={10.1109/CVPR.2012.6247913},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247914,
author={Chang, Hyung Jin and Jeong, Hawook and Choi, Jin Young},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Active attentional sampling for speed-up of background subtraction},
year={2012},
volume={},
number={},
pages={2088-2095},
abstract={In this paper, we present an active sampling method to speed up conventional pixel-wise background subtraction algorithms. The proposed active sampling strategy is designed to focus on attentional region such as foreground regions. The attentional region is estimated by detection results of previous frame in a recursive probabilistic way. For the estimation of the attentional region, we propose a foreground probability map based on temporal, spatial, and frequency properties of foregrounds. By using this foreground probability map, active attentional sampling scheme is developed to make a minimal sampling mask covering almost foregrounds. The effectiveness of the proposed active sampling method is shown through various experiments. The proposed masking method successfully speeds up pixel-wise background subtraction methods approximately 6.6 times without deteriorating detection performance. Also realtime detection with Full HD video is successfully achieved through various conventional background subtraction algorithms.},
keywords={Estimation;Monte Carlo methods;Probabilistic logic;High definition video;Real time systems;Indexes;Educational institutions},
doi={10.1109/CVPR.2012.6247914},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247915,
author={Varadarajan, Jagannadan and Emonet, Rémi and Odobez, Jean-Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Bridging the past, present and future: Modeling scene activities from event relationships and global rules},
year={2012},
volume={},
number={},
pages={2096-2103},
abstract={This paper addresses the discovery of activities and learns the underlying processes that govern their occurrences over time in complex surveillance scenes. To this end, we propose a novel topic model that accounts for the two main factors that affect these occurrences: (1) the existence of global scene states that regulate which of the activities can spontaneously occur; (2) local rules that link past activity occurrences to current ones with temporal lags. These complementary factors are mixed in the probabilistic generative process, thanks to the use of a binary random variable that selects for each activity occurrence which one of the above two factors is applicable. All model parameters are efficiently inferred using a collapsed Gibbs sampling inference scheme. Experiments on various datasets from the literature show that the model is able to capture temporal processes at multiple scales: the scene-level first order Markovian process, and causal relationships amongst activities that can be used to predict which activity can happen after another one, and after what delay, thus providing a rich interpretation of the scene's dynamical content.},
keywords={Hidden Markov models;Videos;Junctions;Analytical models;Visualization;Probabilistic logic;Data models},
doi={10.1109/CVPR.2012.6247915},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247916,
author={Narayana, Manjunath and Hanson, Allen and Learned-Miller, Erik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Background modeling using adaptive pixelwise kernel variances in a hybrid feature space},
year={2012},
volume={},
number={},
pages={2104-2111},
abstract={Recent work on background subtraction has shown developments on two major fronts. In one, there has been increasing sophistication of probabilistic models, from mixtures of Gaussians at each pixel [7], to kernel density estimates at each pixel [1], and more recently to joint domainrange density estimates that incorporate spatial information [6]. Another line of work has shown the benefits of increasingly complex feature representations, including the use of texture information, local binary patterns, and recently scale-invariant local ternary patterns [4]. In this work, we use joint domain-range based estimates for background and foreground scores and show that dynamically choosing kernel variances in our kernel estimates at each individual pixel can significantly improve results. We give a heuristic method for selectively applying the adaptive kernel calculations which is nearly as accurate as the full procedure but runs much faster. We combine these modeling improvements with recently developed complex features [4] and show significant improvements on a standard backgrounding benchmark.},
keywords={Kernel;Mathematical model;Adaptation models;Image color analysis;Equations;Estimation;Joints},
doi={10.1109/CVPR.2012.6247916},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247917,
author={Saligrama, Venkatesh and Chen, Zhu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Video anomaly detection based on local statistical aggregates},
year={2012},
volume={},
number={},
pages={2112-2119},
abstract={Anomalies in many video surveillance applications have local spatio-temporal signatures, namely, they occur over a small time window or a small spatial region. The distinguishing feature of these scenarios is that outside this spatio-temporal anomalous region, activities appear normal. We develop a probabilistic framework to account for such local spatio-temporal anomalies. We show that our framework admits elegant characterization of optimal decision rules. A key insight of the paper is that if anomalies are local optimal decision rules are local even when the nominal behavior exhibits global spatial and temporal statistical dependencies. This insight helps collapse the large ambient data dimension for detecting local anomalies. Consequently, consistent data-driven local empirical rules with provable performance can be derived with limited training data. Our empirical rules are based on scores functions derived from local nearest neighbor distances. These rules aggregate statistics across spatio-temporal locations & scales, and produce a single composite score for video segments. We demonstrate the efficacy of our scheme on several video surveillance datasets and compare with existing work.},
keywords={Training;Feature extraction;Hidden Markov models;Vectors;Markov processes;Streaming media;Training data},
doi={10.1109/CVPR.2012.6247917},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247918,
author={Song, Yale and Morency, Louis-Philippe and Davis, Randall},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-view latent variable discriminative models for action recognition},
year={2012},
volume={},
number={},
pages={2120-2127},
abstract={Many human action recognition tasks involve data that can be factorized into multiple views such as body postures and hand shapes. These views often interact with each other over time, providing important cues to understanding the action. We present multi-view latent variable discriminative models that jointly learn both view-shared and view-specific sub-structures to capture the interaction between views. Knowledge about the underlying structure of the data is formulated as a multi-chain structured latent conditional model, explicitly learning the interaction between multiple views using disjoint sets of hidden variables in a discriminative manner. The chains are tied using a predetermined topology that repeats over time. We present three topologies - linked, coupled, and linked-coupled - that differ in the type of interaction between views that they model. We evaluate our approach on both segmented and unsegmented human action recognition tasks, using the ArmGesture, the NATOPS, and the ArmGesture-Continuous data. Experimental results show that our approach outperforms previous state-of-the-art action recognition models.},
keywords={Hidden Markov models;Topology;Mathematical model;Data models;Humans;Accuracy;Equations},
doi={10.1109/CVPR.2012.6247918},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247919,
author={Paisitkriangkrai, Sakrapee and Shen, Chunhua and van den Hengel, Anton},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sharing features in multi-class boosting via group sparsity},
year={2012},
volume={},
number={},
pages={2128-2135},
abstract={We present a novel formulation of fully corrective boosting for multi-class classification problems with the awareness of sharing features. Our multi-class boosting is solved in a single optimization problem. In order to share features across different classes, we introduce the mixed-norm regularization, which promotes group sparsity, into boosting. We then derive the Lagrange dual problems which enable us to design fully corrective multi-class algorithms using the primal-dual optimization technique. We show that sharing features across classes can improve classification performance and efficiency. We empirically show that in many cases, the proposed multi-class boosting generalizes better than a range of competing multi-class boosting algorithms due to the capability of feature sharing. Experimental results on machine learning data, visual scene and object recognition demonstrate the efficiency and effectiveness of proposed algorithms and validate our theoretical findings.},
keywords={Boosting;Training;Logistics;Fasteners;Optimization;Algorithm design and analysis},
doi={10.1109/CVPR.2012.6247919},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247920,
author={Batra, Dhruv and Saxena, Ashutosh},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning the right model: Efficient max-margin learning in Laplacian CRFs},
year={2012},
volume={},
number={},
pages={2136-2143},
abstract={An important modeling decision made while designing Conditional Random Fields (CRFs) is the choice of the potential functions over the cliques of variables. Laplacian potentials are useful because they are robust potentials and match image statistics better than Gaussians. Moreover, energies with Laplacian terms remain convex, which simplifies inference. This makes Laplacian potentials an ideal modeling choice for some applications. In this paper, we study max-margin parameter learning in CRFs with Laplacian potentials (LCRFs). We first show that structured hinge-loss [35] is non-convex for LCRFs and thus techniques used by previous works are not applicable. We then present the first approximate max-margin algorithm for LCRFs. Finally, we make our learning algorithm scalable in the number of training images by using dual-decomposition techniques. Our experiments on single-image depth estimation show that even with simple features, our approach achieves comparable to state-of-art results.},
keywords={Laplace equations;Training;Approximation algorithms;Approximation methods;Labeling;Estimation;Vectors},
doi={10.1109/CVPR.2012.6247920},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247921,
author={Zhang, Weiyu and Yu, Stella X. and Teng, Shang-Hua},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Power SVM: Generalization with exemplar classification uncertainty},
year={2012},
volume={},
number={},
pages={2144-2151},
abstract={The human vision tends to recognize more variants of a distinctive exemplar. This observation suggests that discriminative power of training exemplars could be utilized for shaping a desirable global classifier that generalizes maximally from a few exemplars. We propose to derive classification uncertainty for each exemplar, using a local classification task to separate the exemplar from those in other categories. We then design a global classifier by incorporating these uncertainties into constraints on the classifier margins. We show through the dual form that the classification criterion can be interpreted as finding closest points between convex hulls in the feature space augmented by classification uncertainty. We call this scheme Power SVM (as in Power Diagram), since each exemplar is no longer a singular point in the feature space, but a super-point with its own governing power in the classifier space. We test Power SVM on digit recognition, indoor-outdoor categorization, and large-scale scene classification tasks. It shows consistent improvement over SVM and uncertainty weighted SVM, especially when the number of training exemplars is small.},
keywords={Uncertainty;Support vector machines;Training;Training data;Humans;Measurement uncertainty;Labeling},
doi={10.1109/CVPR.2012.6247921},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247922,
author={Biswas, Arijit and Jacobs, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Active image clustering: Seeking constraints from humans to complement algorithms},
year={2012},
volume={},
number={},
pages={2152-2159},
abstract={We propose a method of clustering images that combines algorithmic and human input. An algorithm provides us with pairwise image similarities. We then actively obtain selected, more accurate pairwise similarities from humans. A novel method is developed to choose the most useful pairs to show a person, obtaining constraints that improve clustering. In a clustering assignment elements in each data pair are either in the same cluster or in different clusters. We simulate inverting these pairwise relations and see how that affects the overall clustering. We choose a pair that maximizes the expected change in the clustering. The proposed algorithm has high time complexity, so we also propose a version of this algorithm that is much faster and exactly replicates our original algorithm. We further improve run-time by adding heuristics, and show that these do not significantly impact the effectiveness of our method. We have run experiments in two different domains, namely leaf images and face images, and show that clustering performance can be improved significantly.},
keywords={Clustering algorithms;Humans;Complexity theory;Vegetation;Face;Accuracy;Videos},
doi={10.1109/CVPR.2012.6247922},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247923,
author={Sharma, Abhishek and Kumar, Abhishek and Daume, Hal and Jacobs, David W.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Generalized Multiview Analysis: A discriminative latent space},
year={2012},
volume={},
number={},
pages={2160-2167},
abstract={This paper presents a general multi-view feature extraction approach that we call Generalized Multiview Analysis or GMA. GMA has all the desirable properties required for cross-view classification and retrieval: it is supervised, it allows generalization to unseen classes, it is multi-view and kernelizable, it affords an efficient eigenvalue based solution and is applicable to any domain. GMA exploits the fact that most popular supervised and unsupervised feature extraction techniques are the solution of a special form of a quadratic constrained quadratic program (QCQP), which can be solved efficiently as a generalized eigenvalue problem. GMA solves a joint, relaxed QCQP over different feature spaces to obtain a single (non)linear subspace. Intuitively, GMA is a supervised extension of Canonical Correlational Analysis (CCA), which is useful for cross-view classification and retrieval. The proposed approach is general and has the potential to replace CCA whenever classification or retrieval is the purpose and label information is available. We outperform previous approaches for textimage retrieval on Pascal and Wiki text-image data. We report state-of-the-art results for pose and lighting invariant face recognition on the MultiPIE face dataset, significantly outperforming other approaches.},
keywords={Lighting;Feature extraction;Nickel;Eigenvalues and eigenfunctions;Face recognition;Bismuth;Face},
doi={10.1109/CVPR.2012.6247923},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247924,
author={Jhuo, I-Hong and Liu, Dong and Lee, D. T. and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust visual domain adaptation with low-rank reconstruction},
year={2012},
volume={},
number={},
pages={2168-2175},
abstract={Visual domain adaptation addresses the problem of adapting the sample distribution of the source domain to the target domain, where the recognition task is intended but the data distributions are different. In this paper, we present a low-rank reconstruction method to reduce the domain distribution disparity. Specifically, we transform the visual samples in the source domain into an intermediate representation such that each transformed source sample can be linearly reconstructed by the samples of the target domain. Unlike the existing work, our method captures the intrinsic relatedness of the source samples during the adaptation process while uncovering the noises and outliers in the source domain that cannot be adapted, making it more robust than previous methods. We formulate our problem as a constrained nuclear norm and ℓ2, 1 norm minimization objective and then adopt the Augmented Lagrange Multiplier (ALM) method for the optimization. Extensive experiments on various visual adaptation tasks show that the proposed method consistently and significantly beats the state-of-the-art domain adaptation methods.},
keywords={Visualization;Optimization;Noise;Support vector machines;Sparse matrices;Training;Robustness},
doi={10.1109/CVPR.2012.6247924},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247925,
author={Lin, Dahua and Fisher, John},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Manifold guided composite of Markov random fields for image modeling},
year={2012},
volume={},
number={},
pages={2176-2183},
abstract={We present a new generative image model, integrating techniques arising from two different domains: manifold modeling and Markov random fields. First, we develop a probabilistic model with a mixture of hyperplanes to approximate the manifold of orientable image patches, and demonstrate that it is more effective than the field of experts in expressing local texture patterns. Next, we develop a construction that yields an MRF for coherent image generation, given a configuration of local patch models, and thereby establish a prior distribution over an MRF space. Taking advantage of the model structure, we derive a variational inference algorithm, and apply it to low-level vision. In contrast to previous methods that rely on a single MRF, the method infers an approximate posterior distribution of MRFs, and recovers the underlying images by combining the predictions in a Bayesian fashion. Experiments quantitatively demonstrate superior performance as compared to state-of-the-art methods on image denoising and inpainting.},
keywords={Manifolds;Coherence;Inference algorithms;Vectors;Approximation methods;Probabilistic logic;Bayesian methods},
doi={10.1109/CVPR.2012.6247925},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247926,
author={Cinbis, Ramazan Gokberk and Verbeek, Jakob and Schmid, Cordelia},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image categorization using Fisher kernels of non-iid image models},
year={2012},
volume={},
number={},
pages={2184-2191},
abstract={The bag-of-words (BoW) model treats images as an unordered set of local regions and represents them by visual word histograms. Implicitly, regions are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. We introduce non-iid models by treating the parameters of BoW models as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel we encode an image by the gradient of the data log-likelihood w.r.t. hyper-parameters that control priors on the model parameters. Our representation naturally involves discounting transformations similar to taking square-roots, providing an explanation of why such transformations have proven successful. Using variational inference we extend the basic model to include Gaussian mixtures over local descriptors, and latent topic models to capture the co-occurrence structure of visual words, both improving performance. Our models yield state-of-the-art categorization performance using linear classifiers; without using non-linear transformations such as taking square-roots of features, or using (approximate) explicit embeddings of non-linear kernels.},
keywords={Visualization;Computational modeling;Kernel;Image representation;Histograms;Vectors;Mathematical model},
doi={10.1109/CVPR.2012.6247926},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247927,
author={Zhang, Debing and Hu, Yao and Ye, Jieping and Li, Xuelong and He, Xiaofei},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Matrix completion by Truncated Nuclear Norm Regularization},
year={2012},
volume={},
number={},
pages={2192-2199},
abstract={Estimating missing values in visual data is a challenging problem in computer vision, which can be considered as a low rank matrix approximation problem. Most of the recent studies use the nuclear norm as a convex relaxation of the rank operator. However, by minimizing the nuclear norm, all the singular values are simultaneously minimized, and thus the rank can not be well approximated in practice. In this paper, we propose a novel matrix completion algorithm based on the Truncated Nuclear Norm Regularization (TNNR) by only minimizing the smallest N-r singular values, where N is the number of singular values and r is the rank of the matrix. In this way, the rank of the matrix can be better approximated than the nuclear norm. We further develop an efficient iterative procedure to solve the optimization problem by using the alternating direction method of multipliers and the accelerated proximal gradient line search method. Experimental results in a wide range of applications demonstrate the effectiveness of our proposed approach.},
keywords={Optimization;Approximation methods;Educational institutions;Minimization;Convergence;Visualization;Acceleration},
doi={10.1109/CVPR.2012.6247927},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247928,
author={Ong, Eng-Jon and Cooper, Helen and Pugeault, Nicolas and Bowden, Richard},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sign Language Recognition using Sequential Pattern Trees},
year={2012},
volume={},
number={},
pages={2200-2207},
abstract={This paper presents a novel, discriminative, multi-class classifier based on Sequential Pattern Trees. It is efficient to learn, compared to other Sequential Pattern methods, and scalable for use with large classifier banks. For these reasons it is well suited to Sign Language Recognition. Using deterministic robust features based on hand trajectories, sign level classifiers are built from sub-units. Results are presented both on a large lexicon single signer data set and a multi-signer Kinect™ data set. In both cases it is shown to out perform the non-discriminative Markov model approach and be equivalent to previous, more costly, Sequential Pattern (SP) techniques.},
keywords={Itemsets;Training;Hidden Markov models;Handicapped aids;Vectors;Boosting;Nickel},
doi={10.1109/CVPR.2012.6247928},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247929,
author={Afsari, Bijan and Chaudhry, Rizwan and Ravichandran, Avinash and Vidal, René},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Group action induced distances for averaging and clustering Linear Dynamical Systems with applications to the analysis of dynamic scenes},
year={2012},
volume={},
number={},
pages={2208-2215},
abstract={We introduce a framework for defining a distance on the (non-Euclidean) space of Linear Dynamical Systems (LDSs). The proposed distance is induced by the action of the group of orthogonal matrices on the space of statespace realizations of LDSs. This distance can be efficiently computed for large-scale problems, hence it is suitable for applications in the analysis of dynamic visual scenes and other high dimensional time series. Based on this distance we devise a simple LDS averaging algorithm, which can be used for classification and clustering of time-series data. We test the validity as well as the performance of our group-action based distance on synthetic as well as real data and provide comparison with state-of-the-art methods.},
keywords={Observability;Computational modeling;Manifolds;Stochastic processes;Standards;Training data;Measurement},
doi={10.1109/CVPR.2012.6247929},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247930,
author={Wang, Shenlong and Zhang, Lei and Liang, Yan and Pan, Quan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis},
year={2012},
volume={},
number={},
pages={2216-2223},
abstract={In various computer vision applications, often we need to convert an image in one style into another style for better visualization, interpretation and recognition; for examples, up-convert a low resolution image to a high resolution one, and convert a face sketch into a photo for matching, etc. A semi-coupled dictionary learning (SCDL) model is proposed in this paper to solve such cross-style image synthesis problems. Under SCDL, a pair of dictionaries and a mapping function will be simultaneously learned. The dictionary pair can well characterize the structural domains of the two styles of images, while the mapping function can reveal the intrinsic relationship between the two styles' domains. In SCDL, the two dictionaries will not be fully coupled, and hence much flexibility can be given to the mapping function for an accurate conversion across styles. Moreover, clustering and image nonlocal redundancy are introduced to enhance the robustness of SCDL. The proposed SCDL model is applied to image super-resolution and photo-sketch synthesis, and the experimental results validated its generality and effectiveness in cross-style image synthesis.},
keywords={Dictionaries;Image resolution;Encoding;Image reconstruction;Face;Image generation;Clustering algorithms},
doi={10.1109/CVPR.2012.6247930},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247931,
author={Yang, Meng and Zhang, Lei and Zhang, David and Wang, Shenlong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Relaxed collaborative representation for pattern classification},
year={2012},
volume={},
number={},
pages={2224-2231},
abstract={Regularized linear representation learning has led to interesting results in image classification, while how the object should be represented is a critical issue to be investigated. Considering the fact that the different features in a sample should contribute differently to the pattern representation and classification, in this paper we present a novel relaxed collaborative representation (RCR) model to effectively exploit the similarity and distinctiveness of features. In RCR, each feature vector is coded on its associated dictionary to allow flexibility of feature coding, while the variance of coding vectors is minimized to address the similarity among features. In addition, the distinctiveness of different features is exploited by weighting its distance to other features in the coding domain. The proposed RCR is simple, while our extensive experimental results on benchmark image databases (e.g., various face and flower databases) show that it is very competitive with state-of-the-art image classification methods.},
keywords={Encoding;Vectors;Face;Dictionaries;Robustness;Collaboration;Image coding},
doi={10.1109/CVPR.2012.6247931},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247932,
author={Zhang, Ziming and Sturgess, Paul and Sengupta, Sunando and Crook, Nigel and Torr, Philip H. S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient discriminative learning of parametric nearest neighbor classifiers},
year={2012},
volume={},
number={},
pages={2232-2239},
abstract={Linear SVMs are efficient in both training and testing, however the data in real applications is rarely linearly separable. Non-linear kernel SVMs are too computationally intensive for applications with large-scale data sets. Recently locally linear classifiers have gained popularity due to their efficiency whilst remaining competitive with kernel methods. The vanilla nearest neighbor algorithm is one of the simplest locally linear classifiers, but it lacks robustness due to the noise often present in real-world data. In this paper, we introduce a novel local classifier, Parametric Nearest Neighbor (P-NN) and its extension Ensemble of P-NN (EP-NN). We parameterize the nearest neighbor algorithm based on the minimum weighted squared Euclidean distances between the data points and the prototypes, where a prototype is represented by a locally linear combination of some data points. Meanwhile, our method attempts to jointly learn both the prototypes and the classifier parameters discriminatively via max-margin. This makes our classifiers suitable to approximate the classification decision boundaries locally based on nonlinear functions. During testing, the computational complexity of both classifiers is linear in the product of the dimension of data and the number of prototypes. Our classification results on MNIST, USPS, LETTER, and Chars 74K are comparable and in some cases are better than many other methods such as the state-of-the-art locally linear classifiers.},
keywords={Prototypes;Kernel;Approximation methods;Testing;Computational complexity;Training data;Vectors},
doi={10.1109/CVPR.2012.6247932},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247933,
author={Wang, Shuhui and Jiang, Shuqiang and Huang, Qingming and Tian, Qi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-feature metric learning with knowledge transfer among semantics and social tagging},
year={2012},
volume={},
number={},
pages={2240-2247},
abstract={Previous metric learning approaches learn a unified metric for all the classes on single feature representation, thus cannot be directly transplanted to applications involving multiple features, hundreds to thousands of hierarchical structured semantics and abundant social tagging. In this paper, we propose a novel multi-task multi-feature metric learning method which models the information sharing mechanism among different learning tasks. We decompose the real world multi-class problems such as semantic categorization or automatic tagging into a set of tasks where each task corresponds to several classes with strong visual correlation. We conduct metric learning to learn a set of (hyper)category-specific metrics for all the tasks. By encouraging model sharing among tasks, more generalization power is acquired. Another advantage is the capability of simultaneous learning with semantic information and social tagging based on the multi-task learning framework, and thus they both benefit from the information provided by each other. Experiments demonstrate the advantages on applications including semantic categorization and automatic tagging compared with other popular metric learning approaches.},
keywords={Measurement;Kernel;Semantics;Tagging;Training;Visualization;Support vector machines},
doi={10.1109/CVPR.2012.6247933},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247934,
author={Varol, Aydin and Salzmann, Mathieu and Fua, Pascal and Urtasun, Raquel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A constrained latent variable model},
year={2012},
volume={},
number={},
pages={2248-2255},
abstract={Latent variable models provide valuable compact representations for learning and inference in many computer vision tasks. However, most existing models cannot directly encode prior knowledge about the specific problem at hand. In this paper, we introduce a constrained latent variable model whose generated output inherently accounts for such knowledge. To this end, we propose an approach that explicitly imposes equality and inequality constraints on the model's output during learning, thus avoiding the computational burden of having to account for these constraints at inference. Our learning mechanism can exploit non-linear kernels, while only involving sequential closed-form updates of the model parameters. We demonstrate the effectiveness of our constrained latent variable model on the problem of non-rigid 3D reconstruction from monocular images, and show that it yields qualitative and quantitative improvements over several baselines.},
keywords={Computational modeling;Training;Image reconstruction;Predictive models;Kernel;Optimization;Shape},
doi={10.1109/CVPR.2012.6247934},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247935,
author={Matikainen, Pyry and Sukthankar, Rahul and Hebert, Martial},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Model recommendation for action recognition},
year={2012},
volume={},
number={},
pages={2256-2263},
abstract={Simply choosing one model out of a large set of possibilities for a given vision task is a surprisingly difficult problem, especially if there is limited evaluation data with which to distinguish among models, such as when choosing the best “walk” action classifier from a large pool of classifiers tuned for different viewing angles, lighting conditions, and background clutter. In this paper we suggest that this problem of selecting a good model can be recast as a recommendation problem, where the goal is to recommend a good model for a particular task based on how well a limited probe set of models appears to perform. Through this conceptual remapping, we can bring to bear all the collaborative filtering techniques developed for consumer recommender systems (e.g., Netflix, Amazon.com). We test this hypothesis on action recognition, and find that even when every model has been directly rated on a training set, recommendation finds better selections for the corresponding test set than the best performers on the training set.},
keywords={Predictive models;Probes;Collaboration;Data models;Accuracy;Legged locomotion;Training data},
doi={10.1109/CVPR.2012.6247935},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247936,
author={Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust Boltzmann Machines for recognition and denoising},
year={2012},
volume={},
number={},
pages={2264-2271},
abstract={While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases.},
keywords={Noise;Face;Robustness;Data models;Noise measurement;Databases;Training},
doi={10.1109/CVPR.2012.6247936},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247937,
author={Ghosh, Soumya and Sudderth, Erik B.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Nonparametric learning for layered segmentation of natural images},
year={2012},
volume={},
number={},
pages={2272-2279},
abstract={We explore recently proposed Bayesian nonparametric models of image partitions, based on spatially dependent Pitman-Yor processes. These models are attractive because they adapt to images of varying complexity, successfully modeling uncertainty in the structure and scale of human segmentations of natural scenes. By developing substantially improved inference and learning algorithms, we achieve performance comparable to state-of-the-art methods. For learning, we show how the Gaussian process (GP) covariance functions underlying these models can be calibrated to accurately match the statistics of example human segmentations. For inference, we develop a stochastic search-based algorithm which is substantially less susceptible to local optima than conventional variational methods. Our approach utilizes the expectation propagation algorithm to approximately marginalize latent GPs, and a low rank covariance representation to improve computational efficiency. Experiments with two benchmark datasets show that our learning and inference innovations substantially improve segmentation accuracy. By hypothesizing multiple partitions for each image, we also take steps towards capturing the variability of human scene interpretations.},
keywords={Image segmentation;Partitioning algorithms;Humans;Inference algorithms;Correlation;Image color analysis;Computational modeling},
doi={10.1109/CVPR.2012.6247937},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247938,
author={Verma, Nakul and Mahajan, Dhruv and Sellamanickam, Sundararajan and Nair, Vinod},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning hierarchical similarity metrics},
year={2012},
volume={},
number={},
pages={2280-2287},
abstract={Categories in multi-class data are often part of an underlying semantic taxonomy. Recent work in object classification has found interesting ways to use this taxonomy structure to develop better recognition algorithms. Here we propose a novel framework to learn similarity metrics using the class taxonomy. We show that a nearest neighbor classifier using the learned metrics gets improved performance over the best discriminative methods. Moreover, by incorporating the taxonomy, our learned metrics can also help in some taxonomy specific applications. We show that the metrics can help determine the correct placement of a new category that was not part of the original taxonomy, and can provide effective classification amongst categories local to specific subtrees of the taxonomy.},
keywords={Measurement;Taxonomy;Prototypes;Accuracy;Training;Optimization;Support vector machines},
doi={10.1109/CVPR.2012.6247938},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247939,
author={Köstinger, Martin and Hirzer, Martin and Wohlhart, Paul and Roth, Peter M. and Bischof, Horst},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Large scale metric learning from equivalence constraints},
year={2012},
volume={},
number={},
pages={2288-2295},
abstract={In this paper, we raise important issues on scalability and the required degree of supervision of existing Mahalanobis metric learning methods. Often rather tedious optimization procedures are applied that become computationally intractable on a large scale. Further, if one considers the constantly growing amount of data it is often infeasible to specify fully supervised labels for all data points. Instead, it is easier to specify labels in form of equivalence constraints. We introduce a simple though effective strategy to learn a distance metric from equivalence constraints, based on a statistical inference perspective. In contrast to existing methods we do not rely on complex optimization problems requiring computationally expensive iterations. Hence, our method is orders of magnitudes faster than comparable methods. Results on a variety of challenging benchmarks with rather diverse nature demonstrate the power of our method. These include faces in unconstrained environments, matching before unseen object instances and person re-identification across spatially disjoint cameras. In the latter two benchmarks we clearly outperform the state-of-the-art.},
keywords={Measurement;Training;Databases;Benchmark testing;Support vector machines;Scalability;Optimization},
doi={10.1109/CVPR.2012.6247939},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247940,
author={Yang, Jimei and Yang, Ming-Hsuan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Top-down visual saliency via joint CRF and dictionary learning},
year={2012},
volume={},
number={},
pages={2296-2303},
abstract={Top-down visual saliency facilities object localization by providing a discriminative representation of target objects and a probability map for reducing the search space. In this paper, we propose a novel top-down saliency model that jointly learns a Conditional Random Field (CRF) and a discriminative dictionary. The proposed model is formulated based on a CRF with latent variables. By using sparse codes as latent variables, we train the dictionary modulated by CRF, and meanwhile a CRF with sparse coding. We propose a max-margin approach to train our model via fast inference algorithms. We evaluate our model on the Graz-02 and PASCAL VOC 2007 datasets. Experimental results show that our model performs favorably against the state-of-the-art top-down saliency methods. We also observe that the dictionary update significantly improves the model performance.},
keywords={Dictionaries;Visualization;Training;Encoding;Computational modeling;Joints;Bicycles},
doi={10.1109/CVPR.2012.6247940},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247941,
author={Ranjbar, Mani and Vahdat, Arash and Mori, Greg},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Complex loss optimization via dual decomposition},
year={2012},
volume={},
number={},
pages={2304-2311},
abstract={We describe a novel max-margin parameter learning approach for structured prediction problems under certain non-decomposable performance measures. Structured prediction is a common approach in many vision problems. Non-decomposable performance measures are also commonplace. However, efficient general methods for learning parameters against non-decomposable performance measures do not exist. In this paper we develop such a method, based on dual decomposition, that is applicable to a large class of non-decomposable performance measures. We exploit dual decomposition to factorize the original hard problem into two smaller problems and show how to optimize each factor efficiently. We show experimentally that the proposed approach significantly outperforms alternatives, which either sacrifice the model structure or approximate the performance measure, and is an order of magnitude faster than a previous approach with comparable results.},
keywords={Markov random fields;Optimization;Image segmentation;Computational modeling;Piecewise linear approximation;Loss measurement;Inference algorithms},
doi={10.1109/CVPR.2012.6247941},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247942,
author={Wang, Bo and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Affinity learning via self-diffusion for image segmentation and clustering},
year={2012},
volume={},
number={},
pages={2312-2319},
abstract={Computing a faithful affinity map is essential to the clustering and segmentation tasks. In this paper, we propose a graph-based affinity (metric) learning method and show its application to image clustering and segmentation. Our method, self-diffusion (SD), performs a diffusion process by propagating the similarity mass along the intrinsic manifold of data points. Theoretical analysis is given to the SD algorithm and we provide a way of deriving the critical time stamp t. Our method therefore has nearly no parameter tuning and leads to significantly improved affinity maps, which help to greatly enhance the quality of clustering. In addition, we show that much improved image segmentation results can be obtained by combining SD with e.g. the normalized cuts algorithm. The proposed method can be used to deliver robust affinity maps for a range of problems.},
keywords={Delta modulation;Kernel;Image segmentation;Laplace equations;Measurement;Manifolds;Accuracy},
doi={10.1109/CVPR.2012.6247942},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247943,
author={Vedaldi, Andrea and Zisserman, Andrew},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sparse kernel approximations for efficient classification and detection},
year={2012},
volume={},
number={},
pages={2320-2327},
abstract={Efficient learning with non-linear kernels is often based on extracting features from the data that “linearise” the kernel. While most constructions aim at obtaining low-dimensional and dense features, in this work we explore high-dimensional and sparse ones. We give a method to compute sparse features for arbitrary kernels, re-deriving as a special case a popular map for the intersection kernel and extending it to arbitrary additive kernels. We show that bundle optimisation methods can handle efficiently these sparse features in learning. As an application, we show that product quantisation can be interpreted as a sparse feature encoding, and use this to significantly accelerate learning with this technique. We demonstrate these ideas on image classification with Fisher kernels and object detection with deformable part models on the challenging PASCAL VOC data, obtaining five to ten-fold speed-ups as well as reducing memory use by an order of magnitude.},
keywords={Kernel;Approximation methods;Vectors;Encoding;Support vector machines;Additives;Quantization},
doi={10.1109/CVPR.2012.6247943},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247944,
author={Zhuang, Liansheng and Gao, Haoyuan and Lin, Zhouchen and Ma, Yi and Zhang, Xin and Yu, Nenghai},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Non-negative low rank and sparse graph for semi-supervised learning},
year={2012},
volume={},
number={},
pages={2328-2335},
abstract={Constructing a good graph to represent data structures is critical for many important machine learning tasks such as clustering and classification. This paper proposes a novel non-negative low-rank and sparse (NNLRS) graph for semi-supervised learning. The weights of edges in the graph are obtained by seeking a nonnegative low-rank and sparse matrix that represents each data sample as a linear combination of others. The so-obtained NNLRS-graph can capture both the global mixture of subspaces structure (by the low rankness) and the locally linear structure (by the sparseness) of the data, hence is both generative and discriminative. We demonstrate the effectiveness of NNLRS-graph in semi-supervised classification and discriminative analysis. Extensive experiments testify to the significant advantages of NNLRS-graph over graphs obtained through conventional means.},
keywords={Databases;Sparse matrices;Noise;Strontium;Vectors;Optimization;Educational institutions},
doi={10.1109/CVPR.2012.6247944},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247945,
author={Hu, Wenze},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning 3D object templates by hierarchical quantization of geometry and appearance spaces},
year={2012},
volume={},
number={},
pages={2336-2343},
abstract={This paper presents a method for learning 3D object templates from view labeled object images. The 3D template is defined in a joint appearance and geometry space composed of deformable planar part templates placed at different 3D positions and orientations. Appearance of each part template is represented by Gabor filters, which are hierarchically grouped into line segments and geometric shapes. AND-OR trees are further used to quantize the possible geometry and appearance of part templates, so that learning can be done on a subsampled discrete space. Using information gain as a criterion, the best 3D template can be searched through the AND-OR trees using one bottom-up pass and one top-down pass. Experiments on a new car dataset with diverse views show that the proposed method can learn meaningful 3D car templates, and give satisfactory detection and view estimation performance. Experiments are also performed on a public car dataset, which show comparable performance with recent methods.},
keywords={Geometry;Solid modeling;Shape;Image segmentation;Computational modeling;Quantization;Abstracts},
doi={10.1109/CVPR.2012.6247945},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247946,
author={Wu, Jianxin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Power mean SVM for large scale visual classification},
year={2012},
volume={},
number={},
pages={2344-2351},
abstract={PmSVM (Power Mean SVM), a classifier that trains significantly faster than state-of-the-art linear and non-linear SVM solvers in large scale visual classification tasks, is presented. PmSVM also achieves higher accuracies. A scalable learning method for large vision problems, e.g., with millions of examples or dimensions, is a key component in many current vision systems. Recent progresses have enabled linear classifiers to efficiently process such large scale problems. Linear classifiers, however, usually have inferior accuracies in vision tasks. Non-linear classifiers, on the other hand, may take weeks or even years to train. We propose a power mean kernel and present an efficient learning algorithm through gradient approximation. The power mean kernel family include as special cases many popular additive kernels. Empirically, PmSVM is up to 5 times faster than LIBLINEAR, and two times faster than state-of-the-art additive kernel classifiers. In terms of accuracy, it outperforms state-of-the-art additive kernel implementations, and has major advantages over linear SVM.},
keywords={Kernel;Support vector machines;Additives;Training;Approximation methods;Accuracy;Approximation algorithms},
doi={10.1109/CVPR.2012.6247946},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247947,
author={Kong, Deguang and Ding, Chris and Huang, Heng and Zhao, Haifeng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-label ReliefF and F-statistic feature selections for image annotation},
year={2012},
volume={},
number={},
pages={2352-2359},
abstract={The classical ReliefF and F-statistic feature selections can not be directly applied into multi-label problems due to the ambiguity produced from a data point attributed to multiple classes simultaneously. In this paper, we present MReliefF and MF-statistic algorithms for multi-label feature selections. Discriminant features are selected to boost the multi-label classification accuracy. The proposed MReliefF and MF-statistic can be used in image categorization and annotation problems. Extensive experiments on image annotation tasks show the good performance of our approach. To our knowledge, this is the first work to generalize the ReliefF and F-statistic feature selection algorithms for multi-label image annotation tasks.},
keywords={Feature extraction;Standards;Support vector machines;Image color analysis;Buildings;Educational institutions;Semantics},
doi={10.1109/CVPR.2012.6247947},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247948,
author={Yang, Jianchao and Wang, Zhaowen and Lin, Zhe and Shu, Xianbiao and Huang, Thomas},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Bilevel sparse coding for coupled feature spaces},
year={2012},
volume={},
number={},
pages={2360-2367},
abstract={In this paper, we propose a bilevel sparse coding model for coupled feature spaces, where we aim to learn dictionaries for sparse modeling in both spaces while enforcing some desired relationships between the two signal spaces. We first present our new general sparse coding model that relates signals from the two spaces by their sparse representations and the corresponding dictionaries. The learning algorithm is formulated as a generic bilevel optimization problem, which is solved by a projected first-order stochastic gradient descent algorithm. This general sparse coding model can be applied to many specific applications involving coupled feature spaces in computer vision and signal processing. In this work, we tailor our general model to learning dictionaries for compressive sensing recovery and single image super-resolution to demonstrate its effectiveness. In both cases, the new sparse coding model remarkably outperforms previous approaches in terms of recovery accuracy.},
keywords={Dictionaries;Encoding;Compressed sensing;Sparse matrices;Optimization;Image resolution;Sensors},
doi={10.1109/CVPR.2012.6247948},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247949,
author={Li, Wen and Duan, Lixin and Tsang, Ivor Wai-Hung and Xu, Dong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Batch mode Adaptive Multiple Instance Learning for computer vision tasks},
year={2012},
volume={},
number={},
pages={2368-2375},
abstract={Multiple Instance Learning (MIL) has been widely exploited in many computer vision tasks, such as image retrieval, object tracking and so on. To handle ambiguity of instance labels in positive bags, the training process of traditional MIL methods is usually computationally expensive, which limits the applications of MIL in more computer vision tasks. In this paper, we propose a novel batch mode framework, namely Batch mode Adaptive Multiple Instance Learning (BAMIL), to accelerate the instance-level MIL methods. Specifically, instead of using all training bags at once, we divide the training bags into several sets of bags (i.e., batches). At each time, we use one batch of training bags to train a new classifier which is adapted from the latest pre-learned classifier. Such batch mode framework significantly accelerates the traditional MIL methods for large scale applications and can be also used in dynamic environments such as object tracking. The experimental results show that our BAMIL is much faster than the recently developed MIL with constrained positive bags while achieves comparable performance for text-based web image retrieval. In dynamic settings, BAMIL also achieves the better overall performance for object tracking when compared with other online MIL methods.},
keywords={Training;Bismuth;Kernel;Vectors;Acceleration;Silicon;Image retrieval},
doi={10.1109/CVPR.2012.6247949},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247950,
author={Jancsary, Jeremy and Nowozin, Sebastian and Sharp, Toby and Rother, Carsten},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Regression Tree Fields — An efficient, non-parametric approach to image labeling problems},
year={2012},
volume={},
number={},
pages={2376-2383},
abstract={We introduce Regression Tree Fields (RTFs), a fully conditional random field model for image labeling problems. RTFs gain their expressive power from the use of non-parametric regression trees that specify a tractable Gaussian random field, thereby ensuring globally consistent predictions. Our approach improves on the recently introduced decision tree field (DTF) model [14] in three key ways: (i) RTFs have tractable test-time inference, making efficient optimal predictions feasible and orders of magnitude faster than for DTFs, (ii) RTFs can be applied to both discrete and continuous vector-valued labeling tasks, and (Hi) the entire model, including the structure of the regression trees and energy function parameters, can be efficiently and jointly learned from training data. We demonstrate the expressive power and flexibility of the RTF model on a wide variety of tasks, including inpainting, colorization, denoising, and joint detection and registration. We achieve excellent predictive performance which is on par with, or even surpassing, DTFs on all tasks where a comparison is possible.},
keywords={Regression tree analysis;Training;Joints;Labeling;Data models;Training data;Computational modeling},
doi={10.1109/CVPR.2012.6247950},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247951,
author={Minh, Hà Quang and Cristani, Marco and Perina, Alessandro and Murino, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A regularized spectral algorithm for Hidden Markov Models with applications in computer vision},
year={2012},
volume={},
number={},
pages={2384-2391},
abstract={Hidden Markov Models (HMMs) are among the most important and widely used techniques to deal with sequential or temporal data. Their application in computer vision ranges from action/gesture recognition to videosurveillance through shape analysis. Although HMMs are often embedded in complex frameworks, this paper focuses on theoretical aspects of HMM learning. We propose a regularized algorithm for learning HMMs in the spectral framework, whose computations have no local minima. Compared with recently proposed spectral algorithms for HMMs, our method is guaranteed to produce probability values which are always physically meaningful and which, on synthetic mathematical models, give very good approximations to true probability values. Furthermore, we place no restriction on the number of symbols and the number of states. On various pattern recognition data sets, our algorithm consistently outperforms classical HMMs, both in accuracy and computational speed. This and the fact that HMMs are used in vision as building blocks for more powerful classification approaches, such as generative embedding approaches or more complex generative models, strongly support spectral HMMs (SHMMs) as a new basic tool for pattern recognition.},
keywords={Hidden Markov models;Vectors;Joints;Matrix decomposition;Mathematical model;Computer vision;Algorithm design and analysis},
doi={10.1109/CVPR.2012.6247951},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247952,
author={Burger, Harold C. and Schuler, Christian J. and Harmeling, Stefan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image denoising: Can plain neural networks compete with BM3D?},
year={2012},
volume={},
number={},
pages={2392-2399},
abstract={Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. The best currently available denoising methods approximate this mapping with cleverly engineered algorithms. In this work we attempt to learn this mapping directly with a plain multi layer perceptron (MLP) applied to image patches. While this has been done before, we will show that by training on large image databases we are able to compete with the current state-of-the-art image denoising methods. Furthermore, our approach is easily adapted to less extensively studied types of noise (by merely exchanging the training data), for which we achieve excellent results as well.},
keywords={Noise;Training;Noise measurement;Noise level;Noise reduction;Neural networks;Standards},
doi={10.1109/CVPR.2012.6247952},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247953,
author={Dai, Zhenwen and Lücke, Jörg},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised learning of translation invariant occlusive components},
year={2012},
volume={},
number={},
pages={2400-2407},
abstract={We study unsupervised learning of occluding objects in images of visual scenes. The derived learning algorithm is based on a probabilistic generative model which parameterizes object shapes, object features and the background. No assumptions are made for the object orders in depth or the objects' planar positions. Parameter optimization is thus subject to the large combinatorics of depth orders and positions. Previous approaches constrained this combinatorics but were still only able to learn a very small number of objects. By applying a novel variational EM approach, we show that even without constraints on the object combinatorics, a relatively large number of objects can be learned. In different numerical experiments, our unsupervised approach extracts explicit object representations with object masks and object features closely aligned with the true objects in the scenes. We investigate the robustness of the approach and the use of the learned representations for inference. Furthermore, we demonstrate generality of the approach by applying it to grayscale images, color-vector images, and Gabor-vector images as well as to motion trajectory data for which the extracted components correspond to motion primitives.},
keywords={Annealing;Visualization;Approximation methods;Feature extraction;Optimization;Vectors;Videos},
doi={10.1109/CVPR.2012.6247953},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247954,
author={Murray, Naila and Marchesotti, Luca and Perronnin, Florent},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={AVA: A large-scale database for aesthetic visual analysis},
year={2012},
volume={},
number={},
pages={2408-2415},
abstract={With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.},
keywords={Semantics;Visualization;Communities;Visual databases;Image color analysis;Social network services},
doi={10.1109/CVPR.2012.6247954},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247955,
author={Chua, Jeroen and Givoni, Inmar and Adams, Ryan and Frey, Brendan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning structural element patch models with hierarchical palettes},
year={2012},
volume={},
number={},
pages={2416-2423},
abstract={Image patches can be factorized into `shapelets' that describe segmentation patterns called structural elements (stels), and palettes that describe how to paint the shapelets. We introduce local palettes for patches, global palettes for entire images and universal palettes for image collections. Using a learned shapelet library, patches from a test image can be analyzed using a variational technique to produce an image descriptor that represents local shapes and colors separately. We show that the shapelet model performs better than SIFT, Gist and the standard stel method on Caltech28 and is very competitive with other methods on Caltech101.},
keywords={Image color analysis;Shape;Libraries;Image reconstruction;Indexes;Object recognition;Educational institutions},
doi={10.1109/CVPR.2012.6247955},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247956,
author={Li, Fuxin and Lebanon, Guy and Sminchisescu, Cristian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Chebyshev approximations to the histogram χ2 kernel},
year={2012},
volume={},
number={},
pages={2424-2431},
abstract={The random Fourier embedding methodology can be used to approximate the performance of non-linear kernel classifiers in linear time on the number of training examples. However, there still exists a non-trivial performance gap between the approximation and the nonlinear models, especially for the exponential χ2 kernel, one of the most powerful models for histograms. Based on analogies with Chebyshev polynomials, we propose an asymptotically convergent analytic series of the χ2 measure. The new series removes the need to use periodic approximations to the χ2 function, as typical in previous methods, and improves the classification accuracy when used in the random Fourier approximation of the exponential χ2 kernel. Besides, out-of-core principal component analysis (PCA) methods are introduced to reduce the dimensionality of the approximation and achieve better performance at the expense of only an additional constant factor to the time complexity. Moreover, when PCA is performed jointly on the training and unlabeled testing data, further performance improvements can be obtained. The proposed approaches are tested on the PASCAL VOC 2010 segmentation and the ImageNet ILSVR-C 2010 datasets, and shown to give statistically significant improvements over alternative approximation methods.},
keywords={Kernel;Chebyshev approximation;Radio frequency;Principal component analysis;Vectors;Training},
doi={10.1109/CVPR.2012.6247956},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247957,
author={Lin, Dahua and Fisher, John},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Low level vision via switchable Markov random fields},
year={2012},
volume={},
number={},
pages={2432-2439},
abstract={Markov random fields play a central role in solving a variety of low level vision problems, including denoising, in-painting, segmentation, and motion estimation. Much previous work was based on MRFs with hand-crafted networks, yet the underlying graphical structure is rarely explored. In this paper, we show that if appropriately estimated, the MRF's graphical structure, which captures significant information about appearance and motion, can provide crucial guidance to low level vision tasks. Motivated by this observation, we propose a principled framework to solve low level vision tasks via an exponential family of MRFs with variable structures, which we call Switchable MRFs. The approach explicitly seeks a structure that optimally adapts to the image or video along the pursuit of task-specific goals. Through theoretical analysis and experimental study, we demonstrate that the proposed method addresses a number of drawbacks suffered by previous methods, including failure to capture heavy-tail statistics, computational difficulties, and lack of generality.},
keywords={Computational modeling;Adaptation models;Noise reduction;Inference algorithms;Optical switches},
doi={10.1109/CVPR.2012.6247957},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247958,
author={Tarlow, Daniel and Adams, Ryan P.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Revisiting uncertainty in graph cut solutions},
year={2012},
volume={},
number={},
pages={2440-2447},
abstract={Graph cuts is a popular algorithm for finding the MAP assignment of many large-scale graphical models that are common in computer vision. While graph cuts is powerful, it does not provide information about the marginal probabilities associated with the solution it finds. To assess uncertainty, we are forced to fall back on less efficient and inexact inference algorithms such as loopy belief propagation, or use less principled surrogate representations of uncertainty such as the min-marginal approach of Kohli & Torr [8]. In this work, we give new justification for using min-marginals to compute the uncertainty in conditional random fields, framing the min-marginal outputs as exact marginals under a specially-chosen generative probabilistic model. We leverage this view to learn properly calibrated marginal probabilities as the result of straightforward maximization of the training likelihood, showing that the necessary subgradients can be computed efficiently using dynamic graph cut operations. We also show how this approach can be extended to compute multi-label marginal distributions, where again dynamic graph cuts enable efficient marginal inference and maximum likelihood learning. We demonstrate empirically that - after proper training - uncertainties based on min-marginals provide better-calibrated probabilities than baselines and that these distributions can be exploited in a decision-theoretic way for improved segmentation in low-level vision.},
keywords={Computational modeling;Training;Uncertainty;Inference algorithms;Graphical models;Optimization;Image segmentation},
doi={10.1109/CVPR.2012.6247958},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247959,
author={Saberian, Mohammad J. and Vasconcelos, Nuno},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Boosting algorithms for simultaneous feature extraction and selection},
year={2012},
volume={},
number={},
pages={2448-2455},
abstract={The problem of simultaneous feature extraction and selection, for classifier design, is considered. A new framework is proposed, based on boosting algorithms that can either 1) select existing features or 2) assemble a combination of these features. This framework is simple and mathematically sound, derived from the statistical view of boosting and Taylor series approximations in functional space. Unlike classical boosting, which is limited to linear feature combinations, the new algorithms support more sophisticated combinations of weak learners, such as “sums of products” or “products of sums”. This is shown to enable the design of fairly complex predictor structures with few weak learners in a fully automated manner, leading to faster and more accurate classifiers, based on more informative features. Extensive experiments on synthetic data, UCI datasets, object detection and scene recognition show that these predictors consistently lead to more accurate classifiers than classical boosting algorithms.},
keywords={Boosting;Prediction algorithms;Iron;Feature extraction;Taylor series;Support vector machines;Algorithm design and analysis},
doi={10.1109/CVPR.2012.6247959},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247960,
author={Timofte, Radu and Van Gool, Luc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Iterative Nearest Neighbors for classification and dimensionality reduction},
year={2012},
volume={},
number={},
pages={2456-2463},
abstract={Representative data in terms of a set of selected samples is of interest for various machine learning applications, e.g. dimensionality reduction and classification. The best-known techniques probably still are k-Nearest Neighbors (kNN) and its variants. Recently, richer representations have become popular. Examples are methods based on l1-regularized least squares (Sparse Representation (SR)) or l2-regularized least squares (Collaborative Representation (CR)), or on l1-constrained least squares (Local Linear Embedding (LLE)). We propose Iterative Nearest Neighbors (INN). This is a novel sparse representation that combines the power of SR and LLE with the computational simplicity of kNN. We test our method in terms of dimensionality reduction and classification, using standard benchmarks such as faces (AR), traffic signs (GTSRB), and PASCAL VOC 2007. INN performs better than NN and comparable with CR and SR, while being orders of magnitude faster than the latter.},
keywords={Training;Strontium;Least squares approximation;Symmetric matrices;Collaboration;Approximation algorithms},
doi={10.1109/CVPR.2012.6247960},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247961,
author={Zhang, Hanwang and Zha, Zheng-Jun and Yan, Shuicheng and Wang, Meng and Chua, Tat-Seng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust Non-negative Graph Embedding: Towards noisy data, unreliable graphs, and noisy labels},
year={2012},
volume={},
number={},
pages={2464-2471},
abstract={Non-negative data factorization has been widely used recently. However, existing techniques, such as Non-negative Graph Embedding (NGE), often suffer from noisy data, unreliable graphs, and noisy labels, which are commonly encountered in real-world applications. To address these issues, in this paper, we propose a Robust Non-negative Graph Embedding (RNGE) framework. The joint sparsity in both graph embedding and reconstruction endues the robustness of RNGE. We develop an elegant multiplicative updating solution that can solve RNGE efficiently and prove the convergence rigourously. RNGE is robust to unreliable graphs, as well as both sample and label noises in training data. Moreover, RNGE provides a general formulation such that all the algorithms unified with the graph embedding framework can be easily extended to obtain their robust non-negative solutions. We conduct extensive experiments on four real-world datasets and compared the proposed RNGE to NGE and other representative non-negative data factorization and subspace learning methods. The experimental results demonstrate the effectiveness and robustness of RNGE.},
keywords={Noise measurement;Robustness;Noise;Vectors;Convergence;Algorithm design and analysis;Principal component analysis},
doi={10.1109/CVPR.2012.6247961},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247962,
author={Zhang, Qing and Ye, Mao and Yang, Ruigang and Matsushita, Yasuyuki and Wilburn, Bennett and Yu, Huimin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Edge-preserving photometric stereo via depth fusion},
year={2012},
volume={},
number={},
pages={2472-2479},
abstract={We present a sensor fusion scheme that combines active stereo with photometric stereo. Aiming at capturing full-frame depth for dynamic scenes at a minimum of three lighting conditions, we formulate an iterative optimization scheme that (1) adaptively adjusts the contribution from photometric stereo so that discontinuity can be preserved; (2) detects shadow areas by checking the visibility of the estimated point with respect to the light source, instead of using image-based heuristics; and (3) behaves well for ill-conditioned pixels that are under shadow, which are inevitable in almost any scene. Furthermore, we decompose our non-linear cost function into subproblems that can be optimized efficiently using linear techniques. Experiments show significantly improved results over the previous state-of-the-art in sensor fusion.},
keywords={Optimization;Light sources;Stereo vision;Lighting;Sensor fusion;Image reconstruction;Cameras},
doi={10.1109/CVPR.2012.6247962},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247963,
author={Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Hierarchical face parsing via deep learning},
year={2012},
volume={},
number={},
pages={2480-2487},
abstract={This paper investigates how to parse (segment) facial components from face images which may be partially occluded. We propose a novel face parser, which recasts segmentation of face components as a cross-modality data transformation problem, i.e., transforming an image patch to a label map. Specifically, a face is represented hierarchically by parts, components, and pixel-wise labels. With this representation, our approach first detects faces at both the part- and component-levels, and then computes the pixel-wise label maps (Fig.1). Our part-based and component-based detectors are generatively trained with the deep belief network (DBN), and are discriminatively tuned by logistic regression. The segmentators transform the detected face components to label maps, which are obtained by learning a highly nonlinear mapping with the deep autoencoder. The proposed hierarchical face parsing is not only robust to partial occlusions but also provide richer information for face analysis and face synthesis compared with face keypoint detection and face alignment. The effectiveness of our algorithm is shown through several tasks on 2, 239 images selected from three datasets (e.g., LFW [12], BioID [13] and CUFSF [29]).},
keywords={Face;Detectors;Image segmentation;Shape;Training;Logistics;Robustness},
doi={10.1109/CVPR.2012.6247963},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247964,
author={Whitehill, Jacob and Movellan, Javier},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminately decreasing discriminability with learned image filters},
year={2012},
volume={},
number={},
pages={2488-2495},
abstract={In machine learning and computer vision, input signals are often filtered to increase data discriminability. For example, preprocessing face images with Gabor band-pass filters is known to improve performance in expression recognition tasks [1]. Sometimes, however, one may wish to purposely decrease discriminability of one classification task (a “distractor” task), while simultaneously preserving information relevant to another task (the target task): For example, due to privacy concerns, it may be important to mask the identity of persons contained in face images before submitting them to a crowdsourcing site (e.g., Mechanical Turk) when labeling them for certain facial attributes. Suppressing discriminability in distractor tasks may also be needed to improve inter-dataset generalization: training datasets may sometimes contain spurious correlations between a target attribute (e.g., facial expression) and a distractor attribute (e.g., gender). We might improve generalization to new datasets by suppressing the signal related to the distractor task in the training dataset. This can be seen as a special form of supervised regularization. In this paper we present an approach to automatically learning preprocessing filters that suppress discriminability in distractor tasks while preserving it in target tasks. We present promising results in simulated image classification problems and in a realistic expression recognition problem.},
keywords={Face;Convolution;Vectors;Kernel;Training;Labeling;Measurement},
doi={10.1109/CVPR.2012.6247964},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247965,
author={Wang, Ruiping and Guo, Huimin and Davis, Larry S. and Dai, Qionghai},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Covariance discriminative learning: A natural and efficient approach to image set classification},
year={2012},
volume={},
number={},
pages={2496-2503},
abstract={We propose a novel discriminative learning approach to image set classification by modeling the image set with its natural second-order statistic, i.e. covariance matrix. Since nonsingular covariance matrices, a.k.a. symmetric positive definite (SPD) matrices, lie on a Riemannian manifold, classical learning algorithms cannot be directly utilized to classify points on the manifold. By exploring an efficient metric for the SPD matrices, i.e., Log-Euclidean Distance (LED), we derive a kernel function that explicitly maps the covariance matrix from the Riemannian manifold to a Euclidean space. With this explicit mapping, any learning method devoted to vector space can be exploited in either its linear or kernel formulation. Linear Discriminant Analysis (LDA) and Partial Least Squares (PLS) are considered in this paper for their feasibility for our specific problem. We further investigate the conventional linear subspace based set modeling technique and cast it in a unified framework with our covariance matrix based modeling. The proposed method is evaluated on two tasks: face recognition and object categorization. Extensive experimental results show not only the superiority of our method over state-of-the-art ones in both accuracy and efficiency, but also its stability to two real challenges: noisy set data and varying set size.},
keywords={Covariance matrix;Kernel;Manifolds;Measurement;Vectors;Symmetric matrices;Light emitting diodes},
doi={10.1109/CVPR.2012.6247965},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247966,
author={He, Ran and Tan, Tieniu and Wang, Liang and Zheng, Wei-Shi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={l2, 1 Regularized correntropy for robust feature selection},
year={2012},
volume={},
number={},
pages={2504-2511},
abstract={In this paper, we study the problem of robust feature extraction based on l2,1 regularized correntropy in both theoretical and algorithmic manner. In theoretical part, we point out that an l2,1-norm minimization can be justified from the viewpoint of half-quadratic (HQ) optimization, which facilitates convergence study and algorithmic development. In particular, a general formulation is accordingly proposed to unify l1-norm and l2,1-norm minimization within a common framework. In algorithmic part, we propose an l2,1 regularized correntropy algorithm to extract informative features meanwhile to remove outliers from training data. A new alternate minimization algorithm is also developed to optimize the non-convex correntropy objective. In terms of face recognition, we apply the proposed method to obtain an appearance-based model, called Sparse-Fisherfaces. Extensive experiments show that our method can select robust and sparse features, and outperforms several state-of-the-art subspace methods on largescale and open face recognition datasets.},
keywords={Robustness;Minimization;Face;Vectors;Face recognition;Optimization;Feature extraction},
doi={10.1109/CVPR.2012.6247966},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247967,
author={Lei, Zhen and Yi, Dong and Li, Stan Z.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminant image filter learning for face recognition with local binary pattern like representation},
year={2012},
volume={},
number={},
pages={2512-2517},
abstract={Local binary pattern (LBP) and its variants are effective descriptors for face recognition. The traditional LBP like features are extracted based on the original pixel or patch values of images. In this paper, we propose to learn the discriminative image filter to improve the discriminant power of the LBP like feature. The basic idea is after the image filtering with the learned filter, the difference of pixel difference vectors (PDVs) between the images from the same person is consistent and the difference between the images from different persons is enlarged. In this way, the LBP like features extracted from the filtered images are considered to be more discriminant than those extracted from the original images. Moreover, a coupled discriminant image filters learning method is proposed to deal with the heterogenous face images matching problem by reducing the feature gap between the heterogeneous images. Experiments on FERET, FRGC and a VIS-NIR heterogeneous face databases validate the effectiveness of our proposed image filter learning method combined with LBP like features.},
keywords={Face;Feature extraction;Face recognition;Vectors;Databases;Learning systems;Training},
doi={10.1109/CVPR.2012.6247967},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247968,
author={Huang, Gary B. and Lee, Honglak and Learned-Miller, Erik},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning hierarchical representations for face verification with convolutional deep belief networks},
year={2012},
volume={},
number={},
pages={2518-2525},
abstract={Most modern face recognition systems rely on a feature representation given by a hand-crafted image descriptor, such as Local Binary Patterns (LBP), and achieve improved performance by combining several such representations. In this paper, we propose deep learning as a natural source for obtaining additional, complementary representations. To learn features in high-resolution images, we make use of convolutional deep belief networks. Moreover, to take advantage of global structure in an object class, we develop local convolutional restricted Boltzmann machines, a novel convolutional learning model that exploits the global structure by not assuming stationarity of features across the image, while maintaining scalability and robustness to small misalignments. We also present a novel application of deep learning to descriptors other than pixel intensity values, such as LBP. In addition, we compare performance of networks trained using unsupervised learning against networks with random filters, and empirically show that learning weights not only is necessary for obtaining good multilayer representations, but also provides robustness to the choice of the network architecture parameters. Finally, we show that a recognition system using only representations obtained from deep learning can achieve comparable accuracy with a system using a combination of hand-crafted image descriptors. Moreover, by combining these representations, we achieve state-of-the-art results on a real-world face verification database.},
keywords={Face;Face recognition;Training;Vectors;Accuracy;Measurement;Convolutional codes},
doi={10.1109/CVPR.2012.6247968},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247969,
author={Tamaki, Toru and Yuan, Bingzhi and Harada, Kengo and Raytchev, Bisser and Kaneda, Kazufumi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Linear discriminative image processing operator analysis},
year={2012},
volume={},
number={},
pages={2526-2532},
abstract={In this paper, we propose a method to select a discriminative set of image processing operations for Linear Discriminant Analysis (LDA) as an application of the use of generating matrices representing image processing operators acting on images. First we show that generating matrices can be used for formulating LDA with increasing training samples, then analyze them as image processing operators acting on 2D continuous functions for compressing many large generating matrices by using PCA and Hermite decomposition. Then we propose Linear Discriminative Image Processing Operator Analysis, an iterative method for estimating LDA feature space along with a discriminative set of generating matrices. In experiments, we demonstrate that discriminative generating matrices outperform a non-discriminative set on the ORL and FERET datasets.},
keywords={Matrix decomposition;Image processing;Training;Eigenvalues and eigenfunctions;Principal component analysis;Silicon;Symmetric matrices},
doi={10.1109/CVPR.2012.6247969},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247970,
author={Chen, Xinlei and Tong, Zifei and Liu, Haifeng and Cai, Deng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Metric learning with two-dimensional smoothness for visual analysis},
year={2012},
volume={},
number={},
pages={2533-2538},
abstract={In recent years, metric learning methods based on pairwise side information have attracted considerable interests, and lots of efforts have been devoted to utilize these methods for visual analysis like content based image retrieval and face identification. When applied to image analysis, these methods merely look on an n1 × n2 image as a vector in Rn1×n2 space and the pixels of the image are considered as independent. They fail to consider the fact that an image represented in the plane is intrinsically a matrix, and pixels spatially close to each other may probably be correlated. Even though we have n1 × n2 pixels per image, this spatial correlation suggests the real number of freedom is far less. In this paper, we introduce a regularized metric learning framework, Two-Dimensional Smooth Metric Learning (2DSML), which uses a discretized Laplacian penalty to restrict the coefficients to be two-dimensional smooth. Many existing metric learning algorithms can fit into this framework and learn a spatially smooth metric which is better for image applications than their original version. Recognition, clustering and retrieval can be then performed based on the learned metric. Experimental results on benchmark image datasets demonstrate the effectiveness of our method.},
keywords={Laplace equations;Vectors;Visualization;Face;Smoothing methods;Euclidean distance},
doi={10.1109/CVPR.2012.6247970},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247971,
author={Kapoor, Ashish and Baker, Simon and Basu, Sumit and Horvitz, Eric},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Memory constrained face recognition},
year={2012},
volume={},
number={},
pages={2539-2546},
abstract={Real-time recognition may be limited by scarce memory and computing resources for performing classification. Although, prior research has addressed the problem of training classifiers with limited data and computation, few efforts have tackled the problem of memory constraints on recognition. We explore methods that can guide the allocation of limited storage resources for classifying streaming data so as to maximize discriminatory power. We focus on computation of the expected value of information with nearest neighbor classifiers for online face recognition. Experiments on real-world datasets show the effectiveness and power of the approach. The methods provide a principled approach to vision under bounded resources, and have immediate application to enhancing recognition capabilities in consumer devices with limited memory.},
keywords={Face;Face recognition;Streaming media;Training;Data models;Memory management;Tagging},
doi={10.1109/CVPR.2012.6247971},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247972,
author={Guo, Guodong and Wang, Xiaolong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A study on human age estimation under facial expression changes},
year={2012},
volume={},
number={},
pages={2547-2553},
abstract={In this paper, we study human age estimation in face images under significant expression changes. We will address two issues: (1) Is age estimation affected by facial expression changes and how significant is the influence? (2) How to develop a robust method to perform age estimation undergoing various facial expression changes? This systematic study will not only discover the relation between age estimation and expression changes, but also contribute a robust solution to solve the problem of cross-expression age estimation. This study is an important step towards developing a practical and robust age estimation system that allows users to present their faces naturally (with various expressions) rather than constrained to the neutral expression only. Two databases originally captured in the Psychology community are introduced to Computer Vision, to quantitatively demonstrate the influence of expression changes on age estimation, and evaluate the proposed framework and corresponding methods for cross-expression age estimation.},
keywords={Estimation;Face;Databases;Aging;Correlation;Training;Feature extraction},
doi={10.1109/CVPR.2012.6247972},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247973,
author={Chew, S. W. and Lucey, S. and Lucey, P. and Sridharan, S. and Conn, J. F.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Improved facial expression recognition via uni-hyperplane classification},
year={2012},
volume={},
number={},
pages={2554-2561},
abstract={Large margin learning approaches, such as support vector machines (SVM), have been successfully applied to numerous classification tasks, especially for automatic facial expression recognition. The risk of such approaches however, is their sensitivity to large margin losses due to the influence from noisy training examples and outliers which is a common problem in the area of affective computing (i.e., manual coding at the frame level is tedious so coarse labels are normally assigned). In this paper, we leverage the relaxation of the parallel-hyperplanes constraint and propose the use of modified correlation filters (MCF). The MCF is similar in spirit to SVMs and correlation filters, but with the key difference of optimizing only a single hyperplane. We demonstrate the superiority of MCF over current techniques on a battery of experiments.},
keywords={Support vector machines;Correlation;Databases;Training;Face recognition;Noise measurement;Face},
doi={10.1109/CVPR.2012.6247973},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247974,
author={Zhong, Lin and Liu, Qingshan and Yang, Peng and Liu, Bo and Huang, Junzhou and Metaxas, Dimitris N.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning active facial patches for expression analysis},
year={2012},
volume={},
number={},
pages={2562-2569},
abstract={In this paper, we present a new idea to analyze facial expression by exploring some common and specific information among different expressions. Inspired by the observation that only a few facial parts are active in expression disclosure (e.g., around mouth, eye), we try to discover the common and specific patches which are important to discriminate all the expressions and only a particular expression, respectively. A two-stage multi-task sparse learning (MTSL) framework is proposed to efficiently locate those discriminative patches. In the first stage MTSL, expression recognition tasks, each of which aims to find dominant patches for each expression, are combined to located common patches. Second, two related tasks, facial expression recognition and face verification tasks, are coupled to learn specific facial patches for individual expression. Extensive experiments validate the existence and significance of common and specific patches. Utilizing these learned patches, we achieve superior performances on expression recognition compared to the state-of-the-arts.},
keywords={Face recognition;Feature extraction;Databases;Support vector machines;Facial muscles;Educational institutions},
doi={10.1109/CVPR.2012.6247974},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247975,
author={Li, Changsheng and Liu, Qingshan and Liu, Jing and Lu, Hanqing},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning ordinal discriminative features for age estimation},
year={2012},
volume={},
number={},
pages={2570-2577},
abstract={In this paper, we present a new method for facial age estimation based on ordinal discriminative feature learning. Considering the temporally ordinal and continuous characteristic of aging process, the proposed method not only aims at preserving the local manifold structure of facial images, but also it wants to keep the ordinal information among aging faces. Moreover, we try to remove redundant information from both the locality information and ordinal information as much as possible by minimizing nonlinear correlation and rank correlation. Finally, we formulate these two issues into a unified optimization problem of feature selection and present an efficient solution. The experiments are conducted on the public available Images of Groups dataset and the FG-NET dataset, and the experimental results demonstrate the power of the proposed method against the state-of-the-art methods.},
keywords={Estimation;Aging;Manifolds;Correlation;Training;Optimization;Feature extraction},
doi={10.1109/CVPR.2012.6247975},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247976,
author={Dantone, Matthias and Gall, Juergen and Fanelli, Gabriele and Van Gool, Luc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Real-time facial feature detection using conditional regression forests},
year={2012},
volume={},
number={},
pages={2578-2585},
abstract={Although facial feature detection from 2D images is a well-studied field, there is a lack of real-time methods that estimate feature points even on low quality images. Here we propose conditional regression forest for this task. While regression forest learn the relations between facial image patches and the location of feature points from the entire set of faces, conditional regression forest learn the relations conditional to global face properties. In our experiments, we use the head pose as a global property and demonstrate that conditional regression forests outperform regression forests for facial feature detection. We have evaluated the method on the challenging Labeled Faces in the Wild [20] database where close-to-human accuracy is achieved while processing images in real-time.},
keywords={Facial features;Vegetation;Training;Head;Real time systems;Databases;Accuracy},
doi={10.1109/CVPR.2012.6247976},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247977,
author={Ma, Long and Wang, Chunheng and Xiao, Baihua and Zhou, Wen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Sparse representation for face recognition based on discriminative low-rank dictionary learning},
year={2012},
volume={},
number={},
pages={2586-2593},
abstract={In this paper, we propose a discriminative low-rank dictionary learning algorithm for sparse representation. Sparse representation seeks the sparsest coefficients to represent the test signal as linear combination of the bases in an over-complete dictionary. Motivated by low-rank matrix recovery and completion, assume that the data from the same pattern are linearly correlated, if we stack these data points as column vectors of a dictionary, then the dictionary should be approximately low-rank. An objective function with sparse coefficients, class discrimination and rank minimization is proposed and optimized during dictionary learning. We have applied the algorithm for face recognition. Numerous experiments with improved performances over previous dictionary learning methods validate the effectiveness of the proposed algorithm.},
keywords={Dictionaries;Strontium;Noise;Training;Sparse matrices;Face;Encoding},
doi={10.1109/CVPR.2012.6247977},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247978,
author={Lu, Jiwen and Hu, Junlin and Zhou, Xiuzhuang and Shang, Yuanyuan and Tan, Yap-Peng and Wang, Gang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Neighborhood repulsed metric learning for kinship verification},
year={2012},
volume={},
number={},
pages={2594-2601},
abstract={Kinship verification from facial images is a challenging problem in computer vision, and there is a very few attempts on tackling this problem in the literature. In this paper, we propose a new neighborhood repulsed metric learning (NRML) method for kinship verification. Motivated by the fact that interclass samples (without kinship relations) with higher similarity usually lie in a neighborhood and are more easily misclassified than those with lower similarity, we aim to learn a distance metric under which the intraclass samples (with kinship relations) are pushed as close as possible and interclass samples lying in a neighborhood are repulsed and pulled as far as possible, simultaneously, such that more discriminative information can be exploited for verification. Moreover, we propose a multiview NRM-L (MNRML) method to seek a common distance metric to make better use of multiple feature descriptors to further improve the verification performance. Experimental results are presented to demonstrate the efficacy of the proposed methods.},
keywords={Face;Eigenvalues and eigenfunctions;Optimization;Face recognition;Training;Extraterrestrial measurements},
doi={10.1109/CVPR.2012.6247978},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247979,
author={Haj, Murad Al and Gonzàlez, Jordi and Davis, Larry S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On partial least squares in head pose estimation: How to simultaneously deal with misalignment},
year={2012},
volume={},
number={},
pages={2602-2609},
abstract={Head pose estimation is a critical problem in many computer vision applications. These include human computer interaction, video surveillance, face and expression recognition. In most prior work on heads pose estimation, the positions of the faces on which the pose is to be estimated are specified manually. Therefore, the results are reported without studying the effect of misalignment. We propose a method based on partial least squares (PLS) regression to estimate pose and solve the alignment problem simultaneously. The contributions of this paper are two-fold: 1) we show that the kernel version of PLS (kPLS) achieves better than state-of-the-art results on the estimation problem and 2) we develop a technique to reduce misalignment based on the learned PLS factors.},
keywords={Kernel;Estimation;Face;Vectors;Matrix decomposition;Magnetic heads},
doi={10.1109/CVPR.2012.6247979},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247980,
author={Baltrušaitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={3D Constrained Local Model for rigid and non-rigid facial tracking},
year={2012},
volume={},
number={},
pages={2610-2617},
abstract={We present 3D Constrained Local Model (CLM-Z) for robust facial feature tracking under varying pose. Our approach integrates both depth and intensity information in a common framework. We show the benefit of our CLM-Z method in both accuracy and convergence rates over regular CLM formulation through experiments on publicly available datasets. Additionally, we demonstrate a way to combine a rigid head pose tracker with CLM-Z that benefits rigid head tracking. We show better performance than the current state-of-the-art approaches in head pose tracking with our extension of the generalised adaptive view-based appearance model (GAVAM).},
keywords={Face;Solid modeling;Mathematical model;Training;Equations;Facial features},
doi={10.1109/CVPR.2012.6247980},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247981,
author={Chen, Chih-Fan and Wei, Chia-Po and Wang, Yu-Chiang Frank},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Low-rank matrix recovery with structural incoherence for robust face recognition},
year={2012},
volume={},
number={},
pages={2618-2625},
abstract={We address the problem of robust face recognition, in which both training and test image data might be corrupted due to occlusion and disguise. From standard face recognition algorithms such as Eigenfaces to recently proposed sparse representation-based classification (SRC) methods, most prior works did not consider possible contamination of data during training, and thus the associated performance might be degraded. Based on the recent success of low-rank matrix recovery, we propose a novel low-rank matrix approximation algorithm with structural incoherence for robust face recognition. Our method not only decomposes raw training data into a set of representative basis with corresponding sparse errors for better modeling the face images, we further advocate the structural incoherence between the basis learned from different classes. These basis are encouraged to be as independent as possible due to the regularization on structural incoherence. We show that this provides additional discriminating ability to the original low-rank models for improved performance. Experimental results on public face databases verify the effectiveness and robustness of our method, which is also shown to outperform state-of-the-art SRC based approaches.},
keywords={Sparse matrices;Face recognition;Matrix decomposition;Training;Face;Robustness;Principal component analysis},
doi={10.1109/CVPR.2012.6247981},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247982,
author={Cui, Zhen and Shan, Shiguang and Zhang, Haihong and Lao, Shihong and Chen, Xilin},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image sets alignment for Video-Based Face Recognition},
year={2012},
volume={},
number={},
pages={2626-2633},
abstract={Video-based Face Recognition (VFR) can be converted to the matching of two image sets containing face images captured from each video. For this purpose, we propose to bridge the two sets with a reference image set that is well-defined and pre-structured to a number of local models offline. In other words, given two image sets, as long as each of them is aligned to the reference set, they are mutually aligned and well structured. Therefore, the similarity between them can be computed by comparing only the corresponded local models rather than considering all the pairs. To align an image set with the reference set, we further formulate the problem as a quadratic programming. It integrates three constrains to guarantee robust alignment, including appearance matching cost term exploiting principal angles, geometric structure consistency using affine invariant reconstruction weights, smoothness constraint preserving local neighborhood relationship. Extensive experimental evaluations are performed on three databases: Honda, MoBo and YouTube. Compared with competing methods, our approach can consistently achieve better results.},
keywords={Manifolds;Face;Image matching;Face recognition;Image reconstruction;Video sequences;Computational modeling},
doi={10.1109/CVPR.2012.6247982},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247983,
author={Rudovic, Ognjen and Pavlovic, Vladimir and Pantic, Maja},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-output Laplacian dynamic ordinal regression for facial expression recognition and intensity estimation},
year={2012},
volume={},
number={},
pages={2634-2641},
abstract={Automated facial expression recognition has received increased attention over the past two decades. Existing works in the field usually do not encode either the temporal evolution or the intensity of the observed facial displays. They also fail to jointly model multidimensional (multi-class) continuous facial behaviour data; binary classifiers - one for each target basic-emotion class - are used instead. In this paper, intrinsic topology of multidimensional continuous facial affect data is first modeled by an ordinal manifold. This topology is then incorporated into the Hidden Conditional Ordinal Random Field (H-CORF) framework for dynamic ordinal regression by constraining H-CORF parameters to lie on the ordinal manifold. The resulting model attains simultaneous dynamic recognition and intensity estimation of facial expressions of multiple emotions. To the best of our knowledge, the proposed method is the first one to achieve this on both deliberate as well as spontaneous facial affect data.},
keywords={Manifolds;Hidden Markov models;Face recognition;Data models;Laplace equations;Topology;Standards},
doi={10.1109/CVPR.2012.6247983},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247984,
author={Nguyen, K. and Sridharan, S. and Denman, S. and Fookes, C.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Feature-domain super-resolution framework for Gabor-based face and iris recognition},
year={2012},
volume={},
number={},
pages={2642-2649},
abstract={The low resolution of images has been one of the major limitations in recognising humans from a distance using their biometric traits, such as face and iris. Superresolution has been employed to improve the resolution and the recognition performance simultaneously, however the majority of techniques employed operate in the pixel domain, such that the biometric feature vectors are extracted from a super-resolved input image. Feature-domain superresolution has been proposed for face and iris, and is shown to further improve recognition performance by capitalising on direct super-resolving the features which are used for recognition. However, current feature-domain superresolution approaches are limited to simple linear features such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), which are not the most discriminant features for biometrics. Gabor-based features have been shown to be one of the most discriminant features for biometrics including face and iris. This paper proposes a framework to conduct super-resolution in the non-linear Gabor feature domain to further improve the recognition performance of biometric systems. Experiments have confirmed the validity of the proposed approach, demonstrating superior performance to existing linear approaches for both face and iris biometrics.},
keywords={Iris recognition;Face;Image resolution;Strontium;Noise;Face recognition;Equations},
doi={10.1109/CVPR.2012.6247984},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247985,
author={Zheng, Wei-Shi and Gong, Shaogang and Xiang, Tao},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Transfer re-identification: From person to set-based verification},
year={2012},
volume={},
number={},
pages={2650-2657},
abstract={Solving the person re-identification problem has become important for understanding people's behaviours in a multicamera network of non-overlapping views. In this work, we address the problem of re-identification from a set-based verification perspective. More specifically, we have a small set of target people on a watch list (a set) and we aim to verify whether a query image of a person is on this watch list. This differs from the existing person re-identification problem in that the probe is verified against a small set of known people but requires much higher degree of verification accuracy with very limited sampling data for each candidate in the set. That is, rather than recognising everybody in the scene, we consider identifying a small set of target people against non-target people when there is only a limited number of target training samples and a large number of unlabelled (unknown) non-target samples available. To this end, we formulate a transfer learning framework for mining discriminant information from non-target people data to solve the watch list set verification problem. Based on the proposed approach, we introduce the concepts of multi-shot and one-shot verifications. We also design new criteria for evaluating the performance of the proposed transfer learning method against the i-LIDS and ETHZ data sets.},
keywords={Vectors;Training;Cameras;Watches;Testing;Target recognition;Histograms},
doi={10.1109/CVPR.2012.6247985},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247986,
author={Tapaswi, Makarand and Bäuml, Martin and Stiefelhagen, Rainer},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={“Knock! Knock! Who is it?” probabilistic person identification in TV-series},
year={2012},
volume={},
number={},
pages={2658-2665},
abstract={We describe a probabilistic method for identifying characters in TV series or movies. We aim at labeling every character appearance, and not only those where a face can be detected. Consequently, our basic unit of appearance is a person track (as opposed to a face track). We model each TV series episode as a Markov Random Field, integrating face recognition, clothing appearance, speaker recognition and contextual constraints in a probabilistic manner. The identification task is then formulated as an energy minimization problem. In order to identify tracks without faces, we learn clothing models by adapting available face recognition results. Within a scene, as indicated by prior analysis of the temporal structure of the TV series, clothing features are combined by agglomerative clustering. We evaluate our approach on the first 6 episodes of The Big Bang Theory and achieve an absolute improvement of 20% for person identification and 12% for face recognition.},
keywords={Face;Clothing;TV;Labeling;Face recognition;Feature extraction;Videos},
doi={10.1109/CVPR.2012.6247986},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247987,
author={Mignon, Alexis and Jurie, Frédéric},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={PCCA: A new approach for distance learning from sparse pairwise constraints},
year={2012},
volume={},
number={},
pages={2666-2672},
abstract={This paper introduces Pairwise Constrained Component Analysis (PCCA), a new algorithm for learning distance metrics from sparse pairwise similarity/dissimilarity constraints in high dimensional input space, problem for which most existing distance metric learning approaches are not adapted. PCCA learns a projection into a low-dimensional space where the distance between pairs of data points respects the desired constraints, exhibiting good generalization properties in presence of high dimensional data. The paper also shows how to efficiently kernelize the approach. PCCA is experimentally validated on two challenging vision tasks, face verification and person re-identification, for which we obtain state-of-the-art results.},
keywords={Training;Measurement;Face;Kernel;Histograms;Training data;Vectors},
doi={10.1109/CVPR.2012.6247987},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247988,
author={Simo-Serra, E. and Ramisa, A. and Alenyà, G. and Torras, C. and Moreno-Noguer, F.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Single image 3D human pose estimation from noisy observations},
year={2012},
volume={},
number={},
pages={2673-2680},
abstract={Markerless 3D human pose detection from a single image is a severely underconstrained problem because different 3D poses can have similar image projections. In order to handle this ambiguity, current approaches rely on prior shape models that can only be correctly adjusted if 2D image features are accurately detected. Unfortunately, although current 2D part detector algorithms have shown promising results, they are not yet accurate enough to guarantee a complete disambiguation of the 3D inferred shape. In this paper, we introduce a novel approach for estimating 3D human pose even when observations are noisy. We propose a stochastic sampling strategy to propagate the noise from the image plane to the shape space. This provides a set of ambiguous 3D shapes, which are virtually undistinguishable from their image projections. Disambiguation is then achieved by imposing kinematic constraints that guarantee the resulting pose resembles a 3D human shape. We validate the method on a variety of situations in which state-of-the-art 2D detectors yield either inaccurate estimations or partly miss some of the body parts.},
keywords={Shape;Joints;Detectors;Humans;Space exploration;Covariance matrix;Kinematics},
doi={10.1109/CVPR.2012.6247988},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247989,
author={Poh, Norman and Tistarelli, Massimo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Customizing biometric authentication systems via discriminative score calibration},
year={2012},
volume={},
number={},
pages={2681-2686},
abstract={There is mounting evidence about the benefit of tailoring a biometric authentication system to each user by postprocessing the system output at the score level, also known as client-specific score normalisation. Examples of these procedures are Z-norm and F-norm. These procedures can calibrate the uneven hypothesis space such that the dispropotionate false acceptance and false rejection errors are reduced after the calibration. The interest in studying these schemes is that they are applicable to any biometric authentication system regardless of the underlying biometric modality, and furthermore, potentially be extended to object recognition framed as a verification problem. We propose to further improve these procedures by adding additional client-specific terms that cannot be incorporated easily in their respective existing form. Experiments carried out on 13 face and speech systems show that both variants systematically outperform their respective score normalisation scheme (Z-norm or F-norm).},
keywords={Measurement;Face;Databases;Biometrics;Authentication;Logistics;Speech},
doi={10.1109/CVPR.2012.6247989},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247990,
author={Mishra, Anand and Alahari, Karteek and Jawahar, C. V.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Top-down and bottom-up cues for scene text recognition},
year={2012},
volume={},
number={},
pages={2687-2694},
abstract={Scene text recognition has gained significant attention from the computer vision community in recent years. Recognizing such text is a challenging problem, even more so than the recognition of scanned documents. In this work, we focus on the problem of recognizing text extracted from street images. We present a framework that exploits both bottom-up and top-down cues. The bottom-up cues are derived from individual character detections from the image. We build a Conditional Random Field model on these detections to jointly model the strength of the detections and the interactions between them. We impose top-down cues obtained from a lexicon-based prior, i.e. language statistics, on the model. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We show significant improvements in accuracies on two challenging public datasets, namely Street View Text (over 15%) and ICDAR 2003 (nearly 10%).},
keywords={Text recognition;Character recognition;Support vector machines;Accuracy;Optical character recognition software;Image edge detection},
doi={10.1109/CVPR.2012.6247990},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247991,
author={Xiao, Jianxiong and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Recognizing scene viewpoint using panoramic place representation},
year={2012},
volume={},
number={},
pages={2695-2702},
abstract={We introduce the problem of scene viewpoint recognition, the goal of which is to classify the type of place shown in a photo, and also recognize the observer's viewpoint within that category of place. We construct a database of 360° panoramic images organized into 26 place categories. For each category, our algorithm automatically aligns the panoramas to build a full-view representation of the surrounding place. We also study the symmetry properties and canonical viewpoint of each place category. At test time, given a photo of a scene, the model can recognize the place category, produce a compass-like indication of the observer's most likely viewpoint within that place, and use this information to extrapolate beyond the available view, filling in the probable visual layout that would appear beyond the boundary of the photo.},
keywords={Training;Sun;Layout;Prediction algorithms;Visualization;Observers;Predictive models},
doi={10.1109/CVPR.2012.6247991},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247992,
author={Bao, Sid Yingze and Bagra, Mohit and Chao, Yu-Wei and Savarese, Silvio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Semantic structure from motion with points, regions, and objects},
year={2012},
volume={},
number={},
pages={2703-2710},
abstract={Structure from motion (SFM) aims at jointly recovering the structure of a scene as a collection of 3D points and estimating the camera poses from a number of input images. In this paper we generalize this concept: not only do we want to recover 3D points, but also recognize and estimate the location of high level semantic scene components such as regions and objects in 3D. As a key ingredient for this joint inference problem, we seek to model various types of interactions between scene components. Such interactions help regularize our solution and obtain more accurate results than solving these problems in isolation. Experiments on public datasets demonstrate that: 1) our framework estimates camera poses more robustly than SFM algorithms that use points only; 2) our framework is capable of accurately estimating pose and location of objects, regions, and points in the 3D scene; 3) our framework recognizes objects and regions more accurately than state-of-the-art single image recognition methods.},
keywords={Cameras;Semantics;Estimation;Robustness;Roads;Geometry;Energy measurement},
doi={10.1109/CVPR.2012.6247992},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247993,
author={Kadar, Ilan and Ben-Shahar, Ohad},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Small sample scene categorization from perceptual relations},
year={2012},
volume={},
number={},
pages={2711-2718},
abstract={This paper addresses the problem of scene categorization while arguing that better and more accurate results can be obtained by endowing the computational process with perceptual relations between scene categories. We first describe a psychophysical paradigm that probes human scene categorization, extracts perceptual relations between scene categories, and suggests that these perceptual relations do not always conform the semantic structure between categories. We then incorporate the obtained perceptual findings into a computational classification scheme, which takes inter-class relationships into account to obtain better scene categorization regardless of the particular descriptors with which scenes are represented. We present such improved classification results using several popular descriptors, we discuss why the contribution of inter-class perceptual relations is particularly pronounced for under-sampled training sets, and we argue that this mechanism may explain the ability of the human visual system to perform well under similar conditions. Finally, we introduce an online experimental system for obtaining perceptual relations for large collections of scene categories.},
keywords={Visualization;Training;Humans;Road transportation;Accuracy;Semantics;Complexity theory},
doi={10.1109/CVPR.2012.6247993},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247994,
author={Del Pero, Luca and Bowdish, Joshua and Fried, Daniel and Kermgard, Bonnie and Hartley, Emily and Barnard, Kobus},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Bayesian geometric modeling of indoor scenes},
year={2012},
volume={},
number={},
pages={2719-2726},
abstract={We propose a method for understanding the 3D geometry of indoor environments (e.g. bedrooms, kitchens) while simultaneously identifying objects in the scene (e.g. beds, couches, doors). We focus on how modeling the geometry and location of specific objects is helpful for indoor scene understanding. For example, beds are shorter than they are wide, and are more likely to be in the center of the room than cabinets, which are tall and narrow. We use a generative statistical model that integrates a camera model, an enclosing room “box”, frames (windows, doors, pictures), and objects (beds, tables, couches, cabinets), each with their own prior on size, relative dimensions, and locations. We fit the parameters of this complex, multi-dimensional statistical model using an MCMC sampling approach that combines discrete changes (e.g, adding a bed), and continuous parameter changes (e.g., making the bed larger). We find that introducing object category leads to state-of-the-art performance on room layout estimation, while also enabling recognition based only on geometry.},
keywords={Image edge detection;Cameras;Geometry;Layout;Solid modeling;Object recognition;Catalogs},
doi={10.1109/CVPR.2012.6247994},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247995,
author={Myeong, Heesoo and Chang, Ju Yong and Lee, Kyoung Mu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning object relationships via graph-based context model},
year={2012},
volume={},
number={},
pages={2727-2734},
abstract={In this paper, we propose a novel framework for modeling image-dependent contextual relationships using graph-based context model. This approach enables us to selectively utilize the contextual relationships suitable for an input query image. We introduce a context link view of contextual knowledge, where the relationship between a pair of annotated regions is represented as a context link on a similarity graph of regions. Link analysis techniques are used to estimate the pairwise context scores of all pairs of unlabeled regions in the input image. Our system integrates the learned context scores into a Markov Random Field (MRF) framework in the form of pairwise cost and infers the semantic segmentation result by MRF optimization. Experimental results on object class segmentation show that the proposed graph-based context model outperforms the current state-of-the-art methods.},
keywords={Context;Context modeling;Buildings;Training;Visualization;Roads;Image edge detection},
doi={10.1109/CVPR.2012.6247995},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247996,
author={Li, Congcong and Parikh, Devi and Chen, Tsuhan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Automatic discovery of groups of objects for scene understanding},
year={2012},
volume={},
number={},
pages={2735-2742},
abstract={Objects in scenes interact with each other in complex ways. A key observation is that these interactions manifest themselves as predictable visual patterns in the image. Discovering and detecting these structured patterns is an important step towards deeper scene understanding. It goes beyond using either individual objects or the scene as a whole as the semantic unit. In this work, we promote "groups of objects". They are high-order composites of objects that demonstrate consistent spatial, scale, and viewpoint interactions with each other. These groups of objects are likely to correspond to a specific layout of the scene. They can thus provide cues for the scene category and can also prime the likely locations of other objects in the scene. It is not feasible to manually generate a list of all possible groupings of objects we find in our visual world. Hence, we propose an algorithm that automatically discovers groups of arbitrary numbers of participating objects from a collection of images labeled with object categories. Our approach builds a 4-dimensional transform space of location, scale and viewpoint, and efficiently identifies all recurring compositions of objects across images. We then model the discovered groups of objects using the deformable parts-based model. Our experiments on a variety of datasets show that using groups of objects can significantly boost the performance of object detection and scene categorization.},
keywords={Transforms;Visualization;Deformable models;Clustering algorithms;Object detection;Detectors;Training},
doi={10.1109/CVPR.2012.6247996},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247997,
author={Niu, Zhenxing and Hua, Gang and Gao, Xinbo and Tian, Qi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Context aware topic model for scene recognition},
year={2012},
volume={},
number={},
pages={2743-2750},
abstract={We present a discriminative latent topic model for scene recognition. The capacity of our model is originated from the modeling of two types of visual contexts, i.e., the category specific global spatial layout of different scene elements, and the reinforcement of the visual coherence in uniform local regions. In contrast, most previous methods for scene recognition either only modeled one of these two visual contexts, or just totally ignored both of them. We cast these two coupled visual contexts in a discriminative Latent Dirichlet Allocation framework, namely context aware topic model. Then scene recognition is achieved by Bayesian inference given a target image. Our experiments on several scene recognition benchmarks clearly demonstrated the advantages of the proposed model.},
keywords={Visualization;Context;Context modeling;Layout;Feature extraction;Histograms;Mathematical model},
doi={10.1109/CVPR.2012.6247997},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247998,
author={Patterson, Genevieve and Hays, James},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={SUN attribute database: Discovering, annotating, and recognizing scene attributes},
year={2012},
volume={},
number={},
pages={2751-2758},
abstract={In this paper we present the first large-scale scene attribute database. First, we perform crowd-sourced human studies to find a taxonomy of 102 discriminative attributes. Next, we build the “SUN attribute database” on top of the diverse SUN categorical database. Our attribute database spans more than 700 categories and 14,000 images and has potential for use in high-level scene understanding and fine-grained scene recognition. We use our dataset to train attribute classifiers and evaluate how well these relatively simple classifiers can recognize a variety of attributes related to materials, surface properties, lighting, functions and affordances, and spatial envelope properties.},
keywords={Databases;Sun;Visualization;Training;Accuracy;Taxonomy;Humans},
doi={10.1109/CVPR.2012.6247998},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6247999,
author={Ren, Xiaofeng and Bo, Liefeng and Fox, Dieter},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={RGB-(D) scene labeling: Features and algorithms},
year={2012},
volume={},
number={},
pages={2759-2766},
abstract={Scene labeling research has mostly focused on outdoor scenes, leaving the harder case of indoor scenes poorly understood. Microsoft Kinect dramatically changed the landscape, showing great potentials for RGB-D perception (color+depth). Our main objective is to empirically understand the promises and challenges of scene labeling with RGB-D. We use the NYU Depth Dataset as collected and analyzed by Silberman and Fergus [30]. For RGB-D features, we adapt the framework of kernel descriptors that converts local similarities (kernels) to patch descriptors. For contextual modeling, we combine two lines of approaches, one using a superpixel MRF, and the other using a segmentation tree. We find that (1) kernel descriptors are very effective in capturing appearance (RGB) and shape (D) similarities; (2) both superpixel MRF and segmentation tree are useful in modeling context; and (3) the key to labeling accuracy is the ability to efficiently train and test with large-scale data. We improve labeling accuracy on the NYU Dataset from 56.6% to 76.1%. We also apply our approach to image-only scene labeling and improve the accuracy on the Stanford Background Dataset from 79.4% to 82.9%.},
keywords={Labeling;Kernel;Accuracy;Vegetation;Image color analysis;Context modeling;Image segmentation},
doi={10.1109/CVPR.2012.6247999},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248000,
author={Wang, Liwei and Li, Yin and Jia, Jiaya and Sun, Jian and Wipf, David and Rehg, James M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning sparse covariance patterns for natural scenes},
year={2012},
volume={},
number={},
pages={2767-2774},
abstract={For scene classification, patch-level linear features do not always work as well as handcrafted features. In this paper, we present a new model to greatly improve the usefulness of linear features in classification by introducing co-variance patterns. We analyze their properties, discuss the fundamental importance, and present a generative model to properly utilize them. With this set of covariance information, in our framework, even the most naive linear features that originally lack the vital ability in classification become powerful. Experiments show that the performance of our new covariance model based on linear features is comparable with or even better than handcrafted features in scene classification.},
keywords={Encoding;Correlation;Dictionaries;Computational modeling;Visualization;Vectors;Covariance matrix},
doi={10.1109/CVPR.2012.6248000},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248001,
author={Parizi, Sobhan Naderi and Oberlin, John G. and Felzenszwalb, Pedro F.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Reconfigurable models for scene recognition},
year={2012},
volume={},
number={},
pages={2775-2782},
abstract={We propose a new latent variable model for scene recognition. Our approach represents a scene as a collection of region models (“parts”) arranged in a reconfigurable pattern. We partition an image into a predefined set of regions and use a latent variable to specify which region model is assigned to each image region. In our current implementation we use a bag of words representation to capture the appearance of an image region. The resulting method generalizes a spatial bag of words approach that relies on a fixed model for the bag of words in each image region. Our models can be trained using both generative and discriminative methods. In the generative setting we use the Expectation-Maximization (EM) algorithm to estimate model parameters from a collection of images with category labels. In the discriminative setting we use a latent structural SVM (LSSVM). We note that LSSVMs can be very sensitive to initialization and demonstrate that generative training with EM provides a good initialization for discriminative training with LSSVM.},
keywords={Training;Computational modeling;Visualization;Mathematical model;Zirconium;Support vector machines;Vectors},
doi={10.1109/CVPR.2012.6248001},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248002,
author={Gould, Stephen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multiclass pixel labeling with non-local matching constraints},
year={2012},
volume={},
number={},
pages={2783-2790},
abstract={A popular approach to pixel labeling problems, such as multiclass image segmentation, is to construct a pairwise conditional Markov random field (CRF) over image pixels where the pairwise term encodes a preference for smoothness within local 4-connected or 8-connected pixel neighborhoods. Recently, researchers have considered higher-order models that encode soft non-local constraints (e.g., label consistency, connectedness, or co-occurrence statistics). These new models and the associated energy minimization algorithms have significantly pushed the state-of-the-art for pixel labeling problems. In this paper, we consider a new non-local constraint that penalizes inconsistent pixel labels between disjoint image regions having similar appearance. We encode this constraint as a truncated higher-order matching potential function between pairs of image regions in a conditional Markov random field model and show how to perform efficient approximate MAP inference in the model. We experimentally demonstrate quantitative and qualitative improvements over a strong baseline pairwise conditional Markov random field model on two challenging multiclass pixel labeling datasets.},
keywords={Labeling;Mathematical model;Equations;Markov processes;Approximation methods;Image segmentation;Image color analysis},
doi={10.1109/CVPR.2012.6248002},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248003,
author={Sadovnik, Amir and Chiu, Yi-I and Snavely, Noah and Edelman, Shimon and Chen, Tsuhan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image description with a goal: Building efficient discriminating expressions for images},
year={2012},
volume={},
number={},
pages={2791-2798},
abstract={Many works in computer vision attempt to solve different tasks such as object detection, scene recognition or attribute detection, either separately or as a joint problem. In recent years, there has been a growing interest in combining the results from these different tasks in order to provide a textual description of the scene. However, when describing a scene, there are many items that can be mentioned. If we include all the objects, relationships, and attributes that exist in the image, the description would be extremely long and not convey a true understanding of the image. We present a novel approach to ranking the importance of the items to be described. Specifically, we focus on the task of discriminating one image from a group of others. We investigate the factors that contribute to the most efficient description that achieves this task. We also provide a quantitative method to measure the description quality for this specific task using data from human subjects and show that our method achieves better results than baseline methods.},
keywords={Image color analysis;Visualization;Humans;Databases;Educational institutions;Ovens;Natural languages},
doi={10.1109/CVPR.2012.6248003},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248004,
author={Eigen, David and Fergus, Rob},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Nonparametric image parsing using adaptive neighbor sets},
year={2012},
volume={},
number={},
pages={2799-2806},
abstract={This paper proposes a non-parametric approach to scene parsing inspired by the work of Tighe and Lazebnik [22]. In their approach, a simple kNN scheme with multiple descriptor types is used to classify super-pixels. We add two novel mechanisms: (i) a principled and efficient method for learning per-descriptor weights that minimizes classification error, and (ii) a context-driven adaptation of the training set used for each query, which conditions on common classes (which are relatively easy to classify) to improve performance on rare ones. The first technique helps to remove extraneous descriptors that result from the imperfect distance metrics/representations of each super-pixel. The second contribution re-balances the class frequencies, away from the highly-skewed distribution found in real-world scenes. Both methods give a significant performance boost over [22] and the overall system achieves state-of-the-art performance on the SIFT-Flow dataset.},
keywords={Image segmentation;Training;Context;Measurement;Smoothing methods;Indexes;Histograms},
doi={10.1109/CVPR.2012.6248004},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248005,
author={Hedau, Varsha and Hoiem, Derek and Forsyth, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Recovering free space of indoor scenes from a single image},
year={2012},
volume={},
number={},
pages={2807-2814},
abstract={In this paper we consider the problem of recovering the free space of an indoor scene from its single image. We show that exploiting the box like geometric structure of furniture and constraints provided by the scene, allows us to recover the extent of major furniture objects in 3D. Our “boxy” detector localizes box shaped objects oriented parallel to the scene across different scales and object types, and thus blocks out the occupied space in the scene. To localize the objects more accurately in 3D we introduce a set of specially designed features that capture the floor contact points of the objects. Image based metrics are not very indicative of performance in 3D. We make the first attempt to evaluate single view based occupancy estimates for 3D errors and propose several task driven performance measures towards it. On our dataset of 592 indoor images marked with full 3D geometry of the scene, we show that: (a) our detector works well using image based metrics; (b) our refinement method produces significant improvements in localization in 3D; and (c) if one evaluates using 3D metrics, our method offers major improvements over other single view based scene geometry estimation methods.},
keywords={Image edge detection;Detectors;Feature extraction;Context;Solid modeling;Context modeling;Standards},
doi={10.1109/CVPR.2012.6248005},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248006,
author={Schwing, Alexander G. and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient structured prediction for 3D indoor scene understanding},
year={2012},
volume={},
number={},
pages={2815-2822},
abstract={Existing approaches to indoor scene understanding formulate the problem as a structured prediction task focusing on estimating the 3D bounding box which best describes the scene layout. Unfortunately, these approaches utilize high order potentials which are computationally intractable and rely on ad-hoc approximations for both learning and inference. In this paper we show that the potentials commonly used in the literature can be decomposed into pair-wise potentials by extending the concept of integral images to geometry. As a consequence no heuristic reduction of the search space is required. In practice, this results in large improvements in performance over the state-of-the-art, while being orders of magnitude faster.},
keywords={Layout;Geometry;Random variables;Vectors;Training;Context;Complexity theory},
doi={10.1109/CVPR.2012.6248006},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248007,
author={Floros, Georgios and Leibe, Bastian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Joint 2D-3D temporally consistent semantic segmentation of street scenes},
year={2012},
volume={},
number={},
pages={2823-2830},
abstract={In this paper we propose a novel Conditional Random Field (CRF) formulation for the semantic scene labeling problem which is able to enforce temporal consistency between consecutive video frames and take advantage of the 3D scene geometry to improve segmentation quality. The main contribution of this work lies in the novel use of a 3D scene reconstruction as a means to temporally couple the individual image segmentations, allowing information flow from 3D geometry to the 2D image space. As our results show, the proposed framework outperforms state-of-the-art methods and opens a new perspective towards a tighter interplay of 2D and 3D information in the scene understanding problem.},
keywords={Image segmentation;Semantics;Geometry;Labeling;Visualization;Three dimensional displays;Cities and towns},
doi={10.1109/CVPR.2012.6248007},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248008,
author={Wildenauer, Horst and Hanbury, Allan},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust camera self-calibration from monocular images of Manhattan worlds},
year={2012},
volume={},
number={},
pages={2831-2838},
abstract={We focus on the detection of orthogonal vanishing points using line segments extracted from a single view, and using these for camera self-calibration. Recent methods view this problem as a two-stage process. Vanishing points are extracted through line segment clustering and subsequently likely orthogonal candidates are selected for calibration. Unfortunately, such an approach is easily distracted by the presence of clutter. Furthermore, geometric constraints imposed by the camera and scene orthogonality are not enforced during detection, leading to inaccurate results which are often inadmissible for calibration. To overcome these limitations, we present a RANSAC-based approach using a minimal solution for estimating three orthogonal vanishing points and focal length from a set of four lines, aligned with either two or three orthogonal directions. In addition, we propose to refine the estimates using an efficient and robust Maximum Likelihood Estimator. Extensive experiments on standard datasets show that our contributions result in significant improvements over the state-of-the-art.},
keywords={Cameras;Image segmentation;Calibration;Robustness;Maximum likelihood estimation;Optimization},
doi={10.1109/CVPR.2012.6248008},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248009,
author={Li, Patrick S. and Frey, Brendan J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Factorizing appearance using epitomic flobject analysis},
year={2012},
volume={},
number={},
pages={2839-2846},
abstract={Previously, `flobject analysis' was introduced as a method for using motion or stereo disparity information to train better models of static images. During training, but not during testing, optic flow is used as a cue for factorizing appearance-based image features into those belonging to different flow-defined objects, or flobjects. Here, we describe how the image epitome can be extended to model flobjects and introduce a suitable learning algorithm. Using the CityCars and City F'edestrians datasets, we study the tasks of object classification and localization. Our method performs significantly better than the original LDA-based flobject analysis technique, SIFT-based methods with and without spatial pyramid matching, and gist descriptors.},
keywords={Vectors;Analytical models;Iron;Training;Optical imaging;Labeling;Image segmentation},
doi={10.1109/CVPR.2012.6248009},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248010,
author={Pirsiavash, Hamed and Ramanan, Deva},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Detecting activities of daily living in first-person camera views},
year={2012},
volume={},
number={},
pages={2847-2854},
abstract={We present a novel dataset and novel algorithms for the problem of detecting activities of daily living (ADL) in firstperson camera views. We have collected a dataset of 1 million frames of dozens of people performing unscripted, everyday activities. The dataset is annotated with activities, object tracks, hand positions, and interaction events. ADLs differ from typical actions in that they can involve long-scale temporal structure (making tea can take a few minutes) and complex object interactions (a fridge looks different when its door is open). We develop novel representations including (1) temporal pyramids, which generalize the well-known spatial pyramid to approximate temporal correspondence when scoring a model and (2) composite object models that exploit the fact that objects look different when being interacted with. We perform an extensive empirical evaluation and demonstrate that our novel representations produce a two-fold improvement over traditional approaches. Our analysis suggests that real-world ADL recognition is “all about the objects,” and in particular, “all about the objects being interacted with.”},
keywords={Cameras;Hidden Markov models;Visualization;Detectors;Face;Biomedical monitoring;Taxonomy},
doi={10.1109/CVPR.2012.6248010},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248011,
author={Li, Ruonan and Zickler, Todd},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminative virtual views for cross-view action recognition},
year={2012},
volume={},
number={},
pages={2855-2862},
abstract={We propose an approach for cross-view action recognition by way of `virtual views' that connect the action descriptors extracted from one (source) view to those extracted from another (target) view. Each virtual view is associated with a linear transformation of the action descriptor, and the sequence of transformations arising from the sequence of virtual views aims at bridging the source and target views while preserving discrimination among action categories. Our approach is capable of operating without access to labeled action samples in the target view and without access to corresponding action instances in the two views, and it also naturally incorporate and exploit corresponding instances or partial labeling in the target view when they are available. The proposed approach achieves improved or competitive performance relative to existing methods when instance correspondences or target labels are available, and it goes beyond the capabilities of these methods by providing some level of discrimination even when neither correspondences nor target labels exist.},
keywords={Training;Target recognition;Covariance matrix;Cameras;Feature extraction;Transforms;Vectors},
doi={10.1109/CVPR.2012.6248011},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248012,
author={Hoai, Minh and De la Torre, Fernando},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Max-margin early event detectors},
year={2012},
volume={},
number={},
pages={2863-2870},
abstract={The need for early detection of temporal events from sequential data arises in a wide spectrum of applications ranging from human-robot interaction to video security. While temporal event detection has been extensively studied, early detection is a relatively unexplored problem. This paper proposes a maximum-margin framework for training temporal event detectors to recognize partial events, enabling early detection. Our method is based on Structured Output SVM, but extends it to accommodate sequential data. Experiments on datasets of varying complexity, for detecting facial expressions, hand gestures, and human activities, demonstrate the benefits of our approach. To the best of our knowledge, this is the first paper in the literature of computer vision that proposes a learning formulation for early event detection.},
keywords={Detectors;Training;Time series analysis;Event detection;Humans;Hidden Markov models;Testing},
doi={10.1109/CVPR.2012.6248012},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248013,
author={Zhou, Bolei and Wang, Xiaogang and Tang, Xiaoou},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Understanding collective crowd behaviors: Learning a Mixture model of Dynamic pedestrian-Agents},
year={2012},
volume={},
number={},
pages={2871-2878},
abstract={In this paper, a new Mixture model of Dynamic pedestrian-Agents (MDA) is proposed to learn the collective behavior patterns of pedestrians in crowded scenes. Collective behaviors characterize the intrinsic dynamics of the crowd. From the agent-based modeling, each pedestrian in the crowd is driven by a dynamic pedestrian-agent, which is a linear dynamic system with its initial and termination states reflecting a pedestrian's belief of the starting point and the destination. Then the whole crowd is modeled as a mixture of dynamic pedestrian-agents. Once the model is unsupervisedly learned from real data, MDA can simulate the crowd behaviors. Furthermore, MDA can well infer the past behaviors and predict the future behaviors of pedestrians given their trajectories only partially observed, and classify different pedestrian behaviors in the scene. The effectiveness of MDA and its applications are demonstrated by qualitative and quantitative experiments on the video surveillance dataset collected from the New York Grand Central Station.},
keywords={Trajectory;Computational modeling;Timing;Dynamics;Tracking;Heuristic algorithms;Force},
doi={10.1109/CVPR.2012.6248013},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248014,
author={Zhu, Xiangxin and Ramanan, Deva},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Face detection, pose estimation, and landmark localization in the wild},
year={2012},
volume={},
number={},
pages={2879-2886},
abstract={We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new “in the wild” annotated dataset, that suggests our system advances the state-of-the-art, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com).},
keywords={Face;Computational modeling;Estimation;Shape;Detectors;Face detection;Google},
doi={10.1109/CVPR.2012.6248014},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248015,
author={Cao, Xudong and Wei, Yichen and Wen, Fang and Sun, Jian},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Face alignment by Explicit Shape Regression},
year={2012},
volume={},
number={},
pages={2887-2894},
abstract={We present a very efficient, highly accurate, “Explicit Shape Regression” approach for face alignment. Unlike previous regression-based approaches, we directly learn a vectorial regression function to infer the whole facial shape (a set of facial landmarks) from the image and explicitly minimize the alignment errors over the training data. The inherent shape constraint is naturally encoded into the regressor in a cascaded learning framework and applied from coarse to fine during the test, without using a fixed parametric shape model as in most previous methods. To make the regression more effective and efficient, we design a two-level boosted regression, shape-indexed features and a correlation-based feature selection method. This combination enables us to learn accurate models from large training data in a short time (20 minutes for 2,000 training images), and run regression extremely fast in test (15 ms for a 87 landmarks shape). Experiments on challenging data show that our approach significantly outperforms the state-of-the-art in terms of both accuracy and efficiency.},
keywords={Shape;Training;Face;Testing;Correlation;Training data;Silicon},
doi={10.1109/CVPR.2012.6248015},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248016,
author={Ding, Yuanyuan and Xiao, Jing},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Contextual boost for pedestrian detection},
year={2012},
volume={},
number={},
pages={2895-2902},
abstract={Pedestrian detection from images is an important and yet challenging task. The conventional methods usually identify human figures using image features inside the local regions. In this paper we present that, besides the local features, context cues in the neighborhood provide important constraints that are not yet well utilized. We propose a framework to incorporate the context constraints for detection. First, we combine the local window with neighborhood windows to construct a multi-scale image context descriptor, designed to represent the contextual cues in spatial, scaling, and color spaces. Second, we develop an iterative classification algorithm called contextual boost. At each iteration, the classifier responses from the previous iteration across the neighborhood and multiple image scales, called classification context, are incorporated as additional features to learn a new classifier. The number of iterations is determined in the training process when the error rate converges. Since the classification context incorporates contextual cues from the neighborhood, through iterations it implicitly propagates to greater areas and thus provides more global constraints. We evaluate our method on the Caltech benchmark dataset [11]. The results confirm the advantages of the proposed framework. Compared with state of the arts, our method reduces the miss rate from 29% by [30] to 25% at 1 false positive per image (FPPI).},
keywords={Context;Feature extraction;Humans;Training;Shape;Image color analysis;Detectors},
doi={10.1109/CVPR.2012.6248016},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248017,
author={Benenson, Rodrigo and Mathias, Markus and Timofte, Radu and Van Gool, Luc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Pedestrian detection at 100 frames per second},
year={2012},
volume={},
number={},
pages={2903-2910},
abstract={We present a new pedestrian detector that improves both in speed and quality over state-of-the-art. By efficiently handling different scales and transferring computation from test time to training time, detection speed is improved. When processing monocular images, our system provides high quality detections at 50 fps. We also propose a new method for exploiting geometric context extracted from stereo images. On a single CPU+GPU desktop machine, we reach 135 fps, when processing street scenes, from rectified input to detections output.},
keywords={Detectors;Training;Graphics processing unit;Object detection;Feature extraction;Decision trees;Gold},
doi={10.1109/CVPR.2012.6248017},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248018,
author={Arandjelović, Relja and Zisserman, Andrew},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Three things everyone should know to improve object retrieval},
year={2012},
volume={},
number={},
pages={2911-2918},
abstract={The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improvement of the image augmentation method proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of standard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate retrieval speeds. Combining these complementary methods achieves a new state-of-the-art performance on these datasets.},
keywords={Vectors;Visualization;Kernel;Standards;Support vector machines;Indexes;Euclidean distance},
doi={10.1109/CVPR.2012.6248018},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248019,
author={Wang, Hua and Nie, Feiping and Huang, Heng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust and discriminative distance for Multi-Instance Learning},
year={2012},
volume={},
number={},
pages={2919-2924},
abstract={Multi-Instance Learning (MIL) is an emerging topic in machine learning, which has broad applications in computer vision. For example, by considering video classification as a MIL problem where we only need labeled video clips (such as tagged online videos) but not labeled video frames, one can lower down the labeling cost, which is typically very expensive. We propose a novel class specific distance Metrics enhanced Class-to-Bag distance (M-C2B) method to learn a robust and discriminative distance for multi-instance data, which employs the not-squared ℓ2-norm distance to address the most difficult challenge in MIL, i.e., the outlier instances that abound in multi-instance data by nature. As a result, the formulated objective ends up to be a simultaneous ℓ2, 1-norm minimization and maximization (minmax) problem, which is very hard to solve in general due to the non-smoothness of the ℓ2, 1-norm. We thus present an efficient iterative algorithm to solve the general ℓ2, 1-norm minmax problem with rigorously proved convergence. To the best of our knowledge, we are the first to solve a general ℓ2, 1-norm minmax problem in literature. We have conducted extensive experiments to evaluate various aspects of the proposed method, in which promising results validate our new method in cost-effective video classification.},
keywords={Measurement;Robustness;Training;Algorithm design and analysis;Machine learning;Labeling;Iterative methods},
doi={10.1109/CVPR.2012.6248019},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248020,
author={Ji, Rongrong and Duan, Ling-Yu and Chen, Jie and Gao, Wen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Towards compact topical descriptors},
year={2012},
volume={},
number={},
pages={2925-2932},
abstract={We introduce a Compact Topical Descriptor to learn a compact yet discriminative image signature from the reference image corpus. This descriptor is deployed over the well used bag-of-words image histogram, with two merits over the traditional topical features: First, we propose to directly control the topical sparsity to achieve the descriptor compactness. Second, we ensure the descriptor discriminability by minimizing the bag-of-words reconstruction errors during the topical histogram encoding. To this end, we have a generative viewpoint of the topical feature extraction, which is estimated as a sparse MAP estimation over the original bag-of-words. We learn such estimation by a bi-convex optimization, iterating between both hierarchical sparse coding from words to topical histograms and dictionary learning of the corresponding word-to-topic transform. Especially, supervised labels such as image ranking list can be also incorporated into our descriptor learning paradigm. We quantize our performance in both Im-ageNet 10K and NUS-WIDE, with comparisons to bag-of-words, LDA, miniBoF, and Aggregated Local Descriptors. In practice, we also implement our descriptor for a low bit rate mobile visual search application, i.e. sending compact descriptors instead of the image to reduce the query delivery latency. Our descriptor has significantly outperformed the state-of-the-art compact descriptors by quantitative evaluations over 10 million reference images.},
keywords={Visualization;Equations;Mobile communication;Mathematical model;Dictionaries;Histograms;Image coding},
doi={10.1109/CVPR.2012.6248020},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248021,
author={Scheirer, Walter J. and Kumar, Neeraj and Belhumeur, Peter N. and Boult, Terrance E.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-attribute spaces: Calibration for attribute fusion and similarity search},
year={2012},
volume={},
number={},
pages={2933-2940},
abstract={Recent work has shown that visual attributes are a powerful approach for applications such as recognition, image description and retrieval. However, fusing multiple attribute scores - as required during multi-attribute queries or similarity searches - presents a significant challenge. Scores from different attribute classifiers cannot be combined in a simple way; the same score for different attributes can mean different things. In this work, we show how to construct normalized “multi-attribute spaces” from raw classifier outputs, using techniques based on the statistical Extreme Value Theory. Our method calibrates each raw score to a probability that the given attribute is present in the image. We describe how these probabilities can be fused in a simple way to perform more accurate multiattribute searches, as well as enable attribute-based similarity searches. A significant advantage of our approach is that the normalization is done after-the-fact, requiring neither modification to the attribute classification system nor ground truth attribute annotations. We demonstrate results on a large data set of nearly 2 million face images and show significant improvements over prior work. We also show that perceptual similarity of search results increases by using contextual attributes.},
keywords={Face;Support vector machines;Hair;Calibration;Context;Hypercubes;Extraterrestrial measurements},
doi={10.1109/CVPR.2012.6248021},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248022,
author={von Hundelshausen, Felix and Sukthankar, Rahul},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={D-Nets: Beyond patch-based image descriptors},
year={2012},
volume={},
number={},
pages={2941-2948},
abstract={Despite much research on patch-based descriptors, SIFT remains the gold standard for finding correspondences across images and recent descriptors focus primarily on improving speed rather than accuracy. In this paper we propose Descriptor-Nets (D-Nets), a computationally efficient method that significantly improves the accuracy of image matching by going beyond patch-based approaches. D-Nets constructs a network in which nodes correspond to traditional sparsely or densely sampled keypoints, and where image content is sampled from selected edges in this net. Not only is our proposed representation invariant to cropping, translation, scale, reflection and rotation, but it is also significantly more robust to severe perspective and non-linear distortions. We present several variants of our algorithm, including one that tunes itself to the image complexity and an efficient parallelized variant that employs a fixed grid. Comprehensive direct comparisons against SIFT and ORB on standard datasets demonstrate that D-Nets dominates existing approaches in terms of precision and recall while retaining computational efficiency.},
keywords={Strips;Image matching;Robustness;Standards;Image edge detection;Accuracy;Indexes},
doi={10.1109/CVPR.2012.6248022},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248023,
author={Yu, Felix X. and Ji, Rongrong and Tsai, Ming-Hen and Ye, Guangnan and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Weak attributes for large-scale image retrieval},
year={2012},
volume={},
number={},
pages={2949-2956},
abstract={Attribute-based query offers an intuitive way of image retrieval, in which users can describe the intended search targets with understandable attributes. In this paper, we develop a general and powerful framework to solve this problem by leveraging a large pool of weak attributes comprised of automatic classifier scores or other mid-level representations that can be easily acquired with little or no human labor. We extend the existing retrieval model of modeling dependency within query attributes to modeling dependency of query attributes on a large pool of weak attributes, which is more expressive and scalable. To efficiently learn such a large dependency model without overfitting, we further propose a semi-supervised graphical model to map each multiattribute query to a subset of weak attributes. Through extensive experiments over several attribute benchmarks, we demonstrate consistent and significant performance improvements over the state-of-the-art techniques. In addition, we compile the largest multi-attribute image retrieval dateset to date, including 126 fully labeled query attributes and 6,000 weak attributes of 0.26 million images.},
keywords={Graphical models;Mathematical model;Yttrium;Image retrieval;Training;Equations;Humans},
doi={10.1109/CVPR.2012.6248023},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248024,
author={Heo, Jae-Pil and Lee, Youngwoon and He, Junfeng and Chang, Shih-Fu and Yoon, Sung-Eui},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Spherical hashing},
year={2012},
volume={},
number={},
pages={2957-2964},
abstract={Many binary code encoding schemes based on hashing have been actively studied recently, since they can provide efficient similarity search, especially nearest neighbor search, and compact data representations suitable for handling large scale image databases in many computer vision problems. Existing hashing techniques encode high-dimensional data points by using hyperplane-based hashing functions. In this paper we propose a novel hypersphere-based hashing function, spherical hashing, to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions. Furthermore, we propose a new binary code distance function, spherical Hamming distance, that is tailored to our hypersphere-based binary coding scheme, and design an efficient iterative optimization process to achieve balanced partitioning of data points for each hash function and independence between hashing functions. Our extensive experiments show that our spherical hashing technique significantly outperforms six state-of-the-art hashing techniques based on hyperplanes across various image benchmarks of sizes ranging from one to 75 million of GIST descriptors. The performance gains are consistent and large, up to 100% improvements. The excellent results confirm the unique merits of the proposed idea in using hyperspheres to encode proximity regions in high-dimensional spaces. Finally, our method is intuitive and easy to implement.},
keywords={Binary codes;Optimization;Force;Hamming distance;Measurement;Indexing;Image coding},
doi={10.1109/CVPR.2012.6248024},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248025,
author={Le, Duy-Dinh and Satoh, Shin'ichi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Auto face re-ranking by mining the web and video archives},
year={2012},
volume={},
number={},
pages={2965-2972},
abstract={It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing. One popular approach has been to learn visual consistency between images returned by these search engines. Most state-of-the-art methods of learning visual consistency usually learn one specific classifier for each query to re-rank the returned images. The main drawback with these query-specific based methods is that they require computational cost and processing time that are unsuitable for handling a large number of queries. Another approach has been to learn one generic classifier once and then use for all queries. Pursuing the generic classifier based approach, we study the problem of re-ranking faces returned by existing search engines to improve retrieval performance. Learning a generic classifier involves finding good query-dependent feature representation and collecting sufficient large number of training samples. Existing work [9, 15] studies query-dependent features for general objects rather than faces. In addition, training samples are usually collected manually. The key contribution of this research is to introduce a query-dependent feature for faces and an unsupervised method of automatically collecting training samples to learn the generic classifier. The experimental results demonstrated that the proposed method performed very well in various datasets.},
keywords={Face;Visualization;Search engines;Training;Feature extraction;Engines;Vectors},
doi={10.1109/CVPR.2012.6248025},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248026,
author={Kovashka, Adriana and Parikh, Devi and Grauman, Kristen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={WhittleSearch: Image search with relative attribute feedback},
year={2012},
volume={},
number={},
pages={2973-2980},
abstract={We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image(s) sought. For example, perusing image results for a query “black shoes”, the user might state, “Show me shoe images like these, but sportier.” Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (`sportiness', `furriness', etc.). At query time, the system presents an initial set of reference images, and the user selects among them to provide relative attribute feedback. Using the resulting constraints in the multi-dimensional attribute space, our method updates its relevance function and re-ranks the pool of images. This procedure iterates using the accumulated constraints until the top ranked images are acceptably close to the user's envisioned target. In this way, our approach allows a user to efficiently “whittle away” irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate the technique for refining image search for people, products, and scenes, and show it outperforms traditional binary relevance feedback in terms of search speed and accuracy.},
keywords={Footwear;Visualization;Training;Semantics;Image color analysis;Humans;Cognitive science},
doi={10.1109/CVPR.2012.6248026},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248027,
author={Han, Yahong and Wu, Fei and Shao, Jian and Tian, Qi and Zhuang, Yueting},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Graph-guided sparse reconstruction for region tagging},
year={2012},
volume={},
number={},
pages={2981-2988},
abstract={Many of contextual correlations co-exist within the segmented regions among images, like the visual context and semantic context. The appropriate integration and utilization of such contexts are very important to boost the performance of region tagging. Inspired by the recent advances of sparse reconstruction methods, this paper proposes an approach, called Graph-Guided Sparse Reconstruction for Region Tagging (G2SRRT). The G2SRRT consists of two steps: sparse reconstruction for testing regions and tag propagation from training regions to testing regions. In G2SRRT, graph is conducted to flexibly model the contextual correlations among regions. To integrate the graph structure learned from training regions into the sparse reconstruction, we define a Graph-Guided Fusion (G2F) penalty over the graph to encourage the sparsity of differences between two reconstruction coefficients, which corresponds to the linked regions in the graph. Guided by this G2F penalty, the highly correlated regions tend to be jointly selected for the reconstruction, which results in a better performance of region tagging. Experiments on three open benchmark image datasets demonstrate the effectiveness of the proposed algorithm.},
keywords={Image reconstruction;Tagging;Training;Visualization;Correlation;Semantics;Testing},
doi={10.1109/CVPR.2012.6248027},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248028,
author={Póczos, Barnabás and Xiong, Liang and Sutherland, Dougal J. and Schneider, Jeff},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Nonparametric kernel estimators for image classification},
year={2012},
volume={},
number={},
pages={2989-2996},
abstract={We introduce a new discriminative learning method for image classification. We assume that the images are represented by unordered, multi-dimensional, finite sets of feature vectors, and that these sets might have different cardinality. This allows us to use consistent nonparametric divergence estimators to define new kernels over these sets, and then apply them in kernel classifiers. Our numerical results demonstrate that in many cases this approach can outperform state-of-the-art competitors on both simulated and challenging real-world datasets.},
keywords={Kernel;Histograms;Feature extraction;Vectors;Estimation;Symmetric matrices;Support vector machines},
doi={10.1109/CVPR.2012.6248028},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248029,
author={Wang, Bo and Jiang, Jiayan and Wang, Wei and Zhou, Zhi-Hua and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised metric fusion by cross diffusion},
year={2012},
volume={},
number={},
pages={2997-3004},
abstract={Metric learning is n fundamental problem in computer vision. Different features and algorithms may tackle a problem from different angles, and thus often provide complementary information. In this paper; we propose a fusion algorithm which outputs enhanced metrics by combining multiple given metrics (similarity measures). Unlike traditional co-training style algorithms where multi-view features or multiple data subsets are used for classification or regression, we focus on fusing multiple given metrics through diffusion process in an unsupervised way. Our algorithm has its particular advantage when the input similarity' matrices are the outputs from diverse algorithms. We provide both theoretical and empirical explanations to our method. Significant improvements over the state-of-the-art results have been observed on various benchmark datasets. For example, we have achieved 100% accuracy (no longer the bull's eye measure) on the MPEG-7 shape dataset. Our method has a wide range of applications in machine learning and computer vision.},
keywords={Measurement;Shape;Accuracy;Diffusion processes;Manifolds;Kernel;Transform coding},
doi={10.1109/CVPR.2012.6248029},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248030,
author={He, Junfeng and Feng, Jinyuan and Liu, Xianglong and Cheng, Tao and Lin, Tai-Hsu and Chung, Hyunjin and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Mobile product search with Bag of Hash Bits and boundary reranking},
year={2012},
volume={},
number={},
pages={3005-3012},
abstract={Rapidly growing applications on smartphones have provided an excellent platform for mobile visual search. Most of previous visual search systems adopt the framework of ”Bag of Words”, in which words indicate quantized codes of visual features. In this work, we propose a novel visual search system based on ”Bag of Hash Bits” (BoHB), in which each local feature is encoded to a very small number of hash bits, instead of quantized to visual words, and the whole image is represented as bag of hash bits. The proposed BoHB method offers unique benefits in solving the challenges associated with mobile visual search, e.g., low transmission cost, cheap memory and computation on the mobile side, etc. Moreover, our BoHB method leverages the distinct properties of hashing bits such as multi-table indexing, multiple bucket probing, bit reuse, and hamming distance based ranking to achieve efficient search over gigantic visual databases. The proposed method significantly outperforms state-of-the-art mobile visual search methods like CHoG, and other (conventional desktop) visual search approaches like bag of words via vocabulary tree, or product quantization. The proposed BoHB approach is easy to implement on mobile devices, and general in the sense that it can be applied to different types of local features, hashing algorithms and image databases. We also incorporate a boundary feature in the reranking step to describe the object shapes, complementing the local features that are usually used to characterize the local details. The boundary feature can further filter out noisy results and improve the search performance, especially at the coarse category level. Extensive experiments over large-scale data sets up to 400k product images demonstrate the effectiveness of our approach.},
keywords={Mobile communication;Visualization;Feature extraction;Hamming distance;Servers;Indexes},
doi={10.1109/CVPR.2012.6248030},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248031,
author={Shen, Xiaohui and Lin, Zhe and Brandt, Jonathan and Avidan, Shai and Wu, Ying},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Object retrieval and localization with spatially-constrained similarity measure and k-NN re-ranking},
year={2012},
volume={},
number={},
pages={3013-3020},
abstract={One fundamental problem in object retrieval with the bag-of-visual words (BoW) model is its lack of spatial information. Although various approaches are proposed to incorporate spatial constraints into the BoW model, most of them are either too strict or too loose so that they are only effective in limited cases. We propose a new spatially-constrained similarity measure (SCSM) to handle object rotation, scaling, view point change and appearance deformation. The similarity measure can be efficiently calculated by a voting-based method using inverted files. Object retrieval and localization are then simultaneously achieved without post-processing. Furthermore, we introduce a novel and robust re-ranking method with the k-nearest neighbors of the query for automatically refining the initial search results. Extensive performance evaluations on six public datasets show that SCSM significantly outperforms other spatial models, while k-NN re-ranking outperforms most state-of-the-art approaches using query expansion.},
keywords={Databases;Visualization;Nickel;Search problems;Quantization;Robustness;Vocabulary},
doi={10.1109/CVPR.2012.6248031},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248032,
author={Ye, Guangnan and Liu, Dong and Jhuo, I-Hong and Chang, Shih-Fu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Robust late fusion with rank minimization},
year={2012},
volume={},
number={},
pages={3021-3028},
abstract={In this paper, we propose a rank minimization method to fuse the predicted confidence scores of multiple models, each of which is obtained based on a certain kind of feature. Specifically, we convert each confidence score vector obtained from one model into a pairwise relationship matrix, in which each entry characterizes the comparative relationship of scores of two test samples. Our hypothesis is that the relative score relations are consistent among component models up to certain sparse deviations, despite the large variations that may exist in the absolute values of the raw scores. Then we formulate the score fusion problem as seeking a shared rank-2 pairwise relationship matrix based on which each original score matrix from individual model can be decomposed into the common rank-2 matrix and sparse deviation errors. A robust score vector is then extracted to fit the recovered low rank score relation matrix. We formulate the problem as a nuclear norm and ℓ1 norm optimization objective function and employ the Augmented Lagrange Multiplier (ALM) method for the optimization. Our method is isotonic (i.e., scale invariant) to the numeric scales of the scores originated from different models. We experimentally show that the proposed method achieves significant performance gains on various tasks including object categorization and video event detection.},
keywords={Sparse matrices;Matrix decomposition;Robustness;Vectors;Kernel;Predictive models;Optimization},
doi={10.1109/CVPR.2012.6248032},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248033,
author={Lu, Junyang and Zhou, Jiazhen and Wang, Jingdong and Mei, Tao and Hua, Xian-Sheng and Li, Shipeng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Image search results refinement via outlier detection using deep contexts},
year={2012},
volume={},
number={},
pages={3029-3036},
abstract={Visual reranking has become a widely-accepted method to improve traditional text-based image search results. The main principle is to exploit the visual aggregation property of relevant images among top results so as to boost ranking scores of relevant images, by explicitly or implicitly detecting the confident relevant images, and propagating ranking scores among visually similar images. However, such a visual aggregation property does not always hold, and thus these schemes may fail. In this paper, we instead propose to filter out the most probable irrelevant images using deep contexts, which is the extra information that is not limited in the current search results. The deep contexts for each image consist of sets of images that are returned by searches using the queries formed by the textual context of this image. We compare the popularity of this image in the current search results and the deep contexts to check the irrelevance score. Then the irrelevance scores are propagated to the images whose useful textual context is missed. We formulate the two schemes together to reach a Markov random field, which is effectively solved by graph cuts. The key is that our scheme does not rely on the assumption that relevant images are visually aggregated among top results and is based on the observation that an outlier under the current query is likely to be more popular under some other query. After that, we perform graph reranking over filtered results to reorder them. Experimental results on the INRIA dataset show that our proposed method achieves significant improvements over previous approaches.},
keywords={Context;Visualization;Image edge detection;Equations;Markov processes;Humans;Support vector machines},
doi={10.1109/CVPR.2012.6248033},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248034,
author={Wang, Jing and Wang, Jingdong and Ke, Qifa and Zeng, Gang and Li, Shipeng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast approximate k-means via cluster closures},
year={2012},
volume={},
number={},
pages={3037-3044},
abstract={K-means, a simple and effective clustering algorithm, is one of the most widely used algorithms in computer vision community. Traditional k-means is an iterative algorithm - in each iteration new cluster centers are computed and each data point is re-assigned to its nearest center. The cluster re-assignment step becomes prohibitively expensive when the number of data points and cluster centers are large. In this paper, we propose a novel approximate k-means algorithm to greatly reduce the computational complexity in the assignment step. Our approach is motivated by the observation that most active points changing their cluster assignments at each iteration are located on or near cluster boundaries. The idea is to efficiently identify those active points by pre-assembling the data into groups of neighboring points using multiple random spatial partition trees, and to use the neighborhood information to construct a closure for each cluster, in such a way only a small number of cluster candidates need to be considered when assigning a data point to its nearest cluster. Using complexity analysis, real data clustering, and applications to image retrieval, we show that our approach out-performs state-of-the-art approximate k-means algorithms in terms of clustering quality and efficiency.},
keywords={Clustering algorithms;Vegetation;Image retrieval;Visualization;Complexity theory;Algorithm design and analysis;Instruction sets},
doi={10.1109/CVPR.2012.6248034},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248035,
author={Gordoa, Albert and Rodríguez-Serrano, José A. and Perronnin, Florent and Valveny, Ernest},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Leveraging category-level labels for instance-level image retrieval},
year={2012},
volume={},
number={},
pages={3045-3052},
abstract={In this article, we focus on the problem of large-scale instance-level image retrieval. For efficiency reasons, it is common to represent an image by a fixed-length descriptor which is subsequently encoded into a small number of bits. We note that most encoding techniques include an unsupervised dimensionality reduction step. Our goal in this work is to learn a better subspace in a supervised manner. We especially raise the following question: "can category-level labels be used to learn such a subspace?" To answer this question, we experiment with four learning techniques: the first one is based on a metric learning framework, the second one on attribute representations, the third one on Canonical Correlation Analysis (CCA) and the fourth one on Joint Subspace and Classifier Learning (JSCL). While the first three approaches have been applied in the past to the image retrieval problem, we believe we are the first to show the usefulness of JSCL in this context. In our experiments, we use ImageNet as a source of category-level labels and report retrieval results on two standard dataseis: INRIA Holidays and the University of Kentucky benchmark. Our experimental study shows that metric learning and attributes do not lead to any significant improvement in retrieval accuracy, as opposed to CCA and JSCL. As an example, we report on Holidays an increase in accuracy from 39.3% to 48.6% with 32-dimensional representations. Overall JSCL is shown to yield the best results.},
keywords={Measurement;Vectors;Principal component analysis;Accuracy;Encoding;Correlation;Image retrieval},
doi={10.1109/CVPR.2012.6248035},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248036,
author={Hwang, Yoonho and Han, Bohyung and Ahn, Hee-Kap},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A fast nearest neighbor search algorithm by nonlinear embedding},
year={2012},
volume={},
number={},
pages={3053-3060},
abstract={We propose an efficient algorithm to find the exact nearest neighbor based on the Euclidean distance for large-scale computer vision problems. We embed data points nonlinearly onto a low-dimensional space by simple computations and prove that the distance between two points in the embedded space is bounded by the distance in the original space. Instead of computing the distances in the high-dimensional original space to find the nearest neighbor, a lot of candidates are to be rejected based on the distances in the low-dimensional embedded space; due to this property, our algorithm is well-suited for high-dimensional and large-scale problems. We also show that our algorithm is improved further by partitioning input vectors recursively. Contrary to most of existing fast nearest neighbor search algorithms, our technique reports the exact nearest neighbor - not an approximate one - and requires a very simple preprocessing with no sophisticated data structures. We provide the theoretical analysis of our algorithm and evaluate its performance in synthetic and real data.},
keywords={Vectors;Nearest neighbor searches;Euclidean distance;Partitioning algorithms;Approximation algorithms;Computer vision;Data structures},
doi={10.1109/CVPR.2012.6248036},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248037,
author={Stöttinger, Julian and Uijlings, Jasper R. R. and Pandey, Anand K. and Sebe, Nicu and Giunchiglia, Fausto},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={(Unseen) event recognition via semantic compositionality},
year={2012},
volume={},
number={},
pages={3061-3068},
abstract={Since high-level events in images (e.g. “dinner”, “motorcycle stunt”, etc.) may not be directly correlated with their visual appearance, low-level visual features do not carry enough semantics to classify such events satisfactorily. This paper explores a fully compositional approach for event based image retrieval which is able to overcome this shortcoming. Furthermore, the approach is fully scalable in both adding new events and new primitives. Using the Pascal VOC 2007 dataset, our contributions are the following: (i) We apply the Faceted Analysis-Synthesis Theory (FAST) to build a hierarchy of 228 high-level events. (ii) We show that rule-based classifiers are better suited for compositional recognition of events than SVMs. In addition, rule-based classifiers provide semantically meaningful event descriptions which help bridging the semantic gap. (iii) We demonstrate that compositionality enables unseen event recognition: we can use rules learned from non-visual cues, together with object detectors to get reasonable performance on unseen event categories.},
keywords={Motorcycles;Semantics;Visualization;Detectors;Training;Support vector machines;Humans},
doi={10.1109/CVPR.2012.6248037},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248038,
author={Babenko, Artem and Lempitsky, Victor},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={The inverted multi-index},
year={2012},
volume={},
number={},
pages={3069-3076},
abstract={A new data structure for efficient similarity search in very large dataseis of high-dimensional vectors is introduced. This structure called the inverted multi-index generalizes the inverted index idea by replacing the standard quantization within inverted indices with product quantization. For very similar retrieval complexity and preprocessing time, inverted multi-indices achieve a much denser subdivision of the search space compared to inverted indices, while retaining their memory efficiency. Our experiments with large dataseis of SIFT and GIST vectors demonstrate that because of the denser subdivision, inverted multi-indices are able to return much shorter candidate lists with higher recall. Augmented with a suitable reranking procedure, multi-indices were able to improve the speed of approximate nearest neighbor search on the dataset of 1 billion SIFT vectors by an order of magnitude compared to the best previously published systems, while achieving better recall and incurring only few percent of memory overhead.},
keywords={},
doi={10.1109/CVPR.2012.6248038},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248039,
author={Chum, Ondřej and Matas, Jiří},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast computation of min-Hash signatures for image collections},
year={2012},
volume={},
number={},
pages={3077-3084},
abstract={A new method for highly efficient min-Hash generation for document collections is proposed. It exploits the inverted file structure which is available in many applications based on a bag or a set of words. Fast min-Hash generation is important in applications such as image clustering where good recall and precision requires a large number of min-Hash signatures. Using the set of words represenation, the novel exact min-Hash generation algorithm achieves approximately a 50-fold speed-up on two dataset with 105 and 106 images respectively. We also propose an approximate min-Hash assignment process which reaches a more than 200-fold speed-up at the cost of missing about 2-3% of matches. We also experimentally show that the method generalizes to other modalities with significantly different statistics.},
keywords={Visualization;Vocabulary;Standards;Image resolution;Complexity theory;Algorithm design and analysis;Equations},
doi={10.1109/CVPR.2012.6248039},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248040,
author={Bergamo, Alessandro and Torresani, Lorenzo},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Meta-class features for large-scale object categorization on a budget},
year={2012},
volume={},
number={},
pages={3085-3092},
abstract={In this paper we introduce a novel image descriptor enabling accurate object categorization even with linear models. Akin to the popular attribute descriptors, our feature vector comprises the outputs of a set of classifiers evaluated on the image. However, unlike traditional attributes which represent hand-selected object classes and predefined visual properties, our features are learned automatically and correspond to “abstract” categories, which we name meta-classes. Each meta-class is a super-category obtained by grouping a set of object classes such that, collectively, they are easy to distinguish from other sets of categories. By using “learnability” of the meta-classes as criterion for feature generation, we obtain a set of attributes that encode general visual properties shared by multiple object classes and that are effective in describing and recognizing even novel categories, i.e., classes not present in the training set. We demonstrate that simple linear SVMs trained on our meta-class descriptor significantly outperform the best known classifier on the Caltech256 benchmark. We also present results on the 2010 ImageNet Challenge database where our system produces results approaching those of the best systems, but at a much lower computational cost.},
keywords={Training;Accuracy;Vectors;Kernel;Databases;Abstracts;Image recognition},
doi={10.1109/CVPR.2012.6248040},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248041,
author={Pereira, Jose Costa and Vasconcelos, Nuno},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={On the regularization of image semantics by modal expansion},
year={2012},
volume={},
number={},
pages={3093-3099},
abstract={Recent research efforts in semantic representations and context modeling are based on the principle of task expansion: that vision problems such as object recognition, scene classification, or retrieval (RCR) cannot be solved in isolation. The extended principle of modality expansion (that RCR problems cannot be solved from visual information alone) is investigated in this work. A semantic image labeling system is augmented with text. Pairs of images and text are mapped to a semantic space, and the text features used to regularize their image counterparts. This is done with a new cross-modal regularizer, which learns the mapping of the image features that maximizes their average similarity to those derived from text. The proposed regularizer is class-sensitive, combining a set of class-specific denoising transformations and nearest neighbor interpolation of text-based class assignments. Regularization of a state-of-the-art approach to image retrieval is then shown to produce substantial gains in retrieval accuracy, outperforming recent image retrieval approaches.},
keywords={Semantics;Vectors;Visualization;Image retrieval;Training;History;Encyclopedias},
doi={10.1109/CVPR.2012.6248041},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248042,
author={Jiang, Yuning and Meng, Jingjing and Yuan, Junsong},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Randomized visual phrases for object search},
year={2012},
volume={},
number={},
pages={3100-3107},
abstract={Accurate matching of local features plays an essential role in visual object search. Instead of matching individual features separately, using the spatial context, e.g., bundling a group of co-located features into a visual phrase, has shown to enable more discriminative matching. Despite previous work, it remains a challenging problem to extract appropriate spatial context for matching. We propose a randomized approach to deriving visual phrase, in the form of spatial random partition. By averaging the matching scores over multiple randomized visual phrases, our approach offers three benefits: 1) the aggregation of the matching scores over a collection of visual phrases of varying sizes and shapes provides robust local matching; 2) object localization is achieved by simple thresholding on the voting map, which is more efficient than subimage search; 3) our algorithm lends itself to easy parallelization and also allows a flexible trade-off between accuracy and speed by adjusting the number of partition times. Both theoretical studies and experimental comparisons with the state-of-the-art methods validate the advantages of our approach.},
keywords={Visualization;Context;Feature extraction;Search problems;Image segmentation;Databases;Robustness},
doi={10.1109/CVPR.2012.6248042},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248043,
author={Norouzi, Mohammad and Punjani, Ali and Fleet, David J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast search in Hamming space with multi-index hashing},
year={2012},
volume={},
number={},
pages={3108-3115},
abstract={There has been growing interest in mapping image data onto compact binary codes for fast near neighbor search in vision applications. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used in this way, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact K-nearest neighbor search in Hamming space. The algorithm is straightforward to implement, storage efficient, and it has sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speed-ups over a linear scan baseline and for datasets with up to one billion items, 64- or 128-bit codes, and search radii up to 25 bits.},
keywords={Binary codes;Databases;Complexity theory;Hamming distance;Search problems;Upper bound;Memory management},
doi={10.1109/CVPR.2012.6248043},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248044,
author={Mottaghi, Roozbeh},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Augmenting deformable part models with irregular-shaped object patches},
year={2012},
volume={},
number={},
pages={3116-3123},
abstract={The performance of part-based object detectors generally degrades for highly flexible objects. The limited topological structure of models and pre-specified part shapes are two main factors preventing these detectors from fully capturing large deformations. To better capture the deformations, we propose a novel approach to integrate the detections from a family of part-based detectors with patches of objects that have irregular shape. This integration is formulated as MAP inference in a Conditional Random Field (CRF). The energy function defined over the CRF takes into account the information provided by an object patch classifier and the object detector, and the goal is to augment the partial detections with missing patches, and also to refine the detections that include background clutter. The proposed method is evaluated on the object detection task of PASCAL VOC. Our experimental results show significant improvement over a base part-based detector (which is among the current state-of-the-art methods) especially for the deformable object classes.},
keywords={Histograms;Detectors;Deformable models;Object detection;Color;Image color analysis;Shape},
doi={10.1109/CVPR.2012.6248044},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248045,
author={Yan, Junjie and Lei, Zhen and Yi, Dong and Li, Stan Z.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-pedestrian detection in crowded scenes: A global view},
year={2012},
volume={},
number={},
pages={3124-3129},
abstract={Recent state-of-the-art algorithms have achieved good performance on normal pedestrian detection tasks. However, pedestrian detection in crowded scenes is still challenging due to the significant appearance variation caused by heavy occlusions and complex spatial interactions. In this paper we propose a unified probabilistic framework to globally describe multiple pedestrians in crowded scenes in terms of appearance and spatial interaction. We utilize a mixture model, where every pedestrian is assumed in a special subclass and described by the sub-model. Scores of pedestrian parts are used to represent appearance and quadratic kernel is used to represent relative spatial interaction. For efficient inference, multi-pedestrian detection is modeled as a MAP problem and we utilize greedy algorithm to get an approximation. For discriminative parameter learning, we formulate it as a learning to rank problem, and propose Latent Rank SVM for learning from weakly labeled data. Experiments on various databases validate the effectiveness of the proposed approach.},
keywords={Training;Databases;Vectors;Data models;Probabilistic logic;Support vector machines;Optimization},
doi={10.1109/CVPR.2012.6248045},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248046,
author={Endres, Ian and Srikumar, Vivek and Chang, Ming-Wei and Hoiem, Derek},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning shared body plans},
year={2012},
volume={},
number={},
pages={3130-3137},
abstract={We cast the problem of recognizing related categories as a unified learning and structured prediction problem with shared body plans. When provided with detailed annotations of objects and their parts, these body plans model objects in terms of shared parts and layouts, simultaneously capturing a variety of categories in varied poses. We can use these body plans to jointly train many detectors in a shared framework with structured learning, leading to significant gains for each supervised task. Using our model, we can provide detailed predictions of objects and their parts for both familiar and unfamiliar categories.},
keywords={Detectors;Layout;Deformable models;Animals;Legged locomotion;Joints;Training},
doi={10.1109/CVPR.2012.6248046},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248047,
author={Cevikalp, Hakan and Triggs, Bill},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Efficient object detection using cascades of nearest convex model classifiers},
year={2012},
volume={},
number={},
pages={3138-3145},
abstract={An object detector must detect and localize each instance of the object class of interest in the image. Many recent detectors adopt a sliding window approach, reducing the problem to one of deciding whether the detection window currently contains a valid object instance or background. Machine learning based discriminants such as SVM and boosting are typically used for this, often in the form of classifier cascades to allow more rapid rejection of easy negatives. We argue that “one class” methods - ones that focus mainly on modelling the range of the positive class - are a useful alternative to binary discriminants in such applications, particularly in the early stages of the cascade where one-class approaches may allow simpler classifiers and faster rejection. We implement this in the form of a short cascade of efficient nearest-convex-model one-class classifiers, starting with linear distance-to-affine-hyperplane and interior-of-hypersphere classifiers and finishing with kernelized hypersphere classifiers. We show that our methods have very competitive performance on the Faces in the Wild and ESOGU face detection datasets and state-of-the-art performance on the INRIA Person dataset. As predicted, the one-class formulations provide significant reductions in classifier complexity relative to the corresponding two-class ones.},
keywords={Detectors;Support vector machines;Training;Face detection;Approximation methods;Vectors;Feature extraction},
doi={10.1109/CVPR.2012.6248047},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248048,
author={Hsiao, Edward and Hebert, Martial},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Occlusion reasoning for object detection under arbitrary viewpoint},
year={2012},
volume={},
number={},
pages={3146-3153},
abstract={We present a unified occlusion model for object instance detection under arbitrary viewpoint. Whereas previous approaches primarily modeled local coherency of occlusions or attempted to learn the structure of occlusions from data, we propose to explicitly model occlusions by reasoning about 3D interactions of objects. Our approach accurately represents occlusions under arbitrary viewpoint without requiring additional training data, which can often be difficult to obtain. We validate our model by extending the state-of-the-art LINE2D method for object instance detection and demonstrate significant improvement in recognizing textureless objects under severe occlusions.},
keywords={Computational modeling;Object detection;Approximation methods;Solid modeling;Data models;Cognition;Equations},
doi={10.1109/CVPR.2012.6248048},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248049,
author={Ukita, Norimichi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Articulated pose estimation with parts connectivity using discriminative local oriented contours},
year={2012},
volume={},
number={},
pages={3154-3161},
abstract={This paper proposes contour-based features for articulated pose estimation. Most of recent methods are designed using tree-structured models with appearance evaluation only within the region of each part. While these models allow us to speed up global optimization in localizing the whole parts, useful appearance cues between neighboring parts are missing. Our work focuses on how to evaluate parts connectivity using contour cues. Unlike previous works, we locally evaluate parts connectivity only along the orientation between neighboring parts within where they overlap. This adaptive localization of the features is required for suppressing bad effects due to nuisance edges such as those of background clutter and clothing textures, as well as for reducing computational cost. Discriminative training of the contour features improves estimation accuracy more. Experimental results verify the effectiveness of our contour-based features.},
keywords={Training;Estimation;Computational modeling;Feature extraction;Joints;Training data;Computational efficiency},
doi={10.1109/CVPR.2012.6248049},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248050,
author={Vezhnevets, Alexander and Buhmann, Joachim M. and Ferrari, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Active learning for semantic segmentation with expected change},
year={2012},
volume={},
number={},
pages={3162-3169},
abstract={We address the problem of semantic segmentation: classifying each pixel in an image according to the semantic class it belongs to (e.g. dog, road, car). Most existing methods train from fully supervised images, where each pixel is annotated by a class label. To reduce the annotation effort, recently a few weakly supervised approaches emerged. These require only image labels indicating which classes are present. Although their performance reaches a satisfactory level, there is still a substantial gap between the accuracy of fully and weakly supervised methods. We address this gap with a novel active learning method specifically suited for this setting. We model the problem as a pairwise CRF and cast active learning as finding its most informative nodes. These nodes induce the largest expected change in the overall CRF state, after revealing their true label. Our criterion is equivalent to maximizing an upper-bound on accuracy gain. Experiments on two data-sets show that our method achieves 97% percent of the accuracy of the corresponding fully supervised model, while querying less than 17% of the (super-)pixel labels.},
keywords={Semantics;Labeling;Training;Image segmentation;Roads;Accuracy;Computational modeling},
doi={10.1109/CVPR.2012.6248050},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248051,
author={Schels, Johannes and Liebelt, Joerg and Lienhart, Rainer},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning an object class representation on a continuous viewsphere},
year={2012},
volume={},
number={},
pages={3170-3177},
abstract={We propose an approach to multi-view object class detection and approximate 3D pose estimation. It relies on CAD models as positive training examples and discriminatively learns photometric object parts such that an optimal coverage of intra-class and viewpoint variation is guaranteed. In contrast to previous work, the approach shows a significantly reduced training set dependency while avoiding any manual training supervision or annotation, since it is capable of deriving all relevant information exclusively from the provided set of 3D CAD models and an arbitrary set of 2D negative images. In entirely circumventing semantic or view-based representations, part symmetries and co-occurrences between viewpoints can be efficiently exploited. This, in turn, leads to a significantly lower complexity while still achieving state-of-the-art performance on two current benchmark data sets for two different object classes.},
keywords={Training;Layout;Solid modeling;Estimation;Bicycles;Databases;Adaptation models},
doi={10.1109/CVPR.2012.6248051},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248052,
author={Pishchulin, Leonid and Jain, Arjun and Andriluka, Mykhaylo and Thormählen, Thorsten and Schiele, Bernt},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Articulated people detection and pose estimation: Reshaping the future},
year={2012},
volume={},
number={},
pages={3178-3185},
abstract={State-of-the-art methods for human detection and pose estimation require many training samples for best performance. While large, manually collected datasets exist, the captured variations w.r.t. appearance, shape and pose are often uncontrolled thus limiting the overall performance. In order to overcome this limitation we propose a new technique to extend an existing training set that allows to explicitly control pose and shape variations. For this we build on recent advances in computer graphics to generate samples with realistic appearance and background while modifying body shape and pose. We validate the effectiveness of our approach on the task of articulated human detection and articulated pose estimation. We report close to state of the art results on the popular Image Parsing [25] human pose estimation benchmark and demonstrate superior performance for articulated human detection. In addition we define a new challenge of combined articulated human detection and pose estimation in real-world scenes.},
keywords={Shape;Training;Humans;Joints;IP networks;Estimation;Solid modeling},
doi={10.1109/CVPR.2012.6248052},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248053,
author={Ma, Kai and Ben-Arie, Jezekiel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Vector array based Multi-View Face Detection with compound exemplars},
year={2012},
volume={},
number={},
pages={3186-3193},
abstract={We address the problem of Multiple View Face Detection (MVFD) in unconstrained environments. In order to achieve generalized face detection we use part-based image representations by tessellation of small image patches, which are typified by 2D vector arrays. Faces are detected by a method named Vector Array Recognition by Indexing and Sequencing (VARIS). VARIS is designed to find the optimal similarity matching between the input image and stored exemplars while allowing wide geometrical variations that are limited only by topological constraints. Aggregated similarity is further enhanced by matching the input images with compound exemplars. The novel compounding procedure also reduces the number of exemplars necessary for each class representation. VARIS with compounding performs efficient parallel classification and has polynomial computational complexity.},
keywords={Face;Vectors;Indexing;Arrays;Training;Compounds;Feature extraction},
doi={10.1109/CVPR.2012.6248053},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248054,
author={Wang, Peng and Wang, Jingdong and Zeng, Gang and Feng, Jie and Zha, Hongbin and Li, Shipeng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Salient object detection for searched web images via global saliency},
year={2012},
volume={},
number={},
pages={3194-3201},
abstract={In this paper, we deal with the problem of detecting the existence and the location of salient objects for thumbnail images on which most search engines usually perform visual analysis in order to handle web-scale images. Different from previous techniques, such as sliding window-based or segmentation-based schemes for detecting salient objects, we propose to use a learning approach, random forest in our solution. Our algorithm exploits global features from multiple saliency indicators to directly predict the existence and the position of the salient object. To validate our algorithm, we constructed a large image database collected from Bing image search, that contains hundreds of thousands of manually labeled web images. The experimental results using this new database and the resized MSRA database [16] demonstrate that our algorithm outperforms previous state-of-the-art methods.},
keywords={Feature extraction;Image databases;Training;Image segmentation;Vectors;Object detection},
doi={10.1109/CVPR.2012.6248054},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248055,
author={Guillaumin, Matthieu and Ferrari, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Large-scale knowledge transfer for object localization in ImageNet},
year={2012},
volume={},
number={},
pages={3202-3209},
abstract={ImageNet is a large-scale database of object classes with millions of images. Unfortunately only a small fraction of them is manually annotated with bounding-boxes. This prevents useful developments, such as learning reliable object detectors for thousands of classes. In this paper we propose to automatically populate ImageNet with many more bounding-boxes, by leveraging existing manual annotations. The key idea is to localize objects of a target class for which annotations are not available, by transferring knowledge from related source classes with available annotations. We distinguish two kinds of source classes: ancestors and siblings. Each source provides knowledge about the plausible location, appearance and context of the target objects, which induces a probability distribution over windows in images of the target class. We learn to combine these distributions so as to maximize the location accuracy of the most probable window. Finally, we employ the combined distribution in a procedure to jointly localize objects in all images of the target class. Through experiments on 0.5 million images from 219 classes we show that our technique (i) annotates a wide range of classes with bounding-boxes; (ii) effectively exploits the hierarchical structure of ImageNet, since all sources and types of knowledge we propose contribute to the results; (iii) scales efficiently.},
keywords={Context;Prototypes;Training;Airplanes;Visualization;Support vector machines;Vehicles},
doi={10.1109/CVPR.2012.6248055},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248056,
author={Moshe, Yair and Hel-Or, Hagit and Hel-Or, Yacov},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Foreground detection using spatiotemporal projection kernels},
year={2012},
volume={},
number={},
pages={3210-3217},
abstract={In this paper, we propose a novel video foreground detection method that exploits the statistics of 3D spacetime patches. 3D space-time patches are characterized by means of the subspace they span. As the complexity of real-time systems prohibits performing this modeling directly on the raw pixel data, we propose a novel framework in which spatiotemporal data is sequentially reduced in two stages. The first stage reduces the data using a cascade of linear projections of 3D space-time patches onto a small set of 3D Walsh-Hadamard (WH) basis functions known for its energy compaction of natural images and videos. This stage is efficiently implemented using the Gray-Code filtering scheme [2] requiring only 2 operations per projection. In the second stage, the data is further reduced by applying PCA directly to the WH coefficients exploiting the local statistics in an adaptive manner. Unlike common techniques, this spatiotemporal adaptive projection exploits window appearance as well as its dynamic characteristics. Tests show that the proposed method outperforms recent foreground detection methods and is suitable for real-time implementation on streaming video.},
keywords={Kernel;Hidden Markov models;Computational modeling;Spatiotemporal phenomena;Streaming media;Principal component analysis;Memory management},
doi={10.1109/CVPR.2012.6248056},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248057,
author={Zhu, Jun-Yan and Wu, Jiajun and Wei, Yichen and Chang, Eric and Tu, Zhuowen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised object class discovery via saliency-guided multiple class learning},
year={2012},
volume={},
number={},
pages={3218-3225},
abstract={Discovering object classes from images in a fully unsupervised way is an intrinsically ambiguous task; saliency detection approaches however ease the burden on unsupervised learning. We develop an algorithm for simultaneously localizing objects and discovering object classes via bottom-up (saliency-guided) multiple class learning (bMCL), and make the following contributions: (1) saliency detection is adopted to convert unsupervised learning into multiple instance learning, formulated as bottom-up multiple class learning (bMCL); (2) we utilize the Discriminative EM (DiscEM) to solve our bMCL problem and show DiscEM's connection to the MIL-Boost method[34]; (3) localizing objects, discovering object classes, and training object detectors are performed simultaneously in an integrated framework; (4) significant improvements over the existing methods for multi-class object discovery are observed. In addition, we show single class localization as a special case in our bMCL framework and we also demonstrate the advantage of bMCL over purely data-driven saliency methods.},
keywords={Standards;Computational modeling;Training;Microwave integrated circuits;Boosting;Optimization;Unsupervised learning},
doi={10.1109/CVPR.2012.6248057},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248058,
author={Pirsiavash, Hamed and Ramanan, Deva},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Steerable part models},
year={2012},
volume={},
number={},
pages={3226-3233},
abstract={We describe a method for learning steerable deformable part models. Our models exploit the fact that part templates can be written as linear filter banks. We demonstrate that one can enforce steerability and separability during learning by applying rank constraints. These constraints are enforced with a coordinate descent learning algorithm, where each step can be solved with an off-the-shelf structured SVM solver. The resulting models are orders of magnitude smaller than their counterparts, greatly simplifying learning and reducing run-time computation. Limiting the degrees of freedom also reduces overfitting, which is useful for learning large part vocabularies from limited training data. We learn steerable variants of several state-of-the-art models for object detection, human pose estimation, and facial landmark estimation. Our steerable models are smaller, faster, and often improve performance.},
keywords={Computational modeling;Support vector machines;Vocabulary;Estimation;Standards;Object detection;Vectors},
doi={10.1109/CVPR.2012.6248058},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248059,
author={Gao, Ke and Zhang, Yongdong and Luo, Ping and Zhang, Wei and Xia, Junhai and Lin, Shouxun},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Visual stem mapping and Geometric Tense coding for Augmented Visual Vocabulary},
year={2012},
volume={},
number={},
pages={3234-3241},
abstract={This paper addresses the problem of affine distortions caused by viewpoint changes for the application of image retrieval. We study how to expand the visual words from a query image for better retrieval recall without the sacrifice of retrieval precision and efficiency. Our main contribution is the building of visual dictionaries that retain the mapping relationships between visual words extracted from different viewpoints of the same object. Additionally, in each mapping rule we record the affine transformation in which the two visual words are related, as a compact code of viewpoints relationships. By analogizing the concepts of verb stem and verb tense in text, we use Visual Stems to denote visual words extracted from robust local patches, and record the relationships between their affine variants as visual stem mapping rules, including the geometric relationships coded as Geometric Tenses. In this way, our method augments original visual vocabulary with sufficient and accurate expansion information. In query phase, only the objects corresponding to the same visual stems and coherent geometric tense codes will be regarded as similar ones. Moreover, the mapping rules can be learned offline with only one sample for each object. Experiments show that our method can support efficient object retrieval with high recall, requiring little extra time and space cost over traditional visual vocabularies.},
keywords={Visualization;Vocabulary;Robustness;Vectors;Cameras;Feature extraction;Encoding},
doi={10.1109/CVPR.2012.6248059},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248060,
author={Yao, Angela and Gall, Juergen and Leistner, Christian and Van Gool, Luc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Interactive object detection},
year={2012},
volume={},
number={},
pages={3242-3249},
abstract={In recent years, the rise of digital image and video data available has led to an increasing demand for image annotation. In this paper, we propose an interactive object annotation method that incrementally trains an object detector while the user provides annotations. In the design of the system, we have focused on minimizing human annotation time rather than pure algorithm learning performance. To this end, we optimize the detector based on a realistic annotation cost model based on a user study. Since our system gives live feedback to the user by detecting objects on the fly and predicts the potential annotation costs of unseen images, data can be efficiently annotated by a single user without excessive waiting time. In contrast to popular tracking-based methods for video annotation, our method is suitable for both still images and video. We have evaluated our interactive annotation approach on three datasets, ranging from surveillance, television, to cell microscopy.},
keywords={Training;Humans;Detectors;Vegetation;Object detection;Microscopy;Predictive models},
doi={10.1109/CVPR.2012.6248060},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248061,
author={Heng, Cher Keng and Yokomitsu, Sumio and Matsumoto, Yuichi and Tamura, Hajime},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Shrink boost for selecting multi-LBP histogram features in object detection},
year={2012},
volume={},
number={},
pages={3250-3257},
abstract={Feature selection from sparse and high dimension features using conventional greedy based boosting gives classifiers of poor generalization. We propose a novel “shrink boost” method to address this problem. It solves a sparse regularization problem with two iterative steps. First, a “boosting” step uses weighted training samples to learn a full high dimensional classifier on all features. This avoids over fitting to few features and improves generalization. Next, a “shrinkage” step shrinks least discriminative classifier dimension to zero to remove the redundant features. In our object detection system, we use “shrink boost” to select sparse features from histograms of local binary pattern (LBP) of multiple quantization and image channels to learn classifier of additive lookup tables (LUT). Our evaluation shows that our classifier has much better generalization than those from greedy based boosting and those from SVM methods, even under limited number of train samples. On public dataset of human detection and pedestrian detection, we achieve better performance than state of the arts. On our more challenging dataset of bird detection, we show promising results.},
keywords={Histograms;Table lookup;Quantization;Boosting;Training;Classification algorithms;Object detection},
doi={10.1109/CVPR.2012.6248061},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248062,
author={Ouyang, Wanli and Wang, Xiaogang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A discriminative deep model for pedestrian detection with occlusion handling},
year={2012},
volume={},
number={},
pages={3258-3265},
abstract={Part-based models have demonstrated their merit in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large deformations. To handle the imperfectness of part detectors, this paper presents a probabilistic pedestrian detection framework. In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. Unlike previous occlusion handling approaches that assume independence among visibility probabilities of parts or manually define rules for the visibility relationship, a discriminative deep model is used in this paper for learning the visibility relationship among overlapping parts at multiple layers. Experimental results on three public datasets (Caltech, ETH and Daimler) and a new CUHK occlusion dataset1 specially designed for the evaluation of occlusion handling approaches show the effectiveness of the proposed approach.},
keywords={Deformable models;Detectors;Training;Estimation;Correlation;Support vector machines;Probabilistic logic},
doi={10.1109/CVPR.2012.6248062},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248063,
author={Zhang, Zhiqi and Fidler, Sanja and Waggoner, Jarrell and Cao, Yu and Dickinson, Sven and Siskind, Jeffrey Mark and Wang, Song},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Superedge grouping for object localization by combining appearance and shape information},
year={2012},
volume={},
number={},
pages={3266-3273},
abstract={Both appearance and shape play important roles in object localization and object detection. In this paper, we propose a new superedge grouping method for object localization by incorporating both boundary shape and appearance information of objects. Compared with the previous edge grouping methods, the proposed method does not subdivide detected edges into short edgels before grouping. Such long, unsubdivided superedges not only facilitate the incorporation of object shape information into localization, but also increase the robustness against image noise and reduce computation. We identify and address several important problems in achieving the proposed superedge grouping, including gap filling for connecting superedges, accurate encoding of region-based information into individual edges, and the incorporation of object-shape information into object localization. In this paper, we use the bag of visual words technique to quantify the region-based appearance features of the object of interest. We find that the proposed method, by integrating both boundary and region information, can produce better localization performance than previous subwindow search and edge grouping methods on most of the 20 object categories from the VOC 2007 database. Experiments also show that the proposed method is roughly 50 times faster than the previous edge grouping method.},
keywords={Image edge detection;Shape;Image segmentation;Encoding;Training;Joining processes;Turning},
doi={10.1109/CVPR.2012.6248063},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248064,
author={Wang, Meng and Li, Wei and Wang, Xiaogang},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Transferring a generic pedestrian detector towards specific scenes},
year={2012},
volume={},
number={},
pages={3274-3281},
abstract={The performance of a generic pedestrian detector may drop significantly when it is applied to a specific scene due to mismatch between the source dataset used to train the detector and samples in the target scene. In this paper, we investigate how to automatically train a scene-specific pedestrian detector starting with a generic detector in video surveillance without further manually labeling any samples under a novel transfer learning framework. It tackles the problem from three aspects. (1) With a graphical representation and through exploring the indegrees from target samples to source samples, the source samples are properly re-weighted. The indegrees detect the boundary between the distributions of the source dataset and the target dataset. The re-weighted source dataset better matches the target scene. (2) It takes the context information from motions, scene structures and scene geometry as the confidence scores of samples from the target scene to guide transfer learning. (3) The confidence scores propagate among samples on a graph according to the underlying visual structures of samples. All these considerations are formulated under a single objective function called Confidence-Encoded SVM. At the test stage, only the appearance-based detector is used without the context cues. The effectiveness of the proposed framework is demonstrated through experiments on two video surveillance datasets. Compared with a generic pedestrian detector, it significantly improves the detection rate by 48% and 36% at one false positive per image on the two datasets respectively.},
keywords={Detectors;Support vector machines;Training;Context;Visualization;Video sequences;Estimation},
doi={10.1109/CVPR.2012.6248064},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248065,
author={Prest, Alessandro and Leistner, Christian and Civera, Javier and Schmid, Cordelia and Ferrari, Vittorio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning object class detectors from weakly annotated video},
year={2012},
volume={},
number={},
pages={3282-3289},
abstract={Object detectors are typically trained on a large set of still images annotated by bounding-boxes. This paper introduces an approach for learning object detectors from real-world web videos known only to contain objects of a target class. We propose a fully automatic pipeline that localizes objects in a set of videos of the class and learns a detector for it. The approach extracts candidate spatio-temporal tubes based on motion segmentation and then selects one tube per video jointly over all videos. To compare to the state of the art, we test our detector on still images, i.e., Pascal VOC 2007. We observe that frames extracted from web videos can differ significantly in terms of quality to still images taken by a good camera. Thus, we formulate the learning from videos as a domain adaptation task. We show that training from a combination of weakly annotated videos and fully annotated still images using domain adaptation improves the performance of a detector trained from still images alone.},
keywords={Electron tubes;Detectors;Training;Motion segmentation;Image segmentation;Hidden Markov models;Tracking},
doi={10.1109/CVPR.2012.6248065},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248066,
author={Kumar, Shyam Sunder and Sun, Min and Savarese, Silvio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Mobile object detection through client-server based vote transfer},
year={2012},
volume={},
number={},
pages={3290-3297},
abstract={Mobile platforms such as smart-phones and tablet computers have attained the technological capacity to perform tasks beyond their intended purposes. The steady increase of processing power has enticed researchers to attempt increasingly challenging tasks on mobile devices with appropriate modifications over their stationary counterparts. In this work we present a novel multi-frame object detection application for the mobile platform that is capable of object localization. Our work leverages the hough forest based object detector introduced by Gall et al. in [10]. In our experiments, we demonstrate that our novel, multi-frame generalization of [10] notably improves the detection performance. We test the performance of the technique in variable resolutions, the applicability to several object categories and different datasets. We implement the multi-frame detector on a mobile platform through a novel client-server framework that presents a sound and viable environment for the multi-frame detector. Finally, we study implementations of both single and multi-frame object detectors based on this client-server framework on a mobile device running the android OS.},
keywords={Detectors;Object detection;Mobile handsets;Mobile communication;Video sequences;Performance evaluation;Servers},
doi={10.1109/CVPR.2012.6248066},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248067,
author={Sharma, Pramod and Huang, Chang and Nevatia, Ram},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Unsupervised incremental learning for improved object detection in a video},
year={2012},
volume={},
number={},
pages={3298-3305},
abstract={Most common approaches for object detection collect thousands of training examples and train a detector in an offline setting, using supervised learning methods, with the objective of obtaining a generalized detector that would give good performance on various test datasets. However, when an offline trained detector is applied on challenging test datasets, it may fail in some cases by not being able to detect some objects or by producing false alarms. We propose an unsupervised multiple instance learning (MIL) based incremental solution to deal with this issue. We introduce an MIL loss function for Real Adaboost and present a tracking based effective unsupervised online sample collection mechanism to collect the online samples for incremental learning. Experiments demonstrate the effectiveness of our approach by improving the performance of a state of the art offline trained detector on the challenging datasets for pedestrian category.},
keywords={Detectors;Training;Video sequences;Noise measurement;Vectors;Object detection;Accuracy},
doi={10.1109/CVPR.2012.6248067},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248068,
author={Khan, Fahad Shahbaz and Anwer, Rao Muhammad and van de Weijer, Joost and Bagdanov, Andrew D. and Vanrell, Maria and Lopez, Antonio M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Color attributes for object detection},
year={2012},
volume={},
number={},
pages={3306-3313},
abstract={State-of-the-art object detectors typically use shape information as a low level feature representation to capture the local structure of an object. This paper shows that early fusion of shape and color, as is popular in image classification, leads to a significant drop in performance for object detection. Moreover, such approaches also yields suboptimal results for object categories with varying importance of color and shape. In this paper we propose the use of color attributes as an explicit color representation for object detection. Color attributes are compact, computationally efficient, and when combined with traditional shape features provide state-of-the-art results for object detection. Our method is tested on the PASCAL VOC 2007 and 2009 datasets and results clearly show that our method improves over state-of-the-art techniques despite its simplicity. We also introduce a new dataset consisting of cartoon character images in which color plays a pivotal role. On this dataset, our approach yields a significant gain of 14% in mean AP over conventional state-of-the-art methods.},
keywords={Image color analysis;Object detection;Shape;Feature extraction;Histograms;Computational modeling;Lighting},
doi={10.1109/CVPR.2012.6248068},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248069,
author={Dikmen, Mert and Hoiem, Derek and Huang, Thomas S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A data driven method for feature transformation},
year={2012},
volume={},
number={},
pages={3314-3321},
abstract={Most image understanding algorithms begin with the extraction of information thought to be relevant to the particular task. This is commonly known as feature extraction and has, up to this date, been a largely manual process, where a reasonable method is chosen through validation on the experimented dataset. In this work we propose a data driven, local histogram based feature extraction method that reduces the manual intervention during the feature computation process and improves on the performance of widely used gradient histogram based features (e.g., HOG). We demonstrate favorable object detection results against HOG on the Inria Pedestrian[7], Pascal 2007[10] data.},
keywords={Dictionaries;Histograms;Training;Clustering algorithms;Feature extraction;Object detection;Testing},
doi={10.1109/CVPR.2012.6248069},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248070,
author={Dai, Qieyun and Hoiem, Derek},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning to localize detected objects},
year={2012},
volume={},
number={},
pages={3322-3329},
abstract={In this paper, we propose an approach to accurately localize detected objects. The goal is to predict which features pertain to the object and define the object extent with segmentation or bounding box. Our initial detector is a slight modification of the DPM detector by Felzenszwalb et al., which often reduces confusion with background and other objects but does not cover the full object. We then describe and evaluate several color models and edge cues for local predictions, and we propose two approaches for localization: learned graph cut segmentation and structural bounding box prediction. Our experiments on the PASCAL VOC 2010 dataset show that our approach leads to accurate pixel assignment and large improvement in bounding box overlap, sometimes leading to large overall improvement in detection accuracy.},
keywords={Image edge detection;Color;Image color analysis;Detectors;Computational modeling;Shape;Training},
doi={10.1109/CVPR.2012.6248070},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248071,
author={Liu, Si and Song, Zheng and Liu, Guangcan and Xu, Changsheng and Lu, Hanqing and Yan, Shuicheng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set},
year={2012},
volume={},
number={},
pages={3330-3337},
abstract={In this paper, we address a practical problem of cross-scenario clothing retrieval - given a daily human photo captured in general environment, e.g., on street, finding similar clothing in online shops, where the photos are captured more professionally and with clean background. There are large discrepancies between daily photo scenario and online shopping scenario. We first propose to alleviate the human pose discrepancy by locating 30 human parts detected by a well trained human detector. Then, founded on part features, we propose a two-step calculation to obtain more reliable one-to-many similarities between the query daily photo and online shopping photos: 1) the within-scenario one-to-many similarities between a query daily photo and the auxiliary set are derived by direct sparse reconstruction; and 2) by a cross-scenario many-to-many similarity transfer matrix inferred offline from an extra auxiliary set and the online shopping set, the reliable cross-scenario one-to-many similarities between the query daily photo and all online shopping photos are obtained. We collect a large online shopping dataset and a daily photo dataset, both of which are thoroughly labeled with 15 clothing attributes via Mechanic Turk. The extensive experimental evaluations on the collected datasets well demonstrate the effectiveness of the proposed framework for cross-scenario clothing retrieval.},
keywords={Clothing;Humans;Image reconstruction;Feature extraction;Reliability;Image color analysis;Sparse matrices},
doi={10.1109/CVPR.2012.6248071},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248072,
author={Dai, Zhenwen and Lücke, Jörg},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Autonomous cleaning of corrupted scanned documents — A generative modeling approach},
year={2012},
volume={},
number={},
pages={3338-3345},
abstract={We study the task of cleaning scanned text documents that are strongly corrupted by dirt such as manual line strokes, spilled ink etc. We aim at autonomously removing dirt from a single letter-size page based only on the information the page contains. Our approach, therefore, has to learn character representations without supervision and requires a mechanism to distinguish learned representations from irregular patterns. To learn character representations, we use a probabilistic generative model parameterizing pattern features, feature variances, the features' planar arrangements, and pattern frequencies. The latent variables of the model describe pattern class, pattern position, and the presence or absence of individual pattern features. The model parameters are optimized using a novel variational EM approximation. After learning, the parameters represent, independently of their absolute position, planar feature arrangements and their variances. A quality measure defined based on the learned representation then allows for an autonomous discrimination between regular character patterns and the irregular patterns making up the dirt. The irregular patterns can thus be removed to clean the document. For a full Latin alphabet we found that a single page does not contain sufficiently many character examples. However, even if heavily corrupted by dirt, we show that a page containing a lower number of character types can efficiently and autonomously be cleaned solely based on the structural regularity of the characters it contains. In different examples using characters from different alphabets, we demonstrate generality of the approach and discuss its implications for future developments.},
keywords={Vectors;Histograms;Approximation methods;Computational modeling;Data models;Probabilistic logic;Robustness},
doi={10.1109/CVPR.2012.6248072},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248073,
author={Agrawal, Amit and Ramalingam, Srikumar and Taguchi, Yuichi and Chari, Visesh},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A theory of multi-layer flat refractive geometry},
year={2012},
volume={},
number={},
pages={3346-3353},
abstract={Flat refractive geometry corresponds to a perspective camera looking through single/multiple parallel flat refractive mediums. We show that the underlying geometry of rays corresponds to an axial camera. This realization, while missing from previous works, leads us to develop a general theory of calibrating such systems using 2D-3D correspondences. The pose of 3D points is assumed to be unknown and is also recovered. Calibration can be done even using a single image of a plane. We show that the unknown orientation of the refracting layers corresponds to the underlying axis, and can be obtained independently of the number of layers, their distances from the camera and their refractive indices. Interestingly, the axis estimation can be mapped to the classical essential matrix computation and 5-point algorithm [15] can be used. After computing the axis, the thicknesses of layers can be obtained linearly when refractive indices are known, and we derive analytical solutions when they are unknown. We also derive the analytical forward projection (AFP) equations to compute the projection of a 3D point via multiple flat refractions, which allows non-linear refinement by minimizing the reprojection error. For two refractions, AFP is either 4th or 12th degree equation depending on the refractive indices. We analyze ambiguities due to small field of view, stability under noise, and show how a two layer system can be well approximated as a single layer system. Real experiments using a water tank validate our theory.},
keywords={Cameras;Calibration;Equations;Vectors;Geometry;Estimation;Mathematical model},
doi={10.1109/CVPR.2012.6248073},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248074,
author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Are we ready for autonomous driving? The KITTI vision benchmark suite},
year={2012},
volume={},
number={},
pages={3354-3361},
abstract={Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
keywords={Benchmark testing;Cameras;Optical imaging;Visualization;Optical sensors;Measurement},
doi={10.1109/CVPR.2012.6248074},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248075,
author={Pepik, Bojan and Stark, Michael and Gehler, Peter and Schiele, Bernt},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Teaching 3D geometry to deformable part models},
year={2012},
volume={},
number={},
pages={3362-3369},
abstract={Current object class recognition systems typically target 2D bounding box localization, encouraged by benchmark data sets, such as Pascal VOC. While this seems suitable for the detection of individual objects, higher-level applications such as 3D scene understanding or 3D object tracking would benefit from more fine-grained object hypotheses incorporating 3D geometric information, such as viewpoints or the locations of individual parts. In this paper, we help narrowing the representational gap between the ideal input of a scene understanding system and object class detector output, by designing a detector particularly tailored towards 3D geometric reasoning. In particular, we extend the successful discriminatively trained deformable part models to include both estimates of viewpoint and 3D parts that are consistent across viewpoints. We experimentally verify that adding 3D geometric information comes at minimal performance loss w.r.t. 2D bounding box localization, but outperforms prior work in 3D viewpoint estimation and ultra-wide baseline matching.},
keywords={Training;Solid modeling;Estimation;Cognition;Design automation;Detectors;Optimization},
doi={10.1109/CVPR.2012.6248075},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248076,
author={Jia, Yangqing and Huang, Chang and Darrell, Trevor},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Beyond spatial pyramids: Receptive field learning for pooled image features},
year={2012},
volume={},
number={},
pages={3370-3377},
abstract={In this paper we examine the effect of receptive field designs on classification accuracy in the commonly adopted pipeline of image classification. While existing algorithms usually use manually defined spatial regions for pooling, we show that learning more adaptive receptive fields increases performance even with a significantly smaller codebook size at the coding layer. To learn the optimal pooling parameters, we adopt the idea of over-completeness by starting with a large number of receptive field candidates, and train a classifier with structured sparsity to only use a sparse subset of all the features. An efficient algorithm based on incremental feature selection and retraining is proposed for fast learning. With this method, we achieve the best published performance on the CIFAR-10 dataset, using a much lower dimensional feature space than previous methods.},
keywords={Encoding;Dictionaries;Pipelines;Image coding;Vectors;Accuracy;Training data},
doi={10.1109/CVPR.2012.6248076},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248077,
author={Arbeláez, Pablo and Hariharan, Bharath and Gu, Chunhui and Gupta, Saurabh and Bourdev, Lubomir and Malik, Jitendra},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Semantic segmentation using regions and parts},
year={2012},
volume={},
number={},
pages={3378-3385},
abstract={We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.},
keywords={Detectors;Semantics;Image segmentation;Training;Shape;Head;Joints},
doi={10.1109/CVPR.2012.6248077},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248078,
author={Harchaoui, Zaid and Douze, Matthijs and Paulin, Mattis and Dudik, Miroslav and Malick, Jérôme},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Large-scale image classification with trace-norm regularization},
year={2012},
volume={},
number={},
pages={3386-3393},
abstract={With the advent of larger image classification datasets such as ImageNet, designing scalable and efficient multi-class classification algorithms is now an important challenge. We introduce a new scalable learning algorithm for large-scale multi-class image classification, based on the multinomial logistic loss and the trace-norm regularization penalty. Reframing the challenging non-smooth optimization problem into a surrogate infinite-dimensional optimization problem with a regular ℓ1-regularization penalty, we propose a simple and provably efficient accelerated coordinate descent algorithm. Furthermore, we show how to perform efficient matrix computations in the compressed domain for quantized dense visual features, scaling up to 100,000s examples, 1,000s-dimensional features, and 100s of categories. Promising experimental results on the "Fungus", "Ungulate", and "Vehicles" subsets of ImageNet are presented, where we show that our approach performs significantly better than state-of-the-art approaches for Fisher vectors with 16 Gaussians.},
keywords={Vectors;Matrix decomposition;Logistics;Optimization;Training;Convergence;Acceleration},
doi={10.1109/CVPR.2012.6248078},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248079,
author={Sun, Min and Kohli, Pushmeet and Shotton, Jamie},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Conditional regression forests for human pose estimation},
year={2012},
volume={},
number={},
pages={3394-3401},
abstract={Random forests have been successfully applied to various high level computer vision tasks such as human pose estimation and object segmentation. These models are extremely efficient but work under the assumption that the output variables (such as body part locations or pixel labels) are independent. In this paper, we present a conditional regression forest model for human pose estimation that incorporates dependency relationships between output variables through a global latent variable while still maintaining a low computational cost. We show that the incorporation of a global latent variable encoding torso orientation, or human height, etc., can dramatically increase the accuracy of body joint location prediction. Our model also allows efficient and seamless incorporation of prior knowledge about the problem instance such as the height or orientation of the human subject which can be available from the problem context or via a temporal model. We show that our method significantly outperforms state-of-the-art methods for pose estimation from depth images. The conditional regression model proposed in the paper is general and can be applied to other problems where random forests are used.},
keywords={Joints;Estimation;Humans;Computational modeling;Torso;Mathematical model;Vegetation},
doi={10.1109/CVPR.2012.6248079},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248080,
author={Xu, Yong and Quan, Yuhui and Zhang, Zhuming and Ji, Hui and Fermüller, Cornelia and Nishigaki, Morimichi and Dementhon, Daniel},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Contour-based recognition},
year={2012},
volume={},
number={},
pages={3402-3409},
abstract={Contour is an important cue for object recognition. In this paper, built upon the concept of torque in image space, we propose a new contour-related feature to detect and describe local contour information in images. There are two components for our proposed feature: One is a contour patch detector for detecting image patches with interesting information of object contour, which we call the Maximal/Minimal Torque Patch (MTP) detector. The other is a contour patch descriptor for characterizing a contour patch by sampling the torque values, which we call the Multi-scale Torque (MST) descriptor. Experiments for object recognition on the Caltech-101 dataset showed that the proposed contour feature outperforms other contour-related features and is on a par with many other types of features. When combing our descriptor with the complementary SIFT descriptor, impressive recognition results are observed.},
keywords={Torque;Detectors;Image edge detection;Feature extraction;Shape;Vectors;Force},
doi={10.1109/CVPR.2012.6248080},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248081,
author={Xiang, Yu and Savarese, Silvio},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Estimating the aspect layout of object categories},
year={2012},
volume={},
number={},
pages={3410-3417},
abstract={In this work we seek to move away from the traditional paradigm for 2D object recognition whereby objects are identified in the image as 2D bounding boxes. We focus instead on: i) detecting objects; ii) identifying their 3D poses; iii) characterizing the geometrical and topological properties of the objects in terms of their aspect configurations in 3D. We call such characterization an object's aspect layout (see Fig. 1). We propose a new model for solving these problems in a joint fashion from a single image for object categories. Our model is constructed upon a novel framework based on conditional random fields with maximal margin parameter estimation. Extensive experiments are conducted to evaluate our model's performance in determining object pose and layout from images. We achieve superior viewpoint accuracy results on three public datasets and show extensive quantitative analysis to demonstrate the ability of accurately recovering the aspect layout of objects.},
keywords={Solid modeling;Layout;Training;Estimation;Shape;Object recognition;Design automation},
doi={10.1109/CVPR.2012.6248081},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248082,
author={Jiang, Zhuolin and Zhang, Guangxiao and Davis, Larry S.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Submodular dictionary learning for sparse coding},
year={2012},
volume={},
number={},
pages={3418-3425},
abstract={A greedy-based approach to learn a compact and discriminative dictionary for sparse representation is presented. We propose an objective function consisting of two components: entropy rate of a random walk on a graph and a discriminative term. Dictionary learning is achieved by finding a graph topology which maximizes the objective function. By exploiting the monotonicity and submodularity properties of the objective function and the matroid constraint, we present a highly efficient greedy-based optimization algorithm. It is more than an order of magnitude faster than several recently proposed dictionary learning approaches. Moreover, the greedy algorithm gives a near-optimal solution with a (1/2)-approximation bound. Our approach yields dictionaries having the property that feature points from the same class have very similar sparse codes. Experimental results demonstrate that our approach outperforms several recently proposed dictionary learning techniques for face, action and object category recognition.},
keywords={Dictionaries;Entropy;Encoding;Image color analysis;Training;Matching pursuit algorithms;Partitioning algorithms},
doi={10.1109/CVPR.2012.6248082},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248083,
author={Chen, Qiang and Song, Zheng and Hua, Yang and Huang, Zhongyang and Yan, Shuicheng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Hierarchical matching with side information for image classification},
year={2012},
volume={},
number={},
pages={3426-3433},
abstract={In this work, we introduce a hierarchical matching framework with so-called side information for image classification based on bag-of-words representation. Each image is expressed as a bag of orderless pairs, each of which includes a local feature vector encoded over a visual dictionary, and its corresponding side information from priors or contexts. The side information is used for hierarchical clustering of the encoded local features. Then a hierarchical matching kernel is derived as the weighted sum of the similarities over the encoded features pooled within clusters at different levels. Finally the new kernel is integrated with popular machine learning algorithms for classification purpose. This framework is quite general and flexible, other practical and powerful algorithms can be easily designed by using this framework as a template and utilizing particular side information for hierarchical clustering of the encoded local features. To tackle the latent spatial mismatch issues in SPM, we design in this work two exemplar algorithms based on two types of side information: object confidence map and visual saliency map, from object detection priors and within-image contexts respectively. The extensive experiments over the Caltech-UCSD Birds 200, Oxford Flowers 17 and 102, PASCAL VOC 2007, and PASCAL VOC 2010 databases show the state-of-the-art performances from these two exemplar algorithms.},
keywords={Visualization;Kernel;Encoding;Feature extraction;Layout;Vectors;Context},
doi={10.1109/CVPR.2012.6248083},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248084,
author={Fernando, Basura and Fromont, Elisa and Muselet, Damien and Sebban, Marc},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminative feature fusion for image classification},
year={2012},
volume={},
number={},
pages={3434-3441},
abstract={Bag-of-words-based image classification approaches mostly rely on low level local shape features. However, it has been shown that combining multiple cues such as color, texture, or shape is a challenging and promising task which can improve the classification accuracy. Most of the state-of-the-art feature fusion methods usually aim to weight the cues without considering their statistical dependence in the application at hand. In this paper, we present a new logistic regression-based fusion method, called LRFF, which takes advantage of the different cues without being tied to any of them. We also design a new marginalized kernel by making use of the output of the regression model. We show that such kernels, surprisingly ignored so far by the computer vision community, are particularly well suited to achieve image classification tasks. We compare our approach with existing methods that combine color and shape on three datasets. The proposed learning-based feature fusion process clearly outperforms the state-of-the art fusion methods for image classification.},
keywords={Kernel;Visualization;Shape;Image color analysis;Logistics;Dictionaries;Computational modeling},
doi={10.1109/CVPR.2012.6248084},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248085,
author={Liao, Zicheng and Farhadi, Ali and Wang, Yang and Endres, Ian and Forsyth, David},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Building a dictionary of image fragments},
year={2012},
volume={},
number={},
pages={3442-3449},
abstract={We show how to build large dictionaries of meaningful image fragments. These fragments could represent objects, objects in a local context, or parts of scenes. Our fragments operate as region-based exemplars, and we show how they can be used for image classification, to localize objects, and to compose new images. While each of these activities has been demonstrated before, each has required manually extracted fragments. Because our method for fragment extraction is automatic it can operate at a large scale. Our method uses recent advances in generic object detection techniques, together with discriminative tests to obtain good, clean fragment sets with extensive diversity. Our fragments are organized by the tags of the source images to build a semantically organized fragment table. A good set of fragment exemplars describes only the object, rather than object+context. Context could help identify an object; but it could also contribute noise, because other objects might appear in the same context. We show a slight improvement in classification performance by two standard exemplar matching methods using our fragment dictionary over such methods using image exemplars. This suggests that knowing the support of an exemplar is valuable. Furthermore, we demonstrate our automatically built fragment dictionary is capable of good localization. Finally, our fragment dictionary supports a keyword based fragment search system, which allows artists to get the fragments they need to make image collages.},
keywords={Dictionaries;Image segmentation;Proposals;Training;Buildings;Context;Standards},
doi={10.1109/CVPR.2012.6248085},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248086,
author={Deng, Jia and Krause, Jonathan and Berg, Alexander C. and Fei-Fei, Li},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition},
year={2012},
volume={},
number={},
pages={3450-3457},
abstract={As visual recognition scales up to ever larger numbers of categories, maintaining high accuracy is increasingly difficult. In this work, we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition, motivated by the observation that object categories form a semantic hierarchy consisting of many levels of abstraction. A classifier can select the appropriate level, trading off specificity for accuracy in case of uncertainty. By optimizing this trade-off, we obtain classifiers that try to be as specific as possible while guaranteeing an arbitrarily high accuracy. We formulate the problem as maximizing information gain while ensuring a fixed, arbitrarily small error rate with a semantic hierarchy. We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that, under practical conditions, it converges to an optimal solution. Experiments demonstrate the effectiveness of our algorithm on datasets ranging from 65 to over 10,000 categories.},
keywords={Accuracy;Visualization;Semantics;Animals;Materials;Training;Prediction algorithms},
doi={10.1109/CVPR.2012.6248086},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248087,
author={Redondo-Cabrera, Carolina and López-Sastre, Roberto J. and Acevedo-Rodriguez, Javier and Maldonado-Bascón, Saturnino},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={SURFing the point clouds: Selective 3D spatial pyramids for category-level object recognition},
year={2012},
volume={},
number={},
pages={3458-3465},
abstract={This paper proposes a novel approach to recognize object categories in point clouds. By quantizing 3D SURF local descriptors, computed on partial 3D shapes extracted from the point clouds, a vocabulary of 3D visual words is generated. Using this codebook, we build a Bag-of-Words representation in 3D, which is used in conjunction with a SVM classification machinery. We also introduce the 3D Spatial Pyramid Matching Kernel, which works by partitioning a working volume into fine sub-volumes, and computing a hierarchical weighted sum of histogram intersections at each level of the pyramid structure. With the aim of increasing both the classification accuracy and the computational efficiency of the kernel, we propose selective hierarchical volume decomposition strategies, based on representative and discriminative (sub-)volume selection processes, which drastically reduce the pyramid to consider. Results on the challenging large-scale RGB-D object dataset show that our kernels significantly outperform the state-of-the-art results by using a single 3D shape feature type extracted from individual depth images.},
keywords={Shape;Feature extraction;Histograms;Object recognition;Visualization;Kernel;Accuracy},
doi={10.1109/CVPR.2012.6248087},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248088,
author={Yao, Bangpeng and Bradski, Gary and Fei-Fei, Li},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A codebook-free and annotation-free approach for fine-grained image categorization},
year={2012},
volume={},
number={},
pages={3466-3473},
abstract={Fine-grained categorization refers to the task of classifying objects that belong to the same basic-level class (e.g. different bird species) and share similar shape or visual appearances. Most of the state-of-the-art basic-level object classification algorithms have difficulties in this challenging problem. One reason for this can be attributed to the popular codebook-based image representation, often resulting in loss of subtle image information that are critical for fine-grained classification. Another way to address this problem is to introduce human annotations of object attributes or key points, a tedious process that is also difficult to generalize to new tasks. In this work, we propose a codebook-free and annotation-free approach for fine-grained image categorization. Instead of using vector-quantized codewords, we obtain an image representation by running a high throughput template matching process using a large number of randomly generated image templates. We then propose a novel bagging-based algorithm to build a final classifier by aggregating a set of discriminative yet largely uncorrelated classifiers. Experimental results show that our method outperforms state-of-the-art classification approaches on the Caltech-UCSD Birds dataset.},
keywords={Birds;Humans;Training;Visualization;Image representation;Image color analysis;Vectors},
doi={10.1109/CVPR.2012.6248088},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248089,
author={Duan, Kun and Parikh, Devi and Crandall, David and Grauman, Kristen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discovering localized attributes for fine-grained recognition},
year={2012},
volume={},
number={},
pages={3474-3481},
abstract={Attributes are visual concepts that can be detected by machines, understood by humans, and shared across categories. They are particularly useful for fine-grained domains where categories are closely related to one other (e.g. bird species recognition). In such scenarios, relevant attributes are often local (e.g. “white belly”), but the question of how to choose these local attributes remains largely unexplored. In this paper, we propose an interactive approach that discovers local attributes that are both discriminative and semantically meaningful from image datasets annotated only with fine-grained category labels and object bounding boxes. Our approach uses a latent conditional random field model to discover candidate attributes that are detectable and discriminative, and then employs a recommender system that selects attributes likely to be semantically meaningful. Human interaction is used to provide semantic names for the discovered attributes. We demonstrate our method on two challenging datasets, Caltech-UCSD Birds-200-2011 and Leeds Butterflies, and find that our discovered attributes outperform those generated by traditional approaches.},
keywords={Humans;Visualization;Training;Vocabulary;Vectors;Birds;Equations},
doi={10.1109/CVPR.2012.6248089},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248090,
author={Perronnin, Florent and Akata, Zeynep and Harchaoui, Zaid and Schmid, Cordelia},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Towards good practice in large-scale learning for image classification},
year={2012},
volume={},
number={},
pages={3482-3489},
abstract={We propose a benchmark of several objective functions for large-scale image classification: we compare the one-vs-rest, multiclass, ranking and weighted average ranking SVMs. Using stochastic gradient descent optimization, we can scale the learning to millions of images and thousands of classes. Our experimental evaluation shows that ranking based algorithms do not outperform a one-vs-rest strategy and that the gap between the different algorithms reduces in case of high-dimensional data. We also show that for one-vs-rest, learning through cross-validation the optimal degree of imbalance between the positive and the negative samples can have a significant impact. Furthermore, early stopping can be used as an effective regularization strategy when training with stochastic gradient algorithms. Following these "good practices", we were able to improve the state-of-the-art on a large subset of 10K classes and 9M of images of lmageNet from 16.7% accuracy to 19.1%.},
keywords={Support vector machines;Training;Accuracy;Optimization;Vectors;Equations;Benchmark testing},
doi={10.1109/CVPR.2012.6248090},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248091,
author={Zhou, Ning and Shen, Yi and Peng, Jinye and Fan, Jianping},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Learning inter-related visual dictionary for object recognition},
year={2012},
volume={},
number={},
pages={3490-3497},
abstract={Object recognition is challenging especially when the objects from different categories are visually similar to each other. In this paper, we present a novel joint dictionary learning (JDL) algorithm to exploit the visual correlation within a group of visually similar object categories for dictionary learning where a commonly shared dictionary and multiple category-specific dictionaries are accordingly modeled. To enhance the discrimination of the dictionaries, the dictionary learning problem is formulated as a joint optimization by adding a discriminative term on the principle of the Fisher discrimination criterion. As well as presenting the JDL model, a classification scheme is developed to better take advantage of the multiple dictionaries that have been trained. The effectiveness of the proposed algorithm has been evaluated on popular visual benchmarks.},
keywords={Dictionaries;Visualization;Optimization;Training;Learning systems;Sparse matrices;Joints},
doi={10.1109/CVPR.2012.6248091},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248092,
author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Cats and dogs},
year={2012},
volume={},
number={},
pages={3498-3505},
abstract={We investigate the fine grained object categorization problem of determining the breed of animal from an image. To this end we introduce a new annotated dataset of pets covering 37 different breeds of cats and dogs. The visual problem is very challenging as these animals, particularly cats, are very deformable and there can be quite subtle differences between the breeds. We make a number of contributions: first, we introduce a model to classify a pet breed automatically from an image. The model combines shape, captured by a deformable part model detecting the pet face, and appearance, captured by a bag-of-words model that describes the pet fur. Fitting the model involves automatically segmenting the animal in the image. Second, we compare two classification approaches: a hierarchical one, in which a pet is first assigned to the cat or dog family and then to a breed, and a flat one, in which the breed is obtained directly. We also investigate a number of animal and image orientated spatial layouts. These models are very good: they beat all previously published results on the challenging ASIRRA test (cat vs dog discrimination). When applied to the task of discriminating the 37 different breeds of pets, the models obtain an average accuracy of about 59%, a very encouraging result considering the difficulty of the problem.},
keywords={Positron emission tomography;Image segmentation;Cats;Dogs;Layout;Deformable models;Head},
doi={10.1109/CVPR.2012.6248092},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248093,
author={Sharma, Gaurav and Jurie, Frédéric and Schmid, Cordelia},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Discriminative spatial saliency for image classification},
year={2012},
volume={},
number={},
pages={3506-3513},
abstract={In many visual classification tasks the spatial distribution of discriminative information is (i) non uniform e.g. person `reading' can be distinguished from `taking a photo' based on the area around the arms i.e. ignoring the legs and (ii) has intra class variations e.g. different readers may hold the books differently. Motivated by these observations, we propose to learn the discriminative spatial saliency of images while simultaneously learning a max margin classifier for a given visual classification task. Using the saliency maps to weight the corresponding visual features improves the discriminative power of the image representation. We treat the saliency maps as latent variables and allow them to adapt to the image content to maximize the classification score, while regularizing the change in the saliency maps. Our experimental results on three challenging datasets, for (i) human action classification, (ii) fine grained classification and (iii) scene classification, demonstrate the effectiveness and wide applicability of the method.},
keywords={Visualization;Humans;Support vector machines;Optimization;Vectors;Histograms;Training},
doi={10.1109/CVPR.2012.6248093},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248094,
author={Ni, Bingbing and Xu, Mengdi and Tang, Jinhui and Yan, Shuicheng and Moulin, Pierre},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Omni-range spatial contexts for visual classification},
year={2012},
volume={},
number={},
pages={3514-3521},
abstract={Spatial contexts encode rich discriminative information for visual classification. However, as object shapes and scales vary significantly among images, spatial contexts with manually specified distance ranges are not guaranteed with optimality. In this work, we investigate how to automatically select discriminative and stable distance bin groups for modeling image spatial contexts to improve classification performance. We make two observations. First, the number of distance bins for context modeling can be arbitrarily large, and discriminative contexts are only from a small subset of distance bins. Second, adjacent distance bins for contexts modeling often show similar characteristics, thus encouraging grouping them together can result in more stable representation. Utilizing these two observations, we propose an omni-range spatial context mining framework for image classification. A sparse selection and grouping regularizer is employed along with an empirical risk, to discover discriminative and stable distance bin groups for context modeling. To facilitate efficient optimization, the objective function is approximated by a smooth convex function with theoretically guaranteed error bounds. The selected and grouped image spatial contexts, which are applied in food and national flag recognition, are demonstrated to be discriminative, compact and robust.},
keywords={Context;Visualization;Context modeling;Approximation methods;Histograms;Merging;Vectors},
doi={10.1109/CVPR.2012.6248094},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248095,
author={Yang, Yi and Baker, Simon and Kannan, Anitha and Ramanan, Deva},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Recognizing proxemics in personal photos},
year={2012},
volume={},
number={},
pages={3522-3529},
abstract={Proxemics is the study of how people interact. We present a computational formulation of visual proxemics by attempting to label each pair of people in an image with a subset of physically based “touch codes.” A baseline approach would be to first perform pose estimation and then detect the touch codes based on the estimated joint locations. We found that this sequential approach does not perform well because pose estimation step is too unreliable for images of interacting people, due to difficulties with occlusion and limb ambiguities. Instead, we propose a direct approach where we build an articulated model tuned for each touch code. Each such model contains two people, connected in an appropriate manner for the touch code in question. We fit this model to the image and then base classification on the fitting error. Experiments show that this approach significantly outperforms the sequential baseline as well as other related approches.},
keywords={Mathematical model;Estimation;Joints;Shoulder;Springs;Joining processes;Elbow},
doi={10.1109/CVPR.2012.6248095},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248096,
author={Buló, Samuel Rota and Kontschieder, Peter and Pelillo, Marcello and Bischof, Horst},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Structured Local Predictors for image labelling},
year={2012},
volume={},
number={},
pages={3530-3537},
abstract={In this paper we introduce Structured Local Predictors (SLP) - A new formulation that considers the image labelling problem from a structured learning point of view. SLP are locally operating models, which provide a per-pixel labelling by exploiting contextual relations, learned from complex interactions between labels and a customizable intermediate representation of the image data. Our first key contribution is to handle flexible configurations of pairwise interactions between image pixels while allowing them to be made arbitrarily dependent on the image data. Moreover, we pose the parameter learning process as a convex, structured-learning problem, which can be efficiently solved in a globally optimal way due to the introduction of a continuous, structured output space. Finally, we provide an interface to our model by means of a quantization space, allowing to define task-specific intermediate representations for the input data. In our experiments we demonstrate the broad applicability of our model for tasks like inpainting and semantic labelling.},
keywords={Quantization;Labeling;Computational modeling;Training;Decision trees;Data models;Computer vision},
doi={10.1109/CVPR.2012.6248096},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248097,
author={Neumann, Lukáš and Matas, Jiří},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Real-time scene text localization and recognition},
year={2012},
volume={},
number={},
pages={3538-3545},
abstract={An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by “false positives” caused by detected watermark text in the dataset.},
keywords={Erbium;Text recognition;Complexity theory;Real time systems;Detectors;Robustness;Optical character recognition software},
doi={10.1109/CVPR.2012.6248097},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248098,
author={Zuffi, Silvia and Freifeld, Oren and Black, Michael J.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={From Pictorial Structures to deformable structures},
year={2012},
volume={},
number={},
pages={3546-3553},
abstract={Pictorial Structures (PS) define a probabilistic model of 2D articulated objects in images. Typical PS models assume an object can be represented by a set of rigid parts connected with pairwise constraints that define the prior probability of part configurations. These models are widely used to represent non-rigid articulated objects such as humans and animals despite the fact that such objects have parts that deform non-rigidly. Here we define a new Deformable Structures (DS) model that is a natural extension of previous PS models and that captures the non-rigid shape deformation of the parts. Each part in a DS model is represented by a low-dimensional shape deformation space and pairwise potentials between parts capture how the shape varies with pose and the shape of neighboring parts. A key advantage of such a model is that it more accurately models object boundaries. This enables image likelihood models that are more discriminative than previous PS likelihoods. This likelihood is learned using training imagery annotated using a DS “puppet.” We focus on a human DS model learned from 2D projections of a realistic 3D human body model and use it to infer human poses in images using a form of non-parametric belief propagation.},
keywords={Shape;Joints;Deformable models;Training;Torso;Solid modeling;Vectors},
doi={10.1109/CVPR.2012.6248098},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248099,
author={Han, Xufeng and Berg, Alexander C.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={DCMSVM: Distributed parallel training for single-machine multiclass classifiers},
year={2012},
volume={},
number={},
pages={3554-3561},
abstract={We present an algorithm and implementation for distributed parallel training of single-machine multiclass SVMs. While there is ongoing and healthy debate about the best strategy for multiclass classification, there are some features of the single-machine approach that are not available when training alternatives such as one-vs-all, and that are quite complex for tree based methods. One obstacle to exploring single-machine approaches on large datasets is that they are usually limited to running on a single machine! We build on a framework borrowed from parallel convex optimization - the alternating direction method of multipliers (ADMM) - to develop a new consensus based algorithm for distributed training of single-machine approaches. This is demonstrated with an implementation of our novel sequential dual algorithm (DCMSVM) which allows distributed parallel training with small communication requirements. Benchmark results show significant reduction in wall clock time compared to current state of the art multiclass SVM implementation (Liblinear) on a single node. Experiments are performed on large scale image classification including results with modern high-dimensional features.},
keywords={Training;Accuracy;Optimization;Support vector machines;Convergence;Equations;Computational modeling},
doi={10.1109/CVPR.2012.6248099},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248100,
author={Berg, Alexander C. and Berg, Tamara L. and Daumé, Hal and Dodge, Jesse and Goyal, Amit and Han, Xufeng and Mensch, Alyssa and Mitchell, Margaret and Sood, Aneesh and Stratos, Karl and Yamaguchi, Kota},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Understanding and predicting importance in images},
year={2012},
volume={},
number={},
pages={3562-3569},
abstract={What do people care about in an image? To drive computational visual recognition toward more human-centric outputs, we need a better understanding of how people perceive and judge the importance of content in images. In this paper, we explore how a number of factors relate to human perception of importance. Proposed factors fall into 3 broad types: 1) factors related to composition, e.g. size, location, 2) factors related to semantics, e.g. category of object or scene, and 3) contextual factors related to the likelihood of attribute-object, or object-scene pairs. We explore these factors using what people describe as a proxy for importance. Finally, we build models to predict what will be described about an image given either known image content, or image content estimated automatically by recognition systems.},
keywords={Humans;Semantics;Context;Predictive models;Visualization;Image recognition;Educational institutions},
doi={10.1109/CVPR.2012.6248100},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248101,
author={Yamaguchi, Kota and Kiapour, M. Hadi and Ortiz, Luis E. and Berg, Tamara L.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Parsing clothing in fashion photographs},
year={2012},
volume={},
number={},
pages={3570-3577},
abstract={In this paper we demonstrate an effective method for parsing clothing in fashion photographs, an extremely challenging problem due to the large number of possible garment items, variations in configuration, garment appearance, layering, and occlusion. In addition, we provide a large novel dataset and tools for labeling garment items, to enable future research on clothing estimation. Finally, we present intriguing initial results on using clothing estimates to improve pose identification, and demonstrate a prototype application for pose-independent visual garment retrieval.},
keywords={Clothing;Estimation;Labeling;Training;Visualization;Prototypes;Joints},
doi={10.1109/CVPR.2012.6248101},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248102,
author={Cao, Liujuan and Ji, Rongrong and Gao, Yue and Yang, Yi and Tian, Qi},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Weakly supervised sparse coding with geometric consistency pooling},
year={2012},
volume={},
number={},
pages={3578-3585},
abstract={Most recently the Bag-of-Features (BoF) representation has been well advocated for image search and classification, with two decent phases named sparse coding and max pooling to compensate quantization loss as well as inject spatial layouts. But still, much information has been discarded by quantizing local descriptors with two-dimensional layouts into a one-dimensional BoF histogram. In this paper, we revisit this popular “sparse coding + max pooling” paradigm by “looking around” the local descriptor context towards an optimal BoF. First, we introduce a Weakly supervised Sparse Coding (WSC) to exploit the Classemes-based attribute labeling to refine the descriptor coding procedure. It is achieved by learning an attribute-to-word co-occurrence prior to impose a label inconsistency distortion over the ℓ1 based coding regularizer, such that the descriptor codes can maximally preserve the image semantic similarity. Second, we propose an adaptive feature pooling scheme over “superpixels” rather than over fixed spatial pyramids, named Geometric Consistency Pooling (GCP). As an effect, local descriptors enjoying good geometric consistency are pooled together to ensure a more precise spatial layouts embedding in BoF. Both of our phases are unsupervised, which differ from the existing works in supervised dictionary learning, sparse coding and feature pooling. Therefore, our approach enables potential applications like scalable visual search. We evaluate in both image classification and search benchmarks and report good improvements over the state-of-the-arts.},
keywords={Encoding;Image coding;Layout;Dictionaries;Equations;Image segmentation;Semantics},
doi={10.1109/CVPR.2012.6248102},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248103,
author={Liu, Lingqiao and Wang, Lei},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={What has my classifier learned? Visualizing the classification rules of bag-of-feature model by support region detection},
year={2012},
volume={},
number={},
pages={3586-3593},
abstract={In the past decade, the bag-of-feature model has established itself as the state-of-the-art method in various visual classification tasks. Despite its simplicity and high performance, it normally works as a black box and the classification rule is not transparent to users. However, to better understand the classification process, it is favorable to look into the black box to see how an image is recognized. To fill this gap, we developed a tool called Restricted Support Region Set (RSRS) Detection which can be utilized to visualize the image regions that are critical to the classification decision. More specifically, we define the Restricted Support Region Set for a given image as such a set of size-restricted and non-overlapped regions that if any one of them is removed the image will be wrongly classified. Focusing on the state-of-the-art bag-of-feature classification system, we developed an efficient RSRS detection algorithm and discussed its applications. We showed that it can be used to identify the limitation of a classifier, predict its failure mode, discover the classification rules and reveal the database bias. Moreover, as experimentally demonstrated, this tool also enables common users to efficiently tune the classifier by removing the inappropriate support regions, which can lead to a better generalization performance.},
keywords={Visualization;Encoding;Feature extraction;Heating;Humans;Detection algorithms;Image coding},
doi={10.1109/CVPR.2012.6248103},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248104,
author={Hao, Qiang and Cai, Rui and Li, Zhiwei and Zhang, Lei and Pang, Yanwei and Wu, Feng},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={3D visual phrases for landmark recognition},
year={2012},
volume={},
number={},
pages={3594-3601},
abstract={In this paper, we study the problem of landmark recognition and propose to leverage 3D visual phrases to improve the performance. A 3D visual phrase is a triangular facet on the surface of a reconstructed 3D landmark model. In contrast to existing 2D visual phrases which are mainly based on co-occurrence statistics in 2D image planes, such 3D visual phrases explicitly characterize the spatial structure of a 3D object (landmark), and are highly robust to projective transformations due to viewpoint changes. We present an effective solution to discover, describe, and detect 3D visual phrases. The experiments on 10 landmarks have achieved promising results, which demonstrate that our approach provides a good balance between precision and recall of landmark recognition while reducing the dependence on post-verification to reject false positives.},
keywords={Solid modeling;Image reconstruction;Feature extraction;Surface reconstruction;Robustness;Image recognition},
doi={10.1109/CVPR.2012.6248104},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248105,
author={Chi, Yuejie and Porikli, Fatih},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Connecting the dots in multi-class classification: From nearest subspace to collaborative representation},
year={2012},
volume={},
number={},
pages={3602-3609},
abstract={We present a novel multi-class classifier that strikes a balance between the nearest-subspace classifier, which assigns a test sample to the class that minimizes the distance between the test sample and its principal projection in the selected class, and a collaborative representation based classifier, which classifies a sample to the class that minimizes the distance between the collaborative components of the test sample by using all training samples from all classes as the dictionary and its projection in the selected class. In our formulation, the sparse representation based classifier [1] and nearest subspace classifier become special cases under different regularization parameters. We show that the classification performance can be improved by optimally tuning the regularization parameter, which can be done at almost no extra computational cost. We give extensive numerical examples for digit identification and face recognition with performance comparisons of different choices of collaborative representations, in particular when only a partial observation of the test sample is available via compressive sensing measurements.},
keywords={Training;Collaboration;Face recognition;Dictionaries;Feature extraction;Accuracy;Strontium},
doi={10.1109/CVPR.2012.6248105},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248106,
author={Gavves, Efstratios and Snoek, Cees G.M. and Smeulders, Arnold W.M.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Convex reduction of high-dimensional kernels for visual classification},
year={2012},
volume={},
number={},
pages={3610-3617},
abstract={Limiting factors of fast and effective classifiers for large sets of images are their dependence on the number of images analyzed and the dimensionality of the image representation. Considering the growing number of images as a given, we aim to reduce the image feature dimensionality in this paper. We propose reduced linear kernels that use only a portion of the dimensions to reconstruct a linear kernel. We formulate the search for these dimensions as a convex optimization problem, which can be solved efficiently. Different from existing kernel reduction methods, our reduced kernels are faster and maintain the accuracy benefits from non-linear embedding methods that mimic non-linear SVMs. We show these properties on both the Scenes and PASCAL VOC 2007 datasets. In addition, we demonstrate how our reduced kernels allow to compress Fisher vector for use with non-linear embeddings, leading to high accuracy. What is more, without using any labeled examples the selected and weighed kernel dimensions appear to correspond to visually meaningful patches in the images.},
keywords={Kernel;Principal component analysis;Complexity theory;Visualization;Support vector machines;Vectors;Optimization},
doi={10.1109/CVPR.2012.6248106},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248107,
author={Shabou, Aymen and LeBorgne, Hervé},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Locality-constrained and spatially regularized coding for scene categorization},
year={2012},
volume={},
number={},
pages={3618-3625},
abstract={Improving coding and spatial pooling for bag-of-words based feature design have gained a lot of attention in recent works addressing object recognition and scene classification. Regarding the coding step in particular, properties such as sparsity, locality and saliency have been investigated. The main contribution of this work consists in taking into acount the local spatial context of an image into the usual coding strategies proposed in the state-of-the-art. For this purpose, given an imgae, dense local features are extracted and structured in a lattice. The latter is endowed with a neighborhood system and pairwise interactions. We propose a new objective function to encode local features, which preserves locality constraints both in the feature space and the spatial domain of the image. In addition, an appropriate efficient optimization algorithm is provided, inspired from the graph-cut framework. In conjunction with the maximum-pooling operation and the spatial pyramid matching, that reflects a global spatial layout, the proposed method improves the performances of several state-of-the-art coding schemes for scene classification on three publicly available benchmarks (UIUC 8-sport, Scene-15 and Caltech-101).},
keywords={Encoding;Visualization;Optimization;Feature extraction;Vectors;Indexes;Image coding},
doi={10.1109/CVPR.2012.6248107},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248108,
author={Ebert, Sandra and Fritz, Mario and Schiele, Bernt},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={RALF: A reinforced active learning formulation for object class recognition},
year={2012},
volume={},
number={},
pages={3626-3633},
abstract={Active learning aims to reduce the amount of labels required for classification. The main difficulty is to find a good trade-off between exploration and exploitation of the labeling process that depends - among other things - on the classification task, the distribution of the data and the employed classification scheme. In this paper, we analyze different sampling criteria including a novel density-based criteria and demonstrate the importance to combine exploration and exploitation sampling criteria. We also show that a time-varying combination of sampling criteria often improves performance. Finally, by formulating the criteria selection as a Markov decision process, we propose a novel feedback-driven framework based on reinforcement learning. Our method does not require prior information on the dataset or the sampling criteria but rather is able to adapt the sampling strategy during the learning process by experience. We evaluate our approach on three challenging object recognition datasets and show superior performance to previous active learning methods.},
keywords={Support vector machines;Labeling;Training;Accuracy;Uncertainty;Kernel;Markov processes},
doi={10.1109/CVPR.2012.6248108},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248109,
author={Flitton, Greg and Breckon, Toby P. and Megherbi, Najla},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={A 3D extension to cortex like mechanisms for 3D object class recognition},
year={2012},
volume={},
number={},
pages={3634-3641},
abstract={We introduce a novel 3D extension to the hierarchical visual cortex model used for prior work in 2D object recognition. Prior work on the use of the visual cortex standard model for the explicit task of object class recognition has solely concentrated on 2D imagery. In this paper we discuss the explicit 3D extension of each layer in this visual cortex model hierarchy for use in object recognition in 3D volumetric imagery. We apply this extended methodology to the automatic detection of a class of threat items in Computed Tomography (CT) security baggage imagery. The CT imagery suffers from poor resolution and a large number of artefacts generated through the presence of metallic objects. In our examination of recognition performance we make a comparison to a codebook approach derived from a 3D SIFT descriptor and demonstrate that the visual cortex method out-performs in this imagery. Recognition rates in excess of 95% with minimal false positive rates are demonstrated in the detection of a range of threat items.},
keywords={Visualization;Computed tomography;Brain modeling;Vectors;Image recognition;Solid modeling;Object recognition},
doi={10.1109/CVPR.2012.6248109},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248110,
author={Ciregan, Dan and Meier, Ueli and Schmidhuber, Jürgen},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Multi-column deep neural networks for image classification},
year={2012},
volume={},
number={},
pages={3642-3649},
abstract={Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
keywords={Training;Error analysis;Neurons;Computer architecture;Benchmark testing;Graphics processing unit},
doi={10.1109/CVPR.2012.6248110},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248111,
author={McCann, Sancho and Lowe, David G.},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Local Naive Bayes Nearest Neighbor for image classification},
year={2012},
volume={},
number={},
pages={3650-3656},
abstract={We present Local Naive Bayes Nearest Neighbor, an improvement to the NBNN image classification algorithm that increases classification accuracy and improves its ability to scale to large numbers of object classes. The key observation is that only the classes represented in the local neighborhood of a descriptor contribute significantly and reliably to their posterior probability estimates. Instead of maintaining a separate search structure for each class's training descriptors, we merge all of the reference data together into one search structure, allowing quick identification of a descriptor's local neighborhood. We show an increase in classification accuracy when we ignore adjustments to the more distant classes and show that the run time grows with the log of the number of classes rather than linearly in the number of classes as did the original. Local NBNN gives a 100 times speed-up over the original NBNN on the Caltech 256 dataset. We also provide the first head-to-head comparison of NBNN against spatial pyramid methods using a common set of input features. We show that local NBNN outperforms all previous NBNN based methods and the original spatial pyramid model. However, we find that local NBNN, while competitive with, does not beat state-of-the-art spatial pyramid methods that use local soft assignment and max-pooling.},
keywords={Accuracy;Nearest neighbor searches;Indexes;Kernel;Approximation methods;Approximation algorithms;Training},
doi={10.1109/CVPR.2012.6248111},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248112,
author={Kankuekul, Pichai and Kawewong, Aram and Tangruamsub, Sirinart and Hasegawa, Osamu},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Online incremental attribute-based zero-shot learning},
year={2012},
volume={},
number={},
pages={3657-3664},
abstract={The paper presents a new online incremental zero-shot learning method for applications in robotics and mobile communications where attribute labeling is obtained via online interaction with users, and where the potential for inconsistency exists. Unique to most previous offline batch learning methods, the proposed method is based on the indirect-attribute-prediction (IAP) model instead of the direct-attribute-prediction (DAP). Using self-organizing and incremental neural networks (SOINN) as the learning mechanism, our method can learn new attributes and update existing attributes in an online incremental manner while retaining as high accuracy as that of the state-of-the-art offline method. Compared to the offline methods, the computation time has also been reduced by more than 99%. Two experiments evaluated two aspects of the proposed method. First, our method clearly outperforms the previous IAP-based offline method in terms of both time and accuracy, and yield approximately the same accuracy as the DAP-based offline method. Second, the proposed method can deal with situations where object attributes are gradually labeled via interaction with many users and where some of them may be incorrect. This scenario is very important for applications in mobile communications and robotics where some objects and attributes may be initially unknown and must be learnt online.},
keywords={Training;Learning systems;Labeling;Accuracy;Humans;Robots;Support vector machines},
doi={10.1109/CVPR.2012.6248112},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248364,
author={Zhang, Ning and Farrell, Ryan and Darrell, Trever},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Pose pooling kernels for sub-category recognition},
year={2012},
volume={},
number={},
pages={3665-3672},
abstract={The ability to normalize pose based on super-category landmarks can significantly improve models of individual categories when training data are limited. Previous methods have considered the use of volumetric or morphable models for faces and for certain classes of articulated objects. We consider methods which impose fewer representational assumptions on categories of interest, and exploit contemporary detection schemes which consider the ensemble of responses of detectors trained for specific pose-keypoint configurations. We develop representations for poselet-based pose normalization using both explicit warping and implicit pooling as mechanisms. Our method defines a pose normalized similarity or kernel function that is suitable for nearest-neighbor or kernel-based learning methods.},
keywords={Kernel;Birds;Vectors;Training data;Detectors;Feature extraction;Head},
doi={10.1109/CVPR.2012.6248364},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248113,
author={Jamriška, Ondřej and Sýkora, Daniel and Hornung, Alexander},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Cache-efficient graph cuts on structured grids},
year={2012},
volume={},
number={},
pages={3673-3680},
abstract={Finding minimal cuts on graphs with a grid-like structure has become a core task for solving many computer vision and graphics related problems. However, computation speed and memory consumption oftentimes limit the effective use in applications requiring high resolution grids or interactive response. In particular, memory bandwidth represents one of the major bottlenecks even in today's most efficient implementations. We propose a compact data structure with cache-efficient memory layout for the representation of graph instances that are based on regular N-D grids with topologically identical neighborhood systems. For this common class of graphs our data structure allows for 3 to 12 times higher grid resolutions and a 3- to 9-fold speedup compared to existing approaches. Our design is agnostic to the underlying algorithm, and hence orthogonal to other optimizations such as parallel and hierarchical processing. We evaluate the performance gain on a variety of typical problems including 2D/3D segmentation, colorization, and stereo. All experiments show an unconditional improvement in terms of speed and memory consumption, with graceful performance degradation for graphs with increasing topological irregularities.},
keywords={Layout;Optimization;Indexes;Data structures;Performance gain;Memory management;Bandwidth},
doi={10.1109/CVPR.2012.6248113},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248114,
author={Tamrakar, Amir and Ali, Saad and Yu, Qian and Liu, Jingen and Javed, Omar and Divakaran, Ajay and Cheng, Hui and Sawhney, Harpreet},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Evaluation of low-level features and their combinations for complex event detection in open source videos},
year={2012},
volume={},
number={},
pages={3681-3688},
abstract={Low-level appearance as well as spatio-temporal features, appropriately quantized and aggregated into Bag-of-Words (BoW) descriptors, have been shown to be effective in many detection and recognition tasks. However, their effcacy for complex event recognition in unconstrained videos have not been systematically evaluated. In this paper, we use the NIST TRECVID Multimedia Event Detection (MED11 [1]) open source dataset, containing annotated data for 15 high-level events, as the standardized test bed for evaluating the low-level features. This dataset contains a large number of user-generated video clips. We consider 7 different low-level features, both static and dynamic, using BoW descriptors within an SVM approach for event detection. We present performance results on the 15 MED11 events for each of the features as well as their combinations using a number of early and late fusion strategies and discuss their strengths and limitations.},
keywords={Videos;Feature extraction;Trajectory;Event detection;Training;Support vector machines;Computer vision},
doi={10.1109/CVPR.2012.6248114},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{6248115,
author={Wesierski, Daniel and Mkhinini, Maher and Horain, Patrick and Jezierska, Anna},
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, title={Fast recursive ensemble convolution of Haar-like features},
year={2012},
volume={},
number={},
pages={3689-3696},
abstract={Haar-like features are ubiquitous in computer vision, e.g. for Viola and Jones face detection or local descriptors such as Speeded-Up-Robust-Features. They are classically computed in one pass over integral image by reading the values at the feature corners. Here we present a new, general parsing formalism for convolving them more efficiently. Our method is fully automatic and applicable to an arbitrary set of Haar-like features. The parser reduces the number of memory accesses which are the main computational bottleneck during convolution on modern computer architectures. It first splits the features into simpler kernels. Then it aligns and reuses them where applicable forming an ensemble of recursive convolution trees, which can be computed faster. This is illustrated with experiments, which show a significant speed-up over the classic approach.},
keywords={Kernel;Convolution;Vectors;Face detection;Computer architecture;Feature extraction},
doi={10.1109/CVPR.2012.6248115},
ISSN={1063-6919},
month={June},}